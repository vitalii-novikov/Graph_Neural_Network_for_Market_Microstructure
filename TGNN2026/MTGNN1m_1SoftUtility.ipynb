{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nGraph WaveNet / MTGNN-style model (adaptive adjacency + dilated gated TCN)\\n3-node graph trading model: ETH (target), BTC, ADA on 1-minute data.\\n\\nThis notebook implements:\\n- Multi-task training (Variant 3): CE (3-class) + exit_ret regression + soft utility loss\\n- Walk-forward CV with threshold selection on VAL and fixed evaluation on TEST\\n- Safer artifact saving:\\n    * .pt stores ONLY model weights (state_dict)\\n    * scalers stored as .npz (center/scale arrays, no pickle)\\n    * meta stored as .json (cfg, thresholds, fold indices, file names)\\n- Training logs include:\\n    * train/val total loss + components\\n    * val trade_auc and dir_auc each epoch (for selection/early stopping)\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Graph WaveNet / MTGNN-style model (adaptive adjacency + dilated gated TCN)\n",
    "3-node graph trading model: ETH (target), BTC, ADA on 1-minute data.\n",
    "\n",
    "This notebook implements:\n",
    "- Multi-task training (Variant 3): CE (3-class) + exit_ret regression + soft utility loss\n",
    "- Walk-forward CV with threshold selection on VAL and fixed evaluation on TEST\n",
    "- Safer artifact saving:\n",
    "    * .pt stores ONLY model weights (state_dict)\n",
    "    * scalers stored as .npz (center/scale arrays, no pickle)\n",
    "    * meta stored as .json (cfg, thresholds, fold indices, file names)\n",
    "- Training logs include:\n",
    "    * train/val total loss + components\n",
    "    * val trade_auc and dir_auc each epoch (for selection/early stopping)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cpu\n",
      "Assets: ['ADA', 'BTC', 'ETH'] | Target: ETH\n",
      "Artifacts dir: /Users/vitalii/Desktop/Model_Market_Microstructure/Graph_Neural_Network_for_Market_Microstructure/TGNN2026/artifacts_gwnet_multitask\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# Step 0: Imports, seed, config (single unified block)\n",
    "# ======================================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, roc_auc_score\n",
    "\n",
    "\n",
    "def seed_everything(seed: int = 1234) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "seed_everything(100)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "\n",
    "torch.set_num_threads(max(1, os.cpu_count() or 4))\n",
    "\n",
    "# --- Global config (single unified block) ---\n",
    "CFG: Dict[str, Any] = {\n",
    "    # data\n",
    "    \"freq\": \"1min\",\n",
    "    \"data_dir\": \"../dataset\",\n",
    "    \"final_test_frac\": 0.10,\n",
    "\n",
    "    # order book\n",
    "    \"book_levels\": 15,\n",
    "    \"top_levels\": 5,\n",
    "    \"near_levels\": 5,\n",
    "\n",
    "    # walk-forward windows (in sample-space)\n",
    "    \"train_min_frac\": 0.50,\n",
    "    \"val_window_frac\": 0.10,\n",
    "    \"test_window_frac\": 0.10,\n",
    "    \"step_window_frac\": 0.10,\n",
    "\n",
    "    # scaling\n",
    "    \"max_abs_feat\": 10.0,\n",
    "    \"max_abs_edge\": 6.0,\n",
    "\n",
    "    # correlations / graph\n",
    "    \"corr_windows\": [6 * 5, 12 * 5, 24 * 5, 48 * 5, 84 * 5],  # 30m,1h,2h,4h,7h\n",
    "    \"corr_lags\": [0, 1, 2, 5],  # lead-lag (no leakage)\n",
    "    \"edges_mode\": \"all_pairs\",  # \"manual\" | \"all_pairs\"\n",
    "    \"edges\": [(\"ADA\", \"BTC\"), (\"ADA\", \"ETH\"), (\"ETH\", \"BTC\")],  # used if edges_mode=\"manual\"\n",
    "    \"add_self_loops\": True,\n",
    "    \"edge_transform\": \"fisher\",  # \"none\" | \"fisher\"\n",
    "    \"edge_scale\": True,\n",
    "\n",
    "    # triple-barrier\n",
    "    \"tb_horizon\": 1 * 30,\n",
    "    \"lookback\": 4 * 12 * 5,\n",
    "    \"tb_pt_mult\": 1.2,\n",
    "    \"tb_sl_mult\": 1.1,\n",
    "    \"tb_min_barrier\": 0.001,\n",
    "    \"tb_max_barrier\": 0.006,\n",
    "\n",
    "    # training\n",
    "    \"batch_size\": 128,\n",
    "    \"epochs\": 20,\n",
    "    \"lr\": 3e-4,\n",
    "    \"weight_decay\": 5e-4,\n",
    "    \"grad_clip\": 1.0,\n",
    "    \"dropout\": 0.15,\n",
    "\n",
    "    # stability tricks\n",
    "    \"label_smoothing\": 0.02,\n",
    "    \"use_weighted_sampler\": True,\n",
    "    \"use_onecycle\": True,\n",
    "\n",
    "    # --- Graph WaveNet core channels\n",
    "    \"gwn_residual_channels\": 64,\n",
    "    \"gwn_dilation_channels\": 64,\n",
    "    \"gwn_skip_channels\": 128,\n",
    "    \"gwn_end_channels\": 128,\n",
    "\n",
    "    # blocks/layers: dilations reset each block\n",
    "    \"gwn_blocks\": 3,\n",
    "    \"gwn_layers_per_block\": 2,\n",
    "    \"gwn_kernel_size\": 2,\n",
    "\n",
    "    # adaptive adjacency\n",
    "    \"adj_emb_dim\": 8,\n",
    "    \"adj_temperature\": 1.0,\n",
    "    \"adaptive_topk\": 3,  # for 3 nodes, 3 keeps all\n",
    "\n",
    "    # adjacency regularization\n",
    "    \"adj_l1_lambda\": 1e-3,\n",
    "    \"adj_prior_lambda\": 1e-2,\n",
    "\n",
    "    # build prior adjacency from edge_attr (last timestep)\n",
    "    \"prior_use_abs\": False,\n",
    "    \"prior_diag_boost\": 1.0,\n",
    "    \"prior_row_normalize\": True,\n",
    "\n",
    "    # trading eval\n",
    "    \"cost_bps\": 1.0,\n",
    "\n",
    "    # threshold sweep grids (val only)\n",
    "    \"thr_trade_grid\": [0.50, 0.55, 0.60, 0.65, 0.70, 0.75],\n",
    "    \"thr_dir_grid\":   [0.50, 0.55, 0.60, 0.65, 0.70],\n",
    "    \"eval_min_trades\": 50,\n",
    "    \"max_trade_rate_val\": 0.65,\n",
    "    \"trade_rate_penalty\": 0.10,\n",
    "    \"thr_objective\": \"pnl_sum\",\n",
    "    \"proxy_target_trades\": [50, 100, 200],\n",
    "\n",
    "    # selection metric\n",
    "    \"sel_metric_dir_weight\": 0.50,  # selection: trade_auc + w*dir_auc\n",
    "\n",
    "    # extra anti-overtrading penalty on probabilities (optional)\n",
    "    \"trade_prob_penalty\": 0.01,  # penalize mean(p_short+p_long)\n",
    "\n",
    "    # multi-task loss weights\n",
    "    \"loss_w_ce\": 1.0,\n",
    "    \"loss_w_ret\": 0.25,       # regression weight\n",
    "    \"loss_w_utility\": 0.10,   # soft utility weight\n",
    "\n",
    "    # regression / utility stability\n",
    "    \"exit_ret_clip\": 0.03,    # clip exit_ret for ret/utility losses (log-return units)\n",
    "    \"ret_huber_delta\": 0.01,  # Huber delta\n",
    "    \"utility_k\": 2.0,         # tanh slope for soft position\n",
    "\n",
    "    # artifact saving\n",
    "    \"artifact_dir\": \"./artifacts_gwnet_multitask\",\n",
    "}\n",
    "\n",
    "ASSETS = [\"ADA\", \"BTC\", \"ETH\"]\n",
    "ASSET2IDX = {a: i for i, a in enumerate(ASSETS)}\n",
    "TARGET_ASSET = \"ETH\"\n",
    "TARGET_NODE = ASSET2IDX[TARGET_ASSET]\n",
    "\n",
    "ART_DIR = Path(CFG[\"artifact_dir\"])\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Assets:\", ASSETS, \"| Target:\", TARGET_ASSET)\n",
    "print(\"Artifacts dir:\", str(ART_DIR.resolve()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EDGE_LIST: ['ADA->BTC', 'ADA->ETH', 'BTC->ADA', 'BTC->ETH', 'ETH->ADA', 'ETH->BTC', 'ADA->ADA', 'BTC->BTC', 'ETH->ETH']\n",
      "EDGE_INDEX: [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1], [0, 0], [1, 1], [2, 2]]\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# Step 0.1: Graph edges\n",
    "# ======================================================================\n",
    "\n",
    "def build_edge_list(cfg: Dict[str, Any], assets: List[str]) -> List[Tuple[str, str]]:\n",
    "    mode = str(cfg.get(\"edges_mode\", \"manual\"))\n",
    "    if mode == \"manual\":\n",
    "        edges = list(cfg[\"edges\"])\n",
    "    elif mode == \"all_pairs\":\n",
    "        edges = [(s, t) for s in assets for t in assets if s != t]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown edges_mode={mode}\")\n",
    "\n",
    "    if bool(cfg.get(\"add_self_loops\", True)):\n",
    "        edges = edges + [(a, a) for a in assets]\n",
    "    return edges\n",
    "\n",
    "\n",
    "EDGE_LIST = build_edge_list(CFG, ASSETS)\n",
    "EDGE_NAMES = [f\"{s}->{t}\" for (s, t) in EDGE_LIST]\n",
    "EDGE_INDEX = torch.tensor([[ASSET2IDX[s], ASSET2IDX[t]] for (s, t) in EDGE_LIST], dtype=torch.long)\n",
    "\n",
    "print(\"EDGE_LIST:\", EDGE_NAMES)\n",
    "print(\"EDGE_INDEX:\", EDGE_INDEX.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded df: (12831, 106)\n",
      "Time range: 2021-04-07 11:34:00+00:00 -> 2021-04-16 10:15:00+00:00\n",
      "                  timestamp      ADA  spread_ADA      buys_ADA      sells_ADA  \\\n",
      "0 2021-04-07 11:34:00+00:00  1.16205      0.0001  56936.467913  258248.957367   \n",
      "1 2021-04-07 11:35:00+00:00  1.16800      0.0022  56491.336799   78665.286640   \n",
      "\n",
      "   bids_vol_ADA_0  bids_vol_ADA_1  bids_vol_ADA_2  bids_vol_ADA_3  \\\n",
      "0      876.869995     5984.169922        5.810000       18.240000   \n",
      "1    33769.671875    23137.169922      550.299988      550.299988   \n",
      "\n",
      "   bids_vol_ADA_4  ...  asks_vol_ETH_8  asks_vol_ETH_9  asks_vol_ETH_10  \\\n",
      "0    19844.640625  ...      373.700012      196.699997      2059.709961   \n",
      "1    19012.320312  ...     3873.709961     1954.630005       197.039993   \n",
      "\n",
      "   asks_vol_ETH_11  asks_vol_ETH_12  asks_vol_ETH_13  asks_vol_ETH_14  \\\n",
      "0      3874.989990      5901.209961       178.289993     28512.160156   \n",
      "1     12661.990234     20006.970703     28562.310547      3874.379883   \n",
      "\n",
      "     lr_ADA    lr_BTC    lr_ETH  \n",
      "0  0.000000  0.000000  0.000000  \n",
      "1  0.005107  0.000937  0.001931  \n",
      "\n",
      "[2 rows x 106 columns]\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# Step 1: Data loading\n",
    "# ======================================================================\n",
    "\n",
    "def load_asset(asset: str, freq: str, data_dir: Path, book_levels: int, part: Tuple[int, int] = (0, 80)) -> pd.DataFrame:\n",
    "    path = data_dir / f\"{asset}_{freq}.csv\"\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.iloc[int(len(df) * part[0] / 100): int(len(df) * part[1] / 100)]\n",
    "\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"system_time\"]).dt.round(\"min\")\n",
    "    df = df.sort_values(\"timestamp\").set_index(\"timestamp\")\n",
    "\n",
    "    bid_cols = [f\"bids_notional_{i}\" for i in range(book_levels)]\n",
    "    ask_cols = [f\"asks_notional_{i}\" for i in range(book_levels)]\n",
    "\n",
    "    needed = [\"midpoint\", \"spread\", \"buys\", \"sells\"] + bid_cols + ask_cols\n",
    "    missing = [c for c in needed if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"{asset}: missing columns in CSV: {missing[:10]}{'...' if len(missing) > 10 else ''}\")\n",
    "\n",
    "    return df[needed]\n",
    "\n",
    "\n",
    "def load_all_assets() -> pd.DataFrame:\n",
    "    freq = CFG[\"freq\"]\n",
    "    data_dir = Path(CFG[\"data_dir\"])\n",
    "    book_levels = CFG[\"book_levels\"]\n",
    "\n",
    "    def rename_cols(df_one: pd.DataFrame, asset: str) -> pd.DataFrame:\n",
    "        rename_map = {\n",
    "            \"midpoint\": asset,\n",
    "            \"buys\": f\"buys_{asset}\",\n",
    "            \"sells\": f\"sells_{asset}\",\n",
    "            \"spread\": f\"spread_{asset}\",\n",
    "        }\n",
    "        for i in range(book_levels):\n",
    "            rename_map[f\"bids_notional_{i}\"] = f\"bids_vol_{asset}_{i}\"\n",
    "            rename_map[f\"asks_notional_{i}\"] = f\"asks_vol_{asset}_{i}\"\n",
    "        return df_one.rename(columns=rename_map)\n",
    "\n",
    "    df_ada = rename_cols(load_asset(\"ADA\", freq, data_dir, book_levels, part=(0, 75)), \"ADA\")\n",
    "    df_btc = rename_cols(load_asset(\"BTC\", freq, data_dir, book_levels, part=(0, 75)), \"BTC\")\n",
    "    df_eth = rename_cols(load_asset(\"ETH\", freq, data_dir, book_levels, part=(0, 75)), \"ETH\")\n",
    "\n",
    "    df = df_ada.join(df_btc).join(df_eth).reset_index()\n",
    "    return df\n",
    "\n",
    "\n",
    "df = load_all_assets()\n",
    "for a in ASSETS:\n",
    "    df[f\"lr_{a}\"] = np.log(df[a]).diff().fillna(0.0)\n",
    "\n",
    "print(\"Loaded df:\", df.shape)\n",
    "print(\"Time range:\", df[\"timestamp\"].min(), \"->\", df[\"timestamp\"].max())\n",
    "print(df.head(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edge_feat shape: (12831, 9, 20) (T,E,edge_dim)\n",
      "edge_dim = 20\n",
      "Edge names: ['ADA->BTC', 'ADA->ETH', 'BTC->ADA', 'BTC->ETH', 'ETH->ADA', 'ETH->BTC', 'ADA->ADA', 'BTC->BTC', 'ETH->ETH'] ...\n",
      "edge_feat stats: mean= 0.4511896073818207 std= 0.500860333442688\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# Step 2: Edge features (rolling corr with lead-lag)\n",
    "# ======================================================================\n",
    "\n",
    "def _fisher_z(x: np.ndarray, eps: float = 1e-6) -> np.ndarray:\n",
    "    x = np.clip(x, -0.999, 0.999)\n",
    "    return 0.5 * np.log((1.0 + x + eps) / (1.0 - x + eps))\n",
    "\n",
    "\n",
    "def build_corr_array(\n",
    "    df_: pd.DataFrame,\n",
    "    corr_windows: List[int],\n",
    "    edges: List[Tuple[str, str]],\n",
    "    lags: List[int],\n",
    "    transform: str = \"fisher\",\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Edge features per time:\n",
    "      for edge s->t:\n",
    "        for lag in lags:\n",
    "          corr(lr_s.shift(lag), lr_t) over rolling window\n",
    "    No leakage: shift(lag>0) uses past of source.\n",
    "    Self-loop edges a->a: constant 1.0.\n",
    "    \"\"\"\n",
    "    T_ = len(df_)\n",
    "    E_ = len(edges)\n",
    "    W_ = len(corr_windows)\n",
    "    Lg = len(lags)\n",
    "    out = np.zeros((T_, E_, W_ * Lg), dtype=np.float32)\n",
    "\n",
    "    lr_map = {a: df_[f\"lr_{a}\"].astype(float) for a in ASSETS}\n",
    "\n",
    "    for ei, (s, t) in enumerate(edges):\n",
    "        if s == t:\n",
    "            out[:, ei, :] = 1.0\n",
    "            continue\n",
    "\n",
    "        src0 = lr_map[s]\n",
    "        dst0 = lr_map[t]\n",
    "\n",
    "        feat_idx = 0\n",
    "        for lag in lags:\n",
    "            src = src0.shift(int(lag)) if int(lag) > 0 else src0\n",
    "\n",
    "            for w in corr_windows:\n",
    "                r = src.rolling(int(w), min_periods=1).corr(dst0)\n",
    "                r = np.nan_to_num(r.to_numpy(dtype=np.float32), nan=0.0, posinf=0.0, neginf=0.0)\n",
    "                if transform == \"fisher\":\n",
    "                    r = _fisher_z(r).astype(np.float32)\n",
    "                out[:, ei, feat_idx] = r\n",
    "                feat_idx += 1\n",
    "\n",
    "    return out.astype(np.float32)\n",
    "\n",
    "\n",
    "edge_feat = build_corr_array(\n",
    "    df,\n",
    "    CFG[\"corr_windows\"],\n",
    "    EDGE_LIST,\n",
    "    CFG[\"corr_lags\"],\n",
    "    transform=str(CFG.get(\"edge_transform\", \"fisher\")),\n",
    ")\n",
    "\n",
    "print(\"edge_feat shape:\", edge_feat.shape, \"(T,E,edge_dim)\")\n",
    "print(\"edge_dim =\", edge_feat.shape[-1])\n",
    "print(\"Edge names:\", EDGE_NAMES[: min(10, len(EDGE_NAMES))], \"...\")\n",
    "print(\"edge_feat stats: mean=\", float(edge_feat.mean()), \"std=\", float(edge_feat.std()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TB dist [down,flat,up]: [2875 7413 2543]\n",
      "Trade ratio (true): 0.42225859247135844\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# Step 3: Triple-barrier labels\n",
    "# ======================================================================\n",
    "\n",
    "def triple_barrier_labels_from_lr(\n",
    "    lr: pd.Series,\n",
    "    horizon: int,\n",
    "    vol_window: int,\n",
    "    pt_mult: float,\n",
    "    sl_mult: float,\n",
    "    min_barrier: float,\n",
    "    max_barrier: float,\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      y_tb: {0=down, 1=flat/no-trade, 2=up}\n",
    "      exit_ret: realized log-return to exit (tp/sl/timeout)\n",
    "      exit_t: exit index\n",
    "      thr: barrier per t (float array, len T)\n",
    "    No leakage: vol is shift(1).\n",
    "    \"\"\"\n",
    "    lr = lr.astype(float).copy()\n",
    "    T = len(lr)\n",
    "\n",
    "    vol = lr.rolling(vol_window, min_periods=max(10, vol_window // 10)).std().shift(1)\n",
    "    thr = (vol * np.sqrt(horizon)).clip(lower=min_barrier, upper=max_barrier)\n",
    "\n",
    "    y = np.ones(T, dtype=np.int64)\n",
    "    exit_ret = np.zeros(T, dtype=np.float32)\n",
    "    exit_t = np.arange(T, dtype=np.int64)\n",
    "\n",
    "    lr_np = lr.fillna(0.0).to_numpy(dtype=np.float64)\n",
    "    thr_np = thr.fillna(min_barrier).to_numpy(dtype=np.float64)\n",
    "\n",
    "    for t in range(T - horizon - 1):\n",
    "        up = pt_mult * thr_np[t]\n",
    "        dn = -sl_mult * thr_np[t]\n",
    "\n",
    "        cum = 0.0\n",
    "        hit = 1\n",
    "        et = t + horizon\n",
    "        er = 0.0\n",
    "\n",
    "        for dt in range(1, horizon + 1):\n",
    "            cum += lr_np[t + dt]\n",
    "            if cum >= up:\n",
    "                hit, et, er = 2, t + dt, cum\n",
    "                break\n",
    "            if cum <= dn:\n",
    "                hit, et, er = 0, t + dt, cum\n",
    "                break\n",
    "\n",
    "        if hit == 1:\n",
    "            er = float(np.sum(lr_np[t + 1: t + horizon + 1]))\n",
    "            et = t + horizon\n",
    "\n",
    "        y[t] = hit\n",
    "        exit_ret[t] = er\n",
    "        exit_t[t] = et\n",
    "\n",
    "    return y, exit_ret, exit_t, thr_np\n",
    "\n",
    "\n",
    "y_tb, exit_ret, exit_t, tb_thr = triple_barrier_labels_from_lr(\n",
    "    df[\"lr_ETH\"],\n",
    "    horizon=CFG[\"tb_horizon\"],\n",
    "    vol_window=CFG[\"lookback\"],\n",
    "    pt_mult=CFG[\"tb_pt_mult\"],\n",
    "    sl_mult=CFG[\"tb_sl_mult\"],\n",
    "    min_barrier=CFG[\"tb_min_barrier\"],\n",
    "    max_barrier=CFG[\"tb_max_barrier\"],\n",
    ")\n",
    "\n",
    "y_trade = (y_tb != 1).astype(np.int64)  # 1=trade, 0=no-trade\n",
    "\n",
    "dist = np.bincount(y_tb, minlength=3)\n",
    "print(\"TB dist [down,flat,up]:\", dist)\n",
    "print(\"Trade ratio (true):\", float(y_trade.mean()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_node_raw: (12831, 3, 15) edge_feat: (12831, 9, 20)\n",
      "n_samples: 12561 | t range: 239 -> 12799\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# Step 4: Node features\n",
    "# ======================================================================\n",
    "\n",
    "EPS = 1e-6\n",
    "\n",
    "\n",
    "def safe_log1p(x: np.ndarray) -> np.ndarray:\n",
    "    return np.log1p(np.maximum(x, 0.0))\n",
    "\n",
    "\n",
    "def build_node_tensor(df_: pd.DataFrame) -> Tuple[np.ndarray, List[str]]:\n",
    "    \"\"\"\n",
    "    Features per asset:\n",
    "      lr, spread,\n",
    "      log_buys, log_sells, ofi,\n",
    "      DI_15,\n",
    "      DI_L0..DI_L4,\n",
    "      near_ratio_bid, near_ratio_ask,\n",
    "      di_near, di_far\n",
    "    \"\"\"\n",
    "    book_levels = CFG[\"book_levels\"]\n",
    "    top_k = CFG[\"top_levels\"]\n",
    "    near_k = CFG[\"near_levels\"]\n",
    "\n",
    "    if near_k >= book_levels:\n",
    "        raise ValueError(\"CFG['near_levels'] must be < CFG['book_levels']\")\n",
    "\n",
    "    feat_names = [\n",
    "        \"lr\", \"spread\",\n",
    "        \"log_buys\", \"log_sells\", \"ofi\",\n",
    "        \"DI_15\",\n",
    "        \"DI_L0\", \"DI_L1\", \"DI_L2\", \"DI_L3\", \"DI_L4\",\n",
    "        \"near_ratio_bid\", \"near_ratio_ask\",\n",
    "        \"di_near\", \"di_far\",\n",
    "    ]\n",
    "\n",
    "    feats_all = []\n",
    "    for a in ASSETS:\n",
    "        lr = df_[f\"lr_{a}\"].values.astype(np.float32)\n",
    "        spread = df_[f\"spread_{a}\"].values.astype(np.float32)\n",
    "\n",
    "        buys = df_[f\"buys_{a}\"].values.astype(np.float32)\n",
    "        sells = df_[f\"sells_{a}\"].values.astype(np.float32)\n",
    "\n",
    "        log_buys = safe_log1p(buys).astype(np.float32)\n",
    "        log_sells = safe_log1p(sells).astype(np.float32)\n",
    "\n",
    "        ofi = ((buys - sells) / (buys + sells + EPS)).astype(np.float32)\n",
    "\n",
    "        bids_lvls = np.stack([df_[f\"bids_vol_{a}_{i}\"].values.astype(np.float32) for i in range(book_levels)], axis=1)\n",
    "        asks_lvls = np.stack([df_[f\"asks_vol_{a}_{i}\"].values.astype(np.float32) for i in range(book_levels)], axis=1)\n",
    "\n",
    "        bid_sum = bids_lvls.sum(axis=1)\n",
    "        ask_sum = asks_lvls.sum(axis=1)\n",
    "        di_15 = ((bid_sum - ask_sum) / (bid_sum + ask_sum + EPS)).astype(np.float32)\n",
    "\n",
    "        di_levels = []\n",
    "        for i in range(top_k):\n",
    "            b = bids_lvls[:, i]\n",
    "            s = asks_lvls[:, i]\n",
    "            di_levels.append(((b - s) / (b + s + EPS)).astype(np.float32))\n",
    "        di_l0_4 = np.stack(di_levels, axis=1)  # (T,5)\n",
    "\n",
    "        bid_near = bids_lvls[:, :near_k].sum(axis=1)\n",
    "        ask_near = asks_lvls[:, :near_k].sum(axis=1)\n",
    "        bid_far = bids_lvls[:, near_k:].sum(axis=1)\n",
    "        ask_far = asks_lvls[:, near_k:].sum(axis=1)\n",
    "\n",
    "        near_ratio_bid = (bid_near / (bid_far + EPS)).astype(np.float32)\n",
    "        near_ratio_ask = (ask_near / (ask_far + EPS)).astype(np.float32)\n",
    "\n",
    "        di_near = ((bid_near - ask_near) / (bid_near + ask_near + EPS)).astype(np.float32)\n",
    "        di_far = ((bid_far - ask_far) / (bid_far + ask_far + EPS)).astype(np.float32)\n",
    "\n",
    "        Xa = np.column_stack([\n",
    "            lr, spread,\n",
    "            log_buys, log_sells, ofi,\n",
    "            di_15,\n",
    "            di_l0_4[:, 0], di_l0_4[:, 1], di_l0_4[:, 2], di_l0_4[:, 3], di_l0_4[:, 4],\n",
    "            near_ratio_bid, near_ratio_ask,\n",
    "            di_near, di_far,\n",
    "        ]).astype(np.float32)\n",
    "\n",
    "        feats_all.append(Xa)\n",
    "\n",
    "    X = np.stack(feats_all, axis=1).astype(np.float32)  # (T,N,F)\n",
    "    return X, feat_names\n",
    "\n",
    "\n",
    "X_node_raw, node_feat_names = build_node_tensor(df)\n",
    "\n",
    "T = len(df)\n",
    "L = CFG[\"lookback\"]\n",
    "H = CFG[\"tb_horizon\"]\n",
    "\n",
    "t_min = L - 1\n",
    "t_max = T - H - 2\n",
    "sample_t = np.arange(t_min, t_max + 1)\n",
    "n_samples = len(sample_t)\n",
    "\n",
    "print(\"X_node_raw:\", X_node_raw.shape, \"edge_feat:\", edge_feat.shape)\n",
    "print(\"n_samples:\", n_samples, \"| t range:\", int(sample_t[0]), \"->\", int(sample_t[-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holdout split:\n",
      "  n_samples total: 12561\n",
      "  n_samples CV   : 11305\n",
      "  n_samples FINAL: 1256\n",
      "\n",
      "Walk-forward folds: 4\n",
      "  fold 1: train=5652 | val=1130 | test=1130\n",
      "  fold 2: train=6782 | val=1130 | test=1130\n",
      "  fold 3: train=7912 | val=1130 | test=1130\n",
      "  fold 4: train=9042 | val=1130 | test=1130\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# Step 5: Splits\n",
    "# ======================================================================\n",
    "\n",
    "def make_final_holdout_split(n_samples_: int, final_test_frac: float) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    if not (0.0 < final_test_frac < 0.5):\n",
    "        raise ValueError(\"final_test_frac should be in (0, 0.5)\")\n",
    "    n_final = max(1, int(round(final_test_frac * n_samples_)))\n",
    "    n_cv = n_samples_ - n_final\n",
    "    if n_cv <= 50:\n",
    "        raise ValueError(\"Too few samples left for CV after holdout split.\")\n",
    "    idx_cv = np.arange(0, n_cv, dtype=np.int64)\n",
    "    idx_final = np.arange(n_cv, n_samples_, dtype=np.int64)\n",
    "    return idx_cv, idx_final\n",
    "\n",
    "\n",
    "def make_walk_forward_splits(\n",
    "    n_samples_: int,\n",
    "    train_min_frac: float,\n",
    "    val_window_frac: float,\n",
    "    test_window_frac: float,\n",
    "    step_window_frac: float,\n",
    ") -> List[Tuple[np.ndarray, np.ndarray, np.ndarray]]:\n",
    "    train_min = int(train_min_frac * n_samples_)\n",
    "    val_w = max(1, int(val_window_frac * n_samples_))\n",
    "    test_w = max(1, int(test_window_frac * n_samples_))\n",
    "    step_w = max(1, int(step_window_frac * n_samples_))\n",
    "\n",
    "    splits = []\n",
    "    start = train_min\n",
    "    while True:\n",
    "        tr_end = start\n",
    "        va_end = tr_end + val_w\n",
    "        te_end = va_end + test_w\n",
    "        if te_end > n_samples_:\n",
    "            break\n",
    "\n",
    "        idx_train = np.arange(0, tr_end, dtype=np.int64)\n",
    "        idx_val = np.arange(tr_end, va_end, dtype=np.int64)\n",
    "        idx_test = np.arange(va_end, te_end, dtype=np.int64)\n",
    "        splits.append((idx_train, idx_val, idx_test))\n",
    "\n",
    "        start += step_w\n",
    "\n",
    "    return splits\n",
    "\n",
    "\n",
    "idx_cv_all, idx_final_test = make_final_holdout_split(n_samples, CFG[\"final_test_frac\"])\n",
    "n_samples_cv = len(idx_cv_all)\n",
    "\n",
    "walk_splits = make_walk_forward_splits(\n",
    "    n_samples_=n_samples_cv,\n",
    "    train_min_frac=CFG[\"train_min_frac\"],\n",
    "    val_window_frac=CFG[\"val_window_frac\"],\n",
    "    test_window_frac=CFG[\"test_window_frac\"],\n",
    "    step_window_frac=CFG[\"step_window_frac\"],\n",
    ")\n",
    "\n",
    "print(\"Holdout split:\")\n",
    "print(f\"  n_samples total: {n_samples}\")\n",
    "print(f\"  n_samples CV   : {len(idx_cv_all)}\")\n",
    "print(f\"  n_samples FINAL: {len(idx_final_test)}\")\n",
    "print(\"\\nWalk-forward folds:\", len(walk_splits))\n",
    "for i, (a, b, c) in enumerate(walk_splits, 1):\n",
    "    print(f\"  fold {i}: train={len(a)} | val={len(b)} | test={len(c)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# Step 6: Dataset, scaling helpers, sampler\n",
    "# ======================================================================\n",
    "\n",
    "class LobGraphSequenceDataset3Class(Dataset):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      x_seq:    (L,N,F)\n",
    "      e_seq:    (L,E,D)\n",
    "      y_tb:     scalar in {0,1,2} (SHORT, FLAT, LONG)\n",
    "      exit_ret: scalar (log-return to exit)\n",
    "      sidx:     scalar sample index (for time-order reconstruction)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        X_node: np.ndarray,\n",
    "        E_feat: np.ndarray,\n",
    "        y_tb_arr: np.ndarray,\n",
    "        exit_ret_arr: np.ndarray,\n",
    "        sample_t_: np.ndarray,\n",
    "        indices: np.ndarray,\n",
    "        lookback: int,\n",
    "    ):\n",
    "        self.X_node = X_node\n",
    "        self.E_feat = E_feat\n",
    "        self.y_tb = y_tb_arr\n",
    "        self.exit_ret = exit_ret_arr\n",
    "        self.sample_t = sample_t_\n",
    "        self.indices = indices.astype(np.int64)\n",
    "        self.L = int(lookback)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return int(len(self.indices))\n",
    "\n",
    "    def __getitem__(self, i: int):\n",
    "        sidx = int(self.indices[i])\n",
    "        t = int(self.sample_t[sidx])\n",
    "        t0 = t - self.L + 1\n",
    "\n",
    "        x_seq = self.X_node[t0:t + 1]  # (L,N,F)\n",
    "        e_seq = self.E_feat[t0:t + 1]  # (L,E,D)\n",
    "        y = int(self.y_tb[t])\n",
    "        er = float(self.exit_ret[t])\n",
    "\n",
    "        return (\n",
    "            torch.from_numpy(x_seq),\n",
    "            torch.from_numpy(e_seq),\n",
    "            torch.tensor(y, dtype=torch.long),\n",
    "            torch.tensor(er, dtype=torch.float32),\n",
    "            torch.tensor(sidx, dtype=torch.long),\n",
    "        )\n",
    "\n",
    "\n",
    "def collate_fn_3class(batch):\n",
    "    xs, es, ys, ers, sidxs = zip(*batch)\n",
    "    return (\n",
    "        torch.stack(xs, 0),    # (B,L,N,F)\n",
    "        torch.stack(es, 0),    # (B,L,E,D)\n",
    "        torch.stack(ys, 0),    # (B,)\n",
    "        torch.stack(ers, 0),   # (B,)\n",
    "        torch.stack(sidxs, 0), # (B,)\n",
    "    )\n",
    "\n",
    "\n",
    "def make_weighted_sampler_3class(y_np: np.ndarray) -> WeightedRandomSampler:\n",
    "    y_np = np.asarray(y_np, dtype=np.int64)\n",
    "    counts = np.bincount(y_np, minlength=3).astype(np.float64)\n",
    "    counts = np.maximum(counts, 1.0)\n",
    "    class_w = counts.sum() / (3.0 * counts)\n",
    "    sample_w = class_w[y_np].astype(np.float64)\n",
    "    sample_w = torch.tensor(sample_w, dtype=torch.double)\n",
    "    return WeightedRandomSampler(weights=sample_w, num_samples=len(sample_w), replacement=True)\n",
    "\n",
    "\n",
    "def make_ce_weights_3class(y_np: np.ndarray) -> torch.Tensor:\n",
    "    y_np = np.asarray(y_np, dtype=np.int64)\n",
    "    counts = np.bincount(y_np, minlength=3).astype(np.float64)\n",
    "    counts = np.maximum(counts, 1.0)\n",
    "    w = counts.sum() / (3.0 * counts)\n",
    "    return torch.tensor(w, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "\n",
    "def fit_scale_nodes_train_only(\n",
    "    X_node_raw_: np.ndarray,\n",
    "    sample_t_: np.ndarray,\n",
    "    idx_train: np.ndarray,\n",
    "    max_abs: float = 10.0\n",
    ") -> Tuple[np.ndarray, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Fit RobustScaler on train timeline only. Returns scaled X and scaler params dict.\n",
    "    \"\"\"\n",
    "    last_train_t = int(sample_t_[int(idx_train[-1])])\n",
    "    train_time_mask = np.arange(0, last_train_t + 1)\n",
    "\n",
    "    X_train_time = X_node_raw_[train_time_mask]  # (Ttr,N,F)\n",
    "    _, _, Fdim = X_train_time.shape\n",
    "\n",
    "    scaler = RobustScaler(with_centering=True, with_scaling=True, quantile_range=(5.0, 95.0))\n",
    "    scaler.fit(X_train_time.reshape(-1, Fdim))\n",
    "\n",
    "    X_scaled = scaler.transform(X_node_raw_.reshape(-1, Fdim)).reshape(X_node_raw_.shape).astype(np.float32)\n",
    "    X_scaled = np.clip(X_scaled, -max_abs, max_abs).astype(np.float32)\n",
    "    X_scaled = np.nan_to_num(X_scaled, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n",
    "\n",
    "    params = {\n",
    "        \"center_\": scaler.center_.astype(np.float32),\n",
    "        \"scale_\": scaler.scale_.astype(np.float32),\n",
    "        \"max_abs\": float(max_abs),\n",
    "    }\n",
    "    return X_scaled, params\n",
    "\n",
    "\n",
    "def fit_scale_edges_train_only(\n",
    "    E_raw_: np.ndarray,\n",
    "    sample_t_: np.ndarray,\n",
    "    idx_train: np.ndarray,\n",
    "    max_abs: float = 6.0\n",
    ") -> Tuple[np.ndarray, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Fit RobustScaler on train timeline only for edge features. Returns scaled E and scaler params dict.\n",
    "    \"\"\"\n",
    "    last_train_t = int(sample_t_[int(idx_train[-1])])\n",
    "    train_time_mask = np.arange(0, last_train_t + 1)\n",
    "\n",
    "    E_train_time = E_raw_[train_time_mask]  # (Ttr,E,D)\n",
    "    _, _, D = E_train_time.shape\n",
    "\n",
    "    scaler = RobustScaler(with_centering=True, with_scaling=True, quantile_range=(5.0, 95.0))\n",
    "    scaler.fit(E_train_time.reshape(-1, D))\n",
    "\n",
    "    E_scaled = scaler.transform(E_raw_.reshape(-1, D)).reshape(E_raw_.shape).astype(np.float32)\n",
    "    E_scaled = np.clip(E_scaled, -max_abs, max_abs).astype(np.float32)\n",
    "    E_scaled = np.nan_to_num(E_scaled, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n",
    "\n",
    "    params = {\n",
    "        \"center_\": scaler.center_.astype(np.float32),\n",
    "        \"scale_\": scaler.scale_.astype(np.float32),\n",
    "        \"max_abs\": float(max_abs),\n",
    "    }\n",
    "    return E_scaled, params\n",
    "\n",
    "\n",
    "def apply_scaler_params(X: np.ndarray, params: Dict[str, Any]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Apply stored RobustScaler params (center_, scale_) + clip.\n",
    "    \"\"\"\n",
    "    center = np.asarray(params[\"center_\"], dtype=np.float32)\n",
    "    scale = np.asarray(params[\"scale_\"], dtype=np.float32)\n",
    "    max_abs = float(params[\"max_abs\"])\n",
    "\n",
    "    # Broadcast over last dim\n",
    "    X2 = (X - center) / (scale + 1e-12)\n",
    "    X2 = np.clip(X2, -max_abs, max_abs)\n",
    "    X2 = np.nan_to_num(X2, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n",
    "    return X2\n",
    "\n",
    "\n",
    "def split_trade_ratio(indices: np.ndarray, sample_t_: np.ndarray, y_trade_arr: np.ndarray) -> float:\n",
    "    tt = sample_t_[indices]\n",
    "    return float(y_trade_arr[tt].mean()) if len(tt) else float(\"nan\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# Step 7: Graph WaveNet model (multi-task: logits + ret_hat)\n",
    "# ======================================================================\n",
    "\n",
    "def build_static_adjacency_from_edges(edge_index: torch.Tensor, n_nodes: int, eps: float = 1e-8) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Build A_static (N,N), row-normalized, using the presence of edges in EDGE_LIST.\n",
    "    \"\"\"\n",
    "    A = torch.zeros((n_nodes, n_nodes), dtype=torch.float32)\n",
    "    src = edge_index[:, 0].long()\n",
    "    dst = edge_index[:, 1].long()\n",
    "    A[src, dst] = 1.0\n",
    "    A = A / (A.sum(dim=-1, keepdim=True) + eps)\n",
    "    return A\n",
    "\n",
    "\n",
    "def build_adj_prior_from_edge_attr(\n",
    "    edge_attr_last: torch.Tensor,    # (B,E,D)\n",
    "    edge_index: torch.Tensor,        # (E,2) [src,dst]\n",
    "    n_nodes: int,\n",
    "    use_abs: bool = False,\n",
    "    diag_boost: float = 1.0,\n",
    "    row_normalize: bool = True,\n",
    "    eps: float = 1e-8\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Build A_prior (B,N,N) from edge_attr at the last timestep.\n",
    "      w = sigmoid(mean(edge_attr)) in [0,1]\n",
    "    \"\"\"\n",
    "    edge_attr_last = torch.nan_to_num(edge_attr_last, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    B, E, D = edge_attr_last.shape\n",
    "    r = edge_attr_last.mean(dim=-1)  # (B,E)\n",
    "    if use_abs:\n",
    "        r = r.abs()\n",
    "    w = torch.sigmoid(r)  # (B,E)\n",
    "\n",
    "    A = torch.zeros((B, n_nodes, n_nodes), device=edge_attr_last.device, dtype=edge_attr_last.dtype)\n",
    "    src = edge_index[:, 0].to(edge_attr_last.device)\n",
    "    dst = edge_index[:, 1].to(edge_attr_last.device)\n",
    "    A[:, src, dst] = w\n",
    "\n",
    "    diag = torch.arange(n_nodes, device=edge_attr_last.device)\n",
    "    A[:, diag, diag] = torch.maximum(A[:, diag, diag], torch.full_like(A[:, diag, diag], float(diag_boost)))\n",
    "\n",
    "    if row_normalize:\n",
    "        A = A / (A.sum(dim=-1, keepdim=True) + eps)\n",
    "\n",
    "    return torch.nan_to_num(A, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "\n",
    "class AdaptiveAdjacency(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph WaveNet-style adaptive adjacency from node embeddings:\n",
    "      logits = relu(E1 @ E2^T) / temp\n",
    "      (optional) top-k per row\n",
    "      A_adapt = softmax(logits_row)\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes: int, cfg: Dict[str, Any]):\n",
    "        super().__init__()\n",
    "        self.n = int(n_nodes)\n",
    "        k = int(cfg.get(\"adj_emb_dim\", 8))\n",
    "        self.E1 = nn.Parameter(0.01 * torch.randn(self.n, k))\n",
    "        self.E2 = nn.Parameter(0.01 * torch.randn(self.n, k))\n",
    "        temp = float(cfg.get(\"adj_temperature\", 1.0))\n",
    "        self.temp = max(temp, 1e-3)\n",
    "        self.topk = int(cfg.get(\"adaptive_topk\", self.n))\n",
    "\n",
    "    def forward(self) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        logits = (self.E1 @ self.E2.t())\n",
    "        logits = F.relu(logits) / self.temp  # (N,N)\n",
    "\n",
    "        if self.topk is not None and 0 < self.topk < self.n:\n",
    "            vals, idx = torch.topk(logits, k=self.topk, dim=-1)\n",
    "            mask = torch.full_like(logits, fill_value=float(\"-inf\"))\n",
    "            mask.scatter_(-1, idx, vals)\n",
    "            logits = mask\n",
    "\n",
    "        A = torch.softmax(logits, dim=-1)  # row-stochastic\n",
    "        sparsity_proxy = torch.sigmoid(logits)\n",
    "        return A, sparsity_proxy, logits\n",
    "\n",
    "\n",
    "class LearnableSupportMix(nn.Module):\n",
    "    \"\"\"\n",
    "    Blend supports (static, prior, adapt) using softmax weights.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_supports: int = 3):\n",
    "        super().__init__()\n",
    "        self.w_logits = nn.Parameter(torch.zeros(n_supports, dtype=torch.float32))\n",
    "\n",
    "    def forward(self) -> torch.Tensor:\n",
    "        return torch.softmax(self.w_logits, dim=0)\n",
    "\n",
    "\n",
    "class CausalConv2dTime(nn.Module):\n",
    "    \"\"\"\n",
    "    2D convolution causal along time dimension only.\n",
    "    Input:  (B,C,N,T)\n",
    "    Conv kernel: (1,k), dilation: (1,d)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch: int, out_ch: int, kernel_size: int, dilation: int):\n",
    "        super().__init__()\n",
    "        self.k = int(kernel_size)\n",
    "        self.d = int(dilation)\n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, kernel_size=(1, self.k), dilation=(1, self.d))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        pad_left = (self.k - 1) * self.d\n",
    "        x = F.pad(x, (pad_left, 0, 0, 0))\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "def graph_message_passing(x: torch.Tensor, A: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    x: (B,C,N,T)\n",
    "    A: (B,N,N) with A[src,dst]\n",
    "    returns: (B,C,N,T) where dst aggregates from src\n",
    "    \"\"\"\n",
    "    return torch.einsum(\"bcnt,bnm->bcmt\", x, A)\n",
    "\n",
    "\n",
    "class GraphWaveNetBlock(nn.Module):\n",
    "    def __init__(self, residual_ch: int, dilation_ch: int, skip_ch: int, kernel_size: int, dilation: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.filter_conv = CausalConv2dTime(residual_ch, dilation_ch, kernel_size=kernel_size, dilation=dilation)\n",
    "        self.gate_conv = CausalConv2dTime(residual_ch, dilation_ch, kernel_size=kernel_size, dilation=dilation)\n",
    "\n",
    "        self.residual_conv = nn.Conv2d(dilation_ch, residual_ch, kernel_size=(1, 1))\n",
    "        self.skip_conv = nn.Conv2d(dilation_ch, skip_ch, kernel_size=(1, 1))\n",
    "\n",
    "        self.dropout = nn.Dropout(float(dropout))\n",
    "        self.bn = nn.BatchNorm2d(residual_ch)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, A: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        x = torch.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        A = torch.nan_to_num(A, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "        residual = x\n",
    "\n",
    "        f = torch.tanh(self.filter_conv(x))\n",
    "        g = torch.sigmoid(self.gate_conv(x))\n",
    "        z = f * g  # (B,dilation_ch,N,T)\n",
    "\n",
    "        z = self.dropout(z)\n",
    "\n",
    "        skip = self.skip_conv(z)  # (B,skip_ch,N,T)\n",
    "\n",
    "        out = self.residual_conv(z)  # (B,residual_ch,N,T)\n",
    "        out = graph_message_passing(out, A)\n",
    "        out = out + residual\n",
    "        out = self.bn(out)\n",
    "\n",
    "        out = torch.nan_to_num(out, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        skip = torch.nan_to_num(skip, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        return out, skip\n",
    "\n",
    "\n",
    "class GraphWaveNetMultiTask(nn.Module):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "      x_seq: (B,L,N,F)\n",
    "      e_seq: (B,L,E,D)   (only used to build A_prior from last step)\n",
    "\n",
    "    Outputs for target node only:\n",
    "      logits:  (B,3)  classes [SHORT, FLAT, LONG]\n",
    "      ret_hat: (B,)   predicted exit_ret (regression head)\n",
    "    \"\"\"\n",
    "    def __init__(self, node_in: int, edge_dim: int, cfg: Dict[str, Any], n_nodes: int, target_node: int):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.n_nodes = int(n_nodes)\n",
    "        self.target_node = int(target_node)\n",
    "\n",
    "        residual_ch = int(cfg[\"gwn_residual_channels\"])\n",
    "        dilation_ch = int(cfg[\"gwn_dilation_channels\"])\n",
    "        skip_ch = int(cfg[\"gwn_skip_channels\"])\n",
    "        end_ch = int(cfg[\"gwn_end_channels\"])\n",
    "        k = int(cfg[\"gwn_kernel_size\"])\n",
    "        blocks = int(cfg[\"gwn_blocks\"])\n",
    "        layers_per_block = int(cfg[\"gwn_layers_per_block\"])\n",
    "        drop = float(cfg.get(\"dropout\", 0.0))\n",
    "\n",
    "        self.in_proj = nn.Linear(int(node_in), residual_ch)\n",
    "\n",
    "        A_static = build_static_adjacency_from_edges(EDGE_INDEX, n_nodes=self.n_nodes)\n",
    "        self.register_buffer(\"A_static\", A_static)\n",
    "\n",
    "        self.adapt = AdaptiveAdjacency(n_nodes=self.n_nodes, cfg=cfg)\n",
    "        self.support_mix = LearnableSupportMix(n_supports=3)\n",
    "\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for _b in range(blocks):\n",
    "            for l in range(layers_per_block):\n",
    "                dilation = 2 ** l\n",
    "                self.blocks.append(GraphWaveNetBlock(\n",
    "                    residual_ch=residual_ch,\n",
    "                    dilation_ch=dilation_ch,\n",
    "                    skip_ch=skip_ch,\n",
    "                    kernel_size=k,\n",
    "                    dilation=dilation,\n",
    "                    dropout=drop,\n",
    "                ))\n",
    "\n",
    "        self.end1 = nn.Conv2d(skip_ch, end_ch, kernel_size=(1, 1))\n",
    "        self.end2 = nn.Conv2d(end_ch, 3, kernel_size=(1, 1))  # 3-class logits\n",
    "\n",
    "        # regression head on end_ch features for target node at last time\n",
    "        self.ret_head = nn.Linear(end_ch, 1)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def _compute_supports(self, e_seq: torch.Tensor) -> Tuple[torch.Tensor, Dict[str, Any]]:\n",
    "        B, L_, E, D = e_seq.shape\n",
    "        e_last = e_seq[:, -1, :, :]  # (B,E,D)\n",
    "\n",
    "        A_prior = build_adj_prior_from_edge_attr(\n",
    "            edge_attr_last=e_last,\n",
    "            edge_index=EDGE_INDEX.to(e_seq.device),\n",
    "            n_nodes=self.n_nodes,\n",
    "            use_abs=bool(self.cfg.get(\"prior_use_abs\", False)),\n",
    "            diag_boost=float(self.cfg.get(\"prior_diag_boost\", 1.0)),\n",
    "            row_normalize=bool(self.cfg.get(\"prior_row_normalize\", True)),\n",
    "        )  # (B,N,N)\n",
    "\n",
    "        A_adapt_base, sparsity_proxy, _adapt_logits = self.adapt()  # (N,N)\n",
    "        A_adapt = A_adapt_base.unsqueeze(0).expand(B, -1, -1)       # (B,N,N)\n",
    "\n",
    "        w = self.support_mix()  # (3,)\n",
    "        A_static = self.A_static.to(e_seq.device).to(e_seq.dtype).unsqueeze(0).expand(B, -1, -1)\n",
    "\n",
    "        A_mix = w[0] * A_static + w[1] * A_prior + w[2] * A_adapt\n",
    "        A_mix = A_mix / (A_mix.sum(dim=-1, keepdim=True) + 1e-8)\n",
    "\n",
    "        # adjacency regularizers (adapt only, off-diagonal)\n",
    "        N = self.n_nodes\n",
    "        offdiag = (1.0 - torch.eye(N, device=e_seq.device, dtype=e_seq.dtype))\n",
    "        l1_off = (sparsity_proxy.to(e_seq.dtype) * offdiag).abs().mean()\n",
    "        mse_prior = ((A_adapt - A_prior) ** 2 * offdiag).mean()\n",
    "\n",
    "        aux = {\n",
    "            \"support_w\": w.detach().cpu().numpy().tolist(),\n",
    "            \"l1_off\": float(l1_off.detach().cpu().item()),\n",
    "            \"mse_prior\": float(mse_prior.detach().cpu().item()),\n",
    "            \"_l1_off_t\": l1_off,\n",
    "            \"_mse_prior_t\": mse_prior,\n",
    "        }\n",
    "        return A_mix, aux\n",
    "\n",
    "    def forward(self, x_seq: torch.Tensor, e_seq: torch.Tensor, return_aux: bool = False):\n",
    "        x_seq = torch.nan_to_num(x_seq, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        e_seq = torch.nan_to_num(e_seq, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "        B, L_, N, Fdim = x_seq.shape\n",
    "        assert N == self.n_nodes\n",
    "\n",
    "        # (B,L,N,F) -> (B,C,N,T)\n",
    "        x = self.in_proj(x_seq)              # (B,L,N,C)\n",
    "        x = x.permute(0, 3, 2, 1).contiguous()  # (B,C,N,T)\n",
    "\n",
    "        A_mix, aux = self._compute_supports(e_seq)\n",
    "\n",
    "        skip_sum = None\n",
    "        for blk in self.blocks:\n",
    "            x, skip = blk(x, A_mix)\n",
    "            skip_sum = skip if skip_sum is None else (skip_sum + skip)\n",
    "\n",
    "        y = F.relu(skip_sum)\n",
    "        y_end = F.relu(self.end1(y))  # (B,end_ch,N,T)\n",
    "        logits_all = self.end2(y_end)  # (B,3,N,T)\n",
    "\n",
    "        logits = logits_all[:, :, self.target_node, -1]  # (B,3)\n",
    "        feat = y_end[:, :, self.target_node, -1]         # (B,end_ch)\n",
    "        ret_hat = self.ret_head(feat).squeeze(-1)        # (B,)\n",
    "\n",
    "        logits = torch.nan_to_num(logits, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        ret_hat = torch.nan_to_num(ret_hat, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "        if return_aux:\n",
    "            return logits, ret_hat, aux\n",
    "        return logits, ret_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# Step 8: Metrics, PnL helpers, and multi-task loss\n",
    "# ======================================================================\n",
    "\n",
    "def _safe_auc_binary(y_true: np.ndarray, score: np.ndarray) -> float:\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    score = np.asarray(score, dtype=np.float64)\n",
    "    if y_true.size == 0 or len(np.unique(y_true)) < 2:\n",
    "        return float(\"nan\")\n",
    "    return float(roc_auc_score(y_true, score))\n",
    "\n",
    "\n",
    "def compute_trade_dir_auc_from_probs(y_tb_true: np.ndarray, prob3: np.ndarray) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    trade_auc: trade vs no-trade where trade={SHORT,LONG} vs FLAT\n",
    "              score = 1 - p_flat\n",
    "    dir_auc: LONG vs SHORT on true-trade samples only\n",
    "             score = p_long/(p_long+p_short)\n",
    "    \"\"\"\n",
    "    y_tb_true = np.asarray(y_tb_true, dtype=np.int64)\n",
    "    prob3 = np.asarray(prob3, dtype=np.float64)\n",
    "\n",
    "    y_trade_bin = (y_tb_true != 1).astype(np.int64)\n",
    "    p_trade = 1.0 - prob3[:, 1]\n",
    "    trade_auc = _safe_auc_binary(y_trade_bin, p_trade)\n",
    "\n",
    "    mask_trade = (y_tb_true != 1)\n",
    "    y_dir_bin = (y_tb_true[mask_trade] == 2).astype(np.int64)  # 1=LONG, 0=SHORT\n",
    "    p_short = prob3[mask_trade, 0]\n",
    "    p_long = prob3[mask_trade, 2]\n",
    "    p_dir = p_long / (p_long + p_short + 1e-12)\n",
    "    dir_auc = _safe_auc_binary(y_dir_bin, p_dir)\n",
    "\n",
    "    return trade_auc, dir_auc\n",
    "\n",
    "\n",
    "def pnl_from_probs_3class(prob3: np.ndarray, exit_ret_arr: np.ndarray, thr_trade: float, thr_dir: float, cost_bps: float) -> Dict[str, Any]:\n",
    "    prob3 = np.asarray(prob3, dtype=np.float64)\n",
    "    exit_ret_arr = np.asarray(exit_ret_arr, dtype=np.float64)\n",
    "\n",
    "    p_short = prob3[:, 0]\n",
    "    p_flat = prob3[:, 1]\n",
    "    p_long = prob3[:, 2]\n",
    "\n",
    "    trade_conf = 1.0 - p_flat\n",
    "    dir_prob = p_long / (p_long + p_short + 1e-12)\n",
    "    dir_conf = np.maximum(dir_prob, 1.0 - dir_prob)\n",
    "\n",
    "    mask = (trade_conf >= float(thr_trade)) & (dir_conf >= float(thr_dir))\n",
    "\n",
    "    action = np.zeros_like(exit_ret_arr, dtype=np.float64)\n",
    "    action[mask] = np.where(dir_prob[mask] >= 0.5, 1.0, -1.0)\n",
    "\n",
    "    cost = (float(cost_bps) * 1e-4) * mask.astype(np.float64)\n",
    "    pnl = action * exit_ret_arr - cost\n",
    "\n",
    "    n = int(len(exit_ret_arr))\n",
    "    n_tr = int(mask.sum())\n",
    "\n",
    "    return {\n",
    "        \"n\": n,\n",
    "        \"n_trades\": n_tr,\n",
    "        \"trade_rate\": float(n_tr / max(1, n)),\n",
    "        \"pnl_sum\": float(pnl.sum()),\n",
    "        \"pnl_mean\": float(pnl.mean()) if n else float(\"nan\"),\n",
    "        \"pnl_per_trade\": float(pnl.sum() / max(1, n_tr)),\n",
    "        \"pnl_sharpe\": float((pnl.mean() / (pnl.std() + 1e-12)) * np.sqrt(288)) if n else float(\"nan\"),\n",
    "    }\n",
    "\n",
    "\n",
    "def build_trade_threshold_grid(p_trade: np.ndarray, base_grid: Optional[List[float]], target_trades_list: Optional[List[int]]) -> List[float]:\n",
    "    p_trade = np.asarray(p_trade, dtype=np.float64)\n",
    "    p_trade = p_trade[np.isfinite(p_trade)]\n",
    "    if p_trade.size == 0:\n",
    "        return base_grid or [0.5]\n",
    "\n",
    "    thrs = set(float(t) for t in (base_grid or []))\n",
    "\n",
    "    if target_trades_list:\n",
    "        N = int(p_trade.size)\n",
    "        for k in target_trades_list:\n",
    "            k = int(k)\n",
    "            if k <= 0:\n",
    "                continue\n",
    "            if k >= N:\n",
    "                thr = float(np.min(p_trade))\n",
    "            else:\n",
    "                q = 1.0 - (k / N)\n",
    "                thr = float(np.quantile(p_trade, q))\n",
    "            thrs.add(float(np.clip(thr, 0.01, 0.99)))\n",
    "\n",
    "    out = sorted(thrs)\n",
    "    cleaned = []\n",
    "    for t in out:\n",
    "        if not cleaned or abs(t - cleaned[-1]) > 1e-6:\n",
    "            cleaned.append(float(t))\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def sweep_thresholds_3class(prob3: np.ndarray, exit_ret_arr: np.ndarray, cfg: Dict[str, Any], min_trades: int, target_trade_rate: Optional[float]) -> pd.DataFrame:\n",
    "    prob3 = np.asarray(prob3, dtype=np.float64)\n",
    "    p_flat = prob3[:, 1]\n",
    "    p_trade = 1.0 - p_flat\n",
    "\n",
    "    thr_trade_grid = build_trade_threshold_grid(\n",
    "        p_trade=p_trade,\n",
    "        base_grid=cfg.get(\"thr_trade_grid\", [0.5]),\n",
    "        target_trades_list=cfg.get(\"proxy_target_trades\", None),\n",
    "    )\n",
    "    thr_dir_grid = cfg.get(\"thr_dir_grid\", [0.5])\n",
    "\n",
    "    obj = str(cfg.get(\"thr_objective\", \"pnl_sum\"))\n",
    "    max_rate = cfg.get(\"max_trade_rate_val\", None)\n",
    "    penalty = float(cfg.get(\"trade_rate_penalty\", 0.0))\n",
    "\n",
    "    rows = []\n",
    "    for thr_t in thr_trade_grid:\n",
    "        for thr_d in thr_dir_grid:\n",
    "            m = pnl_from_probs_3class(prob3, exit_ret_arr, thr_t, thr_d, cfg[\"cost_bps\"])\n",
    "            if int(m[\"n_trades\"]) < int(min_trades):\n",
    "                continue\n",
    "            if max_rate is not None and float(m[\"trade_rate\"]) > float(max_rate):\n",
    "                continue\n",
    "\n",
    "            base = float(m.get(obj, np.nan))\n",
    "            if not np.isfinite(base):\n",
    "                continue\n",
    "\n",
    "            if target_trade_rate is not None:\n",
    "                score = base - penalty * abs(float(m[\"trade_rate\"]) - float(target_trade_rate))\n",
    "            else:\n",
    "                score = base - penalty * float(m[\"trade_rate\"])\n",
    "\n",
    "            rows.append({\"thr_trade\": float(thr_t), \"thr_dir\": float(thr_d), \"score\": float(score), **m})\n",
    "\n",
    "    if not rows:\n",
    "        return sweep_thresholds_3class(prob3, exit_ret_arr, cfg, min_trades=1, target_trade_rate=target_trade_rate)\n",
    "\n",
    "    return pd.DataFrame(rows).sort_values([\"score\", \"pnl_sum\"], ascending=False)\n",
    "\n",
    "\n",
    "def total_loss_with_adj_reg(loss: torch.Tensor, aux: Dict[str, Any], cfg: Dict[str, Any]) -> torch.Tensor:\n",
    "    lam_l1 = float(cfg.get(\"adj_l1_lambda\", 0.0))\n",
    "    lam_pr = float(cfg.get(\"adj_prior_lambda\", 0.0))\n",
    "    reg = 0.0\n",
    "    if lam_l1 > 0:\n",
    "        reg = reg + lam_l1 * aux[\"_l1_off_t\"]\n",
    "    if lam_pr > 0:\n",
    "        reg = reg + lam_pr * aux[\"_mse_prior_t\"]\n",
    "    return loss + reg\n",
    "\n",
    "\n",
    "def multitask_loss(\n",
    "    logits: torch.Tensor,\n",
    "    ret_hat: torch.Tensor,\n",
    "    y: torch.Tensor,\n",
    "    exit_ret_batch: torch.Tensor,\n",
    "    ce_loss_fn: nn.Module,\n",
    "    cfg: Dict[str, Any],\n",
    ") -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    Total = w_ce * CE + w_ret * Huber(ret_hat, exit_ret) + w_util * (-utility)\n",
    "    utility uses a soft position derived from probs:\n",
    "      pos = trade_conf * tanh(k*(p_long - p_short))\n",
    "      utility = pos * exit_ret - fee * |pos|\n",
    "    \"\"\"\n",
    "    w_ce = float(cfg[\"loss_w_ce\"])\n",
    "    w_ret = float(cfg[\"loss_w_ret\"])\n",
    "    w_ut = float(cfg[\"loss_w_utility\"])\n",
    "\n",
    "    # CE\n",
    "    ce = ce_loss_fn(logits, y)\n",
    "\n",
    "    # Clip exit_ret for stability in ret/utility terms\n",
    "    clip_val = float(cfg.get(\"exit_ret_clip\", 0.0))\n",
    "    if clip_val and clip_val > 0:\n",
    "        er = torch.clamp(exit_ret_batch, -clip_val, clip_val)\n",
    "    else:\n",
    "        er = exit_ret_batch\n",
    "\n",
    "    # Regression (Huber)\n",
    "    delta = float(cfg.get(\"ret_huber_delta\", 0.01))\n",
    "    huber = F.huber_loss(ret_hat, er, delta=delta)\n",
    "\n",
    "    # Soft utility\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    p_short = probs[:, 0]\n",
    "    p_flat = probs[:, 1]\n",
    "    p_long = probs[:, 2]\n",
    "\n",
    "    trade_conf = 1.0 - p_flat\n",
    "    k = float(cfg.get(\"utility_k\", 2.0))\n",
    "    pos = trade_conf * torch.tanh(k * (p_long - p_short))  # [-1,1] approximately\n",
    "\n",
    "    fee = float(cfg.get(\"cost_bps\", 0.0)) * 1e-4\n",
    "    utility = pos * er - fee * torch.abs(pos)\n",
    "    util_loss = -utility.mean()\n",
    "\n",
    "    total = w_ce * ce + w_ret * huber + w_ut * util_loss\n",
    "\n",
    "    parts = {\n",
    "        \"ce\": ce.detach(),\n",
    "        \"huber\": huber.detach(),\n",
    "        \"util\": utility.mean().detach(),       # mean utility (higher is better)\n",
    "        \"util_loss\": util_loss.detach(),\n",
    "        \"pos_abs\": torch.abs(pos).mean().detach(),\n",
    "    }\n",
    "    return total, parts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# Step 9: Evaluation helpers (multitask)\n",
    "# ======================================================================\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_multitask_on_indices(\n",
    "    model: nn.Module,\n",
    "    X_scaled: np.ndarray,\n",
    "    edge_scaled: np.ndarray,\n",
    "    indices: np.ndarray,\n",
    "    ce_loss_fn: nn.Module,\n",
    "    cfg: Dict[str, Any],\n",
    ") -> Dict[str, Any]:\n",
    "    ds = LobGraphSequenceDataset3Class(\n",
    "        X_node=X_scaled,\n",
    "        E_feat=edge_scaled,\n",
    "        y_tb_arr=y_tb,\n",
    "        exit_ret_arr=exit_ret,\n",
    "        sample_t_=sample_t,\n",
    "        indices=indices.astype(np.int64),\n",
    "        lookback=cfg[\"lookback\"],\n",
    "    )\n",
    "    loader = DataLoader(ds, batch_size=int(cfg[\"batch_size\"]), shuffle=False, collate_fn=collate_fn_3class, num_workers=0)\n",
    "\n",
    "    model.eval()\n",
    "    tot_loss = 0.0\n",
    "    tot_ce = 0.0\n",
    "    tot_huber = 0.0\n",
    "    tot_util = 0.0\n",
    "    tot_pos_abs = 0.0\n",
    "    n = 0\n",
    "\n",
    "    probs_all = []\n",
    "    y_all = []\n",
    "    er_all = []\n",
    "    ret_hat_all = []\n",
    "\n",
    "    for x, e, y, er, _sidx in loader:\n",
    "        x = x.to(DEVICE).float()\n",
    "        e = e.to(DEVICE).float()\n",
    "        y = y.to(DEVICE).long()\n",
    "        er = er.to(DEVICE).float()\n",
    "\n",
    "        logits, ret_hat, aux = model(x, e, return_aux=True)\n",
    "        loss, parts = multitask_loss(logits, ret_hat, y, er, ce_loss_fn, cfg)\n",
    "        loss = total_loss_with_adj_reg(loss, aux, cfg)\n",
    "\n",
    "        B = int(y.size(0))\n",
    "        tot_loss += float(loss.item()) * B\n",
    "        tot_ce += float(parts[\"ce\"].item()) * B\n",
    "        tot_huber += float(parts[\"huber\"].item()) * B\n",
    "        tot_util += float(parts[\"util\"].item()) * B\n",
    "        tot_pos_abs += float(parts[\"pos_abs\"].item()) * B\n",
    "        n += B\n",
    "\n",
    "        p = torch.softmax(logits, dim=-1).detach().cpu().numpy()\n",
    "        probs_all.append(p)\n",
    "        y_all.append(y.detach().cpu().numpy())\n",
    "        er_all.append(er.detach().cpu().numpy())\n",
    "        ret_hat_all.append(ret_hat.detach().cpu().numpy())\n",
    "\n",
    "    prob3 = np.concatenate(probs_all, axis=0) if probs_all else np.zeros((0, 3), dtype=np.float64)\n",
    "    y_np = np.concatenate(y_all, axis=0) if y_all else np.zeros((0,), dtype=np.int64)\n",
    "    er_np = np.concatenate(er_all, axis=0) if er_all else np.zeros((0,), dtype=np.float64)\n",
    "    rh_np = np.concatenate(ret_hat_all, axis=0) if ret_hat_all else np.zeros((0,), dtype=np.float64)\n",
    "\n",
    "    trade_auc, dir_auc = compute_trade_dir_auc_from_probs(y_np, prob3)\n",
    "\n",
    "    y_pred = prob3.argmax(axis=1) if len(y_np) else np.array([], dtype=np.int64)\n",
    "    acc = float(accuracy_score(y_np, y_pred)) if len(y_np) else float(\"nan\")\n",
    "    f1m = float(f1_score(y_np, y_pred, average=\"macro\")) if len(y_np) else float(\"nan\")\n",
    "    cm = confusion_matrix(y_np, y_pred, labels=[0, 1, 2]) if len(y_np) else None\n",
    "\n",
    "    out = {\n",
    "        \"loss\": float(tot_loss / max(1, n)),\n",
    "        \"loss_ce\": float(tot_ce / max(1, n)),\n",
    "        \"loss_huber\": float(tot_huber / max(1, n)),\n",
    "        \"util_mean\": float(tot_util / max(1, n)),\n",
    "        \"pos_abs_mean\": float(tot_pos_abs / max(1, n)),\n",
    "        \"acc\": acc,\n",
    "        \"f1m\": f1m,\n",
    "        \"cm\": cm,\n",
    "        \"trade_auc\": float(trade_auc) if np.isfinite(trade_auc) else float(\"nan\"),\n",
    "        \"dir_auc\": float(dir_auc) if np.isfinite(dir_auc) else float(\"nan\"),\n",
    "        \"prob3\": prob3,\n",
    "        \"y\": y_np,\n",
    "        \"er\": er_np,\n",
    "        \"ret_hat\": rh_np,\n",
    "    }\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# Step 10: Artifact saving/loading (weights .pt + scaler .npz + meta .json)\n",
    "# ======================================================================\n",
    "\n",
    "def _to_jsonable_cfg(cfg: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Ensure cfg is JSON-serializable (paths -> str, lists -> lists).\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    for k, v in cfg.items():\n",
    "        if isinstance(v, Path):\n",
    "            out[k] = str(v)\n",
    "        else:\n",
    "            out[k] = v\n",
    "    return out\n",
    "\n",
    "\n",
    "def save_scaler_npz(path: Path, params: Dict[str, Any]) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    np.savez_compressed(\n",
    "        str(path),\n",
    "        center_=np.asarray(params[\"center_\"], dtype=np.float32),\n",
    "        scale_=np.asarray(params[\"scale_\"], dtype=np.float32),\n",
    "        max_abs=np.asarray([float(params[\"max_abs\"])], dtype=np.float32),\n",
    "    )\n",
    "\n",
    "\n",
    "def load_scaler_npz(path: Path) -> Dict[str, Any]:\n",
    "    data = np.load(str(path))\n",
    "    return {\n",
    "        \"center_\": data[\"center_\"].astype(np.float32),\n",
    "        \"scale_\": data[\"scale_\"].astype(np.float32),\n",
    "        \"max_abs\": float(data[\"max_abs\"][0]),\n",
    "    }\n",
    "\n",
    "\n",
    "def save_bundle(\n",
    "    bundle_dir: Path,\n",
    "    name: str,\n",
    "    model_state: Dict[str, torch.Tensor],\n",
    "    cfg: Dict[str, Any],\n",
    "    node_scaler_params: Dict[str, Any],\n",
    "    edge_scaler_params: Optional[Dict[str, Any]],\n",
    "    extra_meta: Dict[str, Any],\n",
    ") -> Dict[str, Path]:\n",
    "    \"\"\"\n",
    "    Saves:\n",
    "      - weights: {name}_weights.pt\n",
    "      - scalers: {name}_node_scaler.npz and {name}_edge_scaler.npz\n",
    "      - meta:    {name}_meta.json\n",
    "    \"\"\"\n",
    "    bundle_dir.mkdir(parents=True, exist_ok=True)\n",
    "    weights_path = bundle_dir / f\"{name}_weights.pt\"\n",
    "    node_scaler_path = bundle_dir / f\"{name}_node_scaler.npz\"\n",
    "    edge_scaler_path = bundle_dir / f\"{name}_edge_scaler.npz\"\n",
    "    meta_path = bundle_dir / f\"{name}_meta.json\"\n",
    "\n",
    "    # weights only\n",
    "    torch.save(model_state, str(weights_path))\n",
    "\n",
    "    # scalers (no pickle)\n",
    "    save_scaler_npz(node_scaler_path, node_scaler_params)\n",
    "    if edge_scaler_params is not None:\n",
    "        save_scaler_npz(edge_scaler_path, edge_scaler_params)\n",
    "        edge_scaler_file = edge_scaler_path.name\n",
    "    else:\n",
    "        edge_scaler_file = None\n",
    "\n",
    "    meta = {\n",
    "        \"name\": name,\n",
    "        \"weights_file\": weights_path.name,\n",
    "        \"node_scaler_file\": node_scaler_path.name,\n",
    "        \"edge_scaler_file\": edge_scaler_file,\n",
    "        \"cfg\": _to_jsonable_cfg(cfg),\n",
    "        **extra_meta,\n",
    "    }\n",
    "    with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(meta, f, indent=2)\n",
    "\n",
    "    return {\n",
    "        \"weights\": weights_path,\n",
    "        \"node_scaler\": node_scaler_path,\n",
    "        \"edge_scaler\": edge_scaler_path if edge_scaler_params is not None else None,\n",
    "        \"meta\": meta_path,\n",
    "    }\n",
    "\n",
    "\n",
    "def load_bundle(bundle_dir: Path, name: str) -> Dict[str, Any]:\n",
    "    meta_path = bundle_dir / f\"{name}_meta.json\"\n",
    "    if not meta_path.exists():\n",
    "        raise FileNotFoundError(str(meta_path))\n",
    "\n",
    "    with open(meta_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        meta = json.load(f)\n",
    "\n",
    "    weights_path = bundle_dir / meta[\"weights_file\"]\n",
    "    node_scaler_path = bundle_dir / meta[\"node_scaler_file\"]\n",
    "    edge_scaler_file = meta.get(\"edge_scaler_file\", None)\n",
    "    edge_scaler_path = (bundle_dir / edge_scaler_file) if edge_scaler_file else None\n",
    "\n",
    "    # weights-only load (safe default in torch>=2.6)\n",
    "    state = torch.load(str(weights_path), map_location=\"cpu\", weights_only=True)\n",
    "\n",
    "    node_scaler_params = load_scaler_npz(node_scaler_path)\n",
    "    edge_scaler_params = load_scaler_npz(edge_scaler_path) if edge_scaler_path else None\n",
    "\n",
    "    return {\n",
    "        \"meta\": meta,\n",
    "        \"state\": state,\n",
    "        \"node_scaler_params\": node_scaler_params,\n",
    "        \"edge_scaler_params\": edge_scaler_params,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# Step 11: Training one fold (multi-task), selection by trade_auc + w*dir_auc\n",
    "# ======================================================================\n",
    "\n",
    "def train_one_fold_multitask(\n",
    "    fold_id: int,\n",
    "    X_scaled: np.ndarray,\n",
    "    edge_scaled: np.ndarray,\n",
    "    idx_train: np.ndarray,\n",
    "    idx_val: np.ndarray,\n",
    "    idx_test: np.ndarray,\n",
    "    node_scaler_params: Dict[str, Any],\n",
    "    edge_scaler_params: Optional[Dict[str, Any]],\n",
    "    cfg: Dict[str, Any],\n",
    ") -> Dict[str, Any]:\n",
    "    # Build loaders\n",
    "    t_train = sample_t[idx_train]\n",
    "    y_train = y_tb[t_train].astype(np.int64)\n",
    "\n",
    "    tr_ds = LobGraphSequenceDataset3Class(X_scaled, edge_scaled, y_tb, exit_ret, sample_t, idx_train, cfg[\"lookback\"])\n",
    "    va_ds = LobGraphSequenceDataset3Class(X_scaled, edge_scaled, y_tb, exit_ret, sample_t, idx_val,   cfg[\"lookback\"])\n",
    "    te_ds = LobGraphSequenceDataset3Class(X_scaled, edge_scaled, y_tb, exit_ret, sample_t, idx_test,  cfg[\"lookback\"])\n",
    "\n",
    "    sampler = None\n",
    "    shuffle = True\n",
    "    if bool(cfg.get(\"use_weighted_sampler\", True)):\n",
    "        sampler = make_weighted_sampler_3class(y_train)\n",
    "        shuffle = False\n",
    "\n",
    "    tr_loader = DataLoader(tr_ds, batch_size=int(cfg[\"batch_size\"]), shuffle=shuffle, sampler=sampler, collate_fn=collate_fn_3class, num_workers=0)\n",
    "    va_loader = DataLoader(va_ds, batch_size=int(cfg[\"batch_size\"]), shuffle=False, collate_fn=collate_fn_3class, num_workers=0)\n",
    "    te_loader = DataLoader(te_ds, batch_size=int(cfg[\"batch_size\"]), shuffle=False, collate_fn=collate_fn_3class, num_workers=0)\n",
    "\n",
    "    model = GraphWaveNetMultiTask(\n",
    "        node_in=int(X_scaled.shape[-1]),\n",
    "        edge_dim=int(edge_scaled.shape[-1]),\n",
    "        cfg=cfg,\n",
    "        n_nodes=len(ASSETS),\n",
    "        target_node=TARGET_NODE,\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    ce_w = make_ce_weights_3class(y_train)\n",
    "    ce_loss_fn = nn.CrossEntropyLoss(weight=ce_w, label_smoothing=float(cfg.get(\"label_smoothing\", 0.0)))\n",
    "\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=float(cfg[\"lr\"]), weight_decay=float(cfg[\"weight_decay\"]))\n",
    "\n",
    "    use_onecycle = bool(cfg.get(\"use_onecycle\", True))\n",
    "    if use_onecycle:\n",
    "        sch = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            opt,\n",
    "            max_lr=float(cfg[\"lr\"]),\n",
    "            epochs=int(cfg[\"epochs\"]),\n",
    "            steps_per_epoch=max(1, len(tr_loader)),\n",
    "            pct_start=0.15,\n",
    "            div_factor=10.0,\n",
    "            final_div_factor=50.0,\n",
    "        )\n",
    "    else:\n",
    "        sch = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"max\", factor=0.5, patience=3)\n",
    "\n",
    "    best_sel = -1e18\n",
    "    best_state = None\n",
    "    best_epoch = -1\n",
    "    patience = 7\n",
    "    bad = 0\n",
    "\n",
    "    w_dir = float(cfg.get(\"sel_metric_dir_weight\", 0.5))\n",
    "    trade_pen = float(cfg.get(\"trade_prob_penalty\", 0.0))\n",
    "\n",
    "    def selection_metric(trade_auc: float, dir_auc: float) -> float:\n",
    "        ta = float(trade_auc) if np.isfinite(trade_auc) else -1e18\n",
    "        da = float(dir_auc) if np.isfinite(dir_auc) else 0.0\n",
    "        return ta + w_dir * da\n",
    "\n",
    "    for ep in range(1, int(cfg[\"epochs\"]) + 1):\n",
    "        model.train()\n",
    "\n",
    "        tot = 0.0\n",
    "        tot_ce = 0.0\n",
    "        tot_huber = 0.0\n",
    "        tot_util = 0.0\n",
    "        n = 0\n",
    "\n",
    "        for x, e, y, er, _sidx in tr_loader:\n",
    "            x = x.to(DEVICE).float()\n",
    "            e = e.to(DEVICE).float()\n",
    "            y = y.to(DEVICE).long()\n",
    "            er = er.to(DEVICE).float()\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "\n",
    "            logits, ret_hat, aux = model(x, e, return_aux=True)\n",
    "\n",
    "            # multi-task base loss\n",
    "            loss_mt, parts = multitask_loss(logits, ret_hat, y, er, ce_loss_fn, cfg)\n",
    "\n",
    "            # optional probability mass penalty to reduce overtrading\n",
    "            if trade_pen > 0:\n",
    "                p = torch.softmax(logits, dim=-1)\n",
    "                p_trade = (p[:, 0] + p[:, 2]).mean()\n",
    "                loss_mt = loss_mt + trade_pen * p_trade\n",
    "\n",
    "            loss = total_loss_with_adj_reg(loss_mt, aux, cfg)\n",
    "\n",
    "            if not torch.isfinite(loss):\n",
    "                continue\n",
    "\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), float(cfg[\"grad_clip\"]))\n",
    "            opt.step()\n",
    "            if use_onecycle:\n",
    "                sch.step()\n",
    "\n",
    "            B = int(y.size(0))\n",
    "            tot += float(loss.item()) * B\n",
    "            tot_ce += float(parts[\"ce\"].item()) * B\n",
    "            tot_huber += float(parts[\"huber\"].item()) * B\n",
    "            tot_util += float(parts[\"util\"].item()) * B\n",
    "            n += B\n",
    "\n",
    "        tr_loss = tot / max(1, n)\n",
    "        tr_ce = tot_ce / max(1, n)\n",
    "        tr_huber = tot_huber / max(1, n)\n",
    "        tr_util = tot_util / max(1, n)\n",
    "\n",
    "        # epoch val AUCs + val losses (for monitoring)\n",
    "        model.eval()\n",
    "        val_probs = []\n",
    "        val_ys = []\n",
    "        val_er = []\n",
    "\n",
    "        val_tot = 0.0\n",
    "        val_n = 0\n",
    "        val_ce_sum = 0.0\n",
    "        val_huber_sum = 0.0\n",
    "        val_util_sum = 0.0\n",
    "\n",
    "        for x, e, y, er, _sidx in va_loader:\n",
    "            x = x.to(DEVICE).float()\n",
    "            e = e.to(DEVICE).float()\n",
    "            y = y.to(DEVICE).long()\n",
    "            er = er.to(DEVICE).float()\n",
    "\n",
    "            logits, ret_hat, aux = model(x, e, return_aux=True)\n",
    "            loss_mt, parts = multitask_loss(logits, ret_hat, y, er, ce_loss_fn, cfg)\n",
    "            loss_val = total_loss_with_adj_reg(loss_mt, aux, cfg)\n",
    "\n",
    "            B = int(y.size(0))\n",
    "            val_tot += float(loss_val.item()) * B\n",
    "            val_ce_sum += float(parts[\"ce\"].item()) * B\n",
    "            val_huber_sum += float(parts[\"huber\"].item()) * B\n",
    "            val_util_sum += float(parts[\"util\"].item()) * B\n",
    "            val_n += B\n",
    "\n",
    "            p = torch.softmax(logits, dim=-1).detach().cpu().numpy()\n",
    "            val_probs.append(p)\n",
    "            val_ys.append(y.detach().cpu().numpy())\n",
    "            val_er.append(er.detach().cpu().numpy())\n",
    "\n",
    "        val_prob3 = np.concatenate(val_probs, axis=0) if val_probs else np.zeros((0, 3), dtype=np.float64)\n",
    "        val_y = np.concatenate(val_ys, axis=0) if val_ys else np.zeros((0,), dtype=np.int64)\n",
    "\n",
    "        trade_auc, dir_auc = compute_trade_dir_auc_from_probs(val_y, val_prob3)\n",
    "        sel = selection_metric(trade_auc, dir_auc)\n",
    "\n",
    "        val_loss = val_tot / max(1, val_n)\n",
    "        val_ce = val_ce_sum / max(1, val_n)\n",
    "        val_huber = val_huber_sum / max(1, val_n)\n",
    "        val_util = val_util_sum / max(1, val_n)\n",
    "\n",
    "        if sel > best_sel:\n",
    "            best_sel = sel\n",
    "            best_epoch = ep\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            bad = 0\n",
    "        else:\n",
    "            bad += 1\n",
    "\n",
    "        if not use_onecycle:\n",
    "            sch.step(sel)\n",
    "\n",
    "        lr_now = opt.param_groups[0][\"lr\"]\n",
    "        w_support = model.support_mix().detach().cpu().numpy().tolist()\n",
    "\n",
    "        print(\n",
    "            f\"[fold {fold_id:02d}] ep {ep:02d} lr={lr_now:.2e} \"\n",
    "            f\"tr_loss={tr_loss:.4f} (ce={tr_ce:.4f}, huber={tr_huber:.4f}, util={tr_util:.5f}) \"\n",
    "            f\"val_loss={val_loss:.4f} (ce={val_ce:.4f}, huber={val_huber:.4f}, util={val_util:.5f}) \"\n",
    "            f\"val_trade_auc={trade_auc:.3f} val_dir_auc={dir_auc:.3f} sel={sel:.3f} \"\n",
    "            f\"best={best_sel:.3f}@ep{best_epoch:02d} supports={np.round(w_support, 3).tolist()}\"\n",
    "        )\n",
    "\n",
    "        if bad >= patience:\n",
    "            break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    # Eval (full) on VAL and TEST\n",
    "    val_eval = eval_multitask_on_indices(model, X_scaled, edge_scaled, idx_val, ce_loss_fn, cfg)\n",
    "    test_eval = eval_multitask_on_indices(model, X_scaled, edge_scaled, idx_test, ce_loss_fn, cfg)\n",
    "\n",
    "    # thresholds chosen on VAL for PnL, then applied to TEST\n",
    "    true_val_trade_rate = split_trade_ratio(idx_val, sample_t, y_trade)\n",
    "    sweep_val = sweep_thresholds_3class(\n",
    "        prob3=val_eval[\"prob3\"],\n",
    "        exit_ret_arr=val_eval[\"er\"],\n",
    "        cfg=cfg,\n",
    "        min_trades=int(cfg[\"eval_min_trades\"]),\n",
    "        target_trade_rate=float(true_val_trade_rate),\n",
    "    )\n",
    "    best_thr = sweep_val.iloc[0].to_dict()\n",
    "    thr_trade = float(best_thr[\"thr_trade\"])\n",
    "    thr_dir = float(best_thr[\"thr_dir\"])\n",
    "\n",
    "    pnl_val = pnl_from_probs_3class(val_eval[\"prob3\"], val_eval[\"er\"], thr_trade, thr_dir, cfg[\"cost_bps\"])\n",
    "    pnl_test = pnl_from_probs_3class(test_eval[\"prob3\"], test_eval[\"er\"], thr_trade, thr_dir, cfg[\"cost_bps\"])\n",
    "\n",
    "    print(\n",
    "        f\"[fold {fold_id:02d}] chosen thresholds on VAL: thr_trade={thr_trade:.3f} thr_dir={thr_dir:.3f} \"\n",
    "        f\"| val pnl_sum={pnl_val['pnl_sum']:.4f} val trade_rate={pnl_val['trade_rate']:.3f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"[fold {fold_id:02d}] TEST (fixed thresholds from VAL): \"\n",
    "        f\"trade_auc={test_eval['trade_auc']:.3f} dir_auc={test_eval['dir_auc']:.3f} \"\n",
    "        f\"pnl_sum={pnl_test['pnl_sum']:.4f} trade_rate={pnl_test['trade_rate']:.3f} trades={pnl_test['n_trades']}\"\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"fold\": int(fold_id),\n",
    "        \"model_state\": {k: v.detach().cpu().clone() for k, v in model.state_dict().items()},\n",
    "        \"node_scaler_params\": node_scaler_params,\n",
    "        \"edge_scaler_params\": edge_scaler_params,\n",
    "        \"idx_train\": idx_train,\n",
    "        \"idx_val\": idx_val,\n",
    "        \"idx_test\": idx_test,\n",
    "        \"best_epoch\": int(best_epoch),\n",
    "        \"best_sel\": float(best_sel),\n",
    "        \"val_eval\": val_eval,\n",
    "        \"test_eval\": test_eval,\n",
    "        \"thr_trade\": thr_trade,\n",
    "        \"thr_dir\": thr_dir,\n",
    "        \"pnl_val\": pnl_val,\n",
    "        \"pnl_test\": pnl_test,\n",
    "        \"sweep_val_head\": sweep_val.head(5),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================================================\n",
      "FOLD 1/4 sizes: train=5652 val=1130 test=1130\n",
      "True trade ratio (val):  0.365\n",
      "True trade ratio (test): 0.304\n",
      "[fold 01] ep 01 lr=9.84e-05 tr_loss=1.0414 (ce=1.0329, huber=0.0024, util=-0.00002) val_loss=1.1555 (ce=1.1546, huber=0.0019, util=0.00002) val_trade_auc=0.517 val_dir_auc=0.433 sel=0.734 best=0.734@ep01 supports=[0.333, 0.334, 0.333]\n",
      "[fold 01] ep 02 lr=2.34e-04 tr_loss=0.9871 (ce=0.9776, huber=0.0023, util=0.00012) val_loss=1.2073 (ce=1.2065, huber=0.0017, util=0.00008) val_trade_auc=0.524 val_dir_auc=0.454 sel=0.751 best=0.751@ep02 supports=[0.333, 0.334, 0.333]\n",
      "[fold 01] ep 03 lr=3.00e-04 tr_loss=0.9616 (ce=0.9521, huber=0.0020, util=0.00029) val_loss=1.2228 (ce=1.2220, huber=0.0015, util=0.00003) val_trade_auc=0.519 val_dir_auc=0.462 sel=0.750 best=0.751@ep02 supports=[0.334, 0.332, 0.334]\n",
      "[fold 01] ep 04 lr=2.97e-04 tr_loss=0.9232 (ce=0.9137, huber=0.0018, util=0.00051) val_loss=1.2143 (ce=1.2135, huber=0.0014, util=0.00003) val_trade_auc=0.518 val_dir_auc=0.527 sel=0.782 best=0.782@ep04 supports=[0.335, 0.33, 0.335]\n",
      "[fold 01] ep 05 lr=2.90e-04 tr_loss=0.9009 (ce=0.8915, huber=0.0017, util=0.00065) val_loss=1.2479 (ce=1.2472, huber=0.0014, util=-0.00004) val_trade_auc=0.524 val_dir_auc=0.505 sel=0.777 best=0.782@ep04 supports=[0.336, 0.328, 0.336]\n",
      "[fold 01] ep 06 lr=2.77e-04 tr_loss=0.8747 (ce=0.8653, huber=0.0016, util=0.00084) val_loss=1.2702 (ce=1.2694, huber=0.0014, util=-0.00009) val_trade_auc=0.527 val_dir_auc=0.500 sel=0.777 best=0.782@ep04 supports=[0.337, 0.325, 0.338]\n",
      "[fold 01] ep 07 lr=2.61e-04 tr_loss=0.8334 (ce=0.8240, huber=0.0016, util=0.00111) val_loss=1.2762 (ce=1.2754, huber=0.0014, util=-0.00012) val_trade_auc=0.518 val_dir_auc=0.488 sel=0.762 best=0.782@ep04 supports=[0.339, 0.321, 0.34]\n",
      "[fold 01] ep 08 lr=2.40e-04 tr_loss=0.7895 (ce=0.7802, huber=0.0016, util=0.00138) val_loss=1.3165 (ce=1.3158, huber=0.0014, util=-0.00014) val_trade_auc=0.518 val_dir_auc=0.521 sel=0.778 best=0.782@ep04 supports=[0.341, 0.317, 0.342]\n",
      "[fold 01] ep 09 lr=2.16e-04 tr_loss=0.7610 (ce=0.7517, huber=0.0016, util=0.00160) val_loss=1.3207 (ce=1.3199, huber=0.0014, util=-0.00015) val_trade_auc=0.521 val_dir_auc=0.533 sel=0.787 best=0.787@ep09 supports=[0.342, 0.314, 0.345]\n",
      "[fold 01] ep 10 lr=1.91e-04 tr_loss=0.7146 (ce=0.7053, huber=0.0016, util=0.00185) val_loss=1.3266 (ce=1.3258, huber=0.0014, util=-0.00014) val_trade_auc=0.510 val_dir_auc=0.507 sel=0.764 best=0.787@ep09 supports=[0.342, 0.311, 0.347]\n",
      "[fold 01] ep 11 lr=1.64e-04 tr_loss=0.6625 (ce=0.6532, huber=0.0016, util=0.00212) val_loss=1.4120 (ce=1.4112, huber=0.0013, util=-0.00019) val_trade_auc=0.512 val_dir_auc=0.524 sel=0.774 best=0.787@ep09 supports=[0.343, 0.308, 0.349]\n",
      "[fold 01] ep 12 lr=1.36e-04 tr_loss=0.6255 (ce=0.6163, huber=0.0016, util=0.00229) val_loss=1.4563 (ce=1.4556, huber=0.0013, util=-0.00023) val_trade_auc=0.514 val_dir_auc=0.504 sel=0.766 best=0.787@ep09 supports=[0.343, 0.307, 0.35]\n",
      "[fold 01] ep 13 lr=1.09e-04 tr_loss=0.6035 (ce=0.5943, huber=0.0016, util=0.00244) val_loss=1.4592 (ce=1.4584, huber=0.0013, util=-0.00023) val_trade_auc=0.507 val_dir_auc=0.508 sel=0.761 best=0.787@ep09 supports=[0.343, 0.306, 0.351]\n",
      "[fold 01] ep 14 lr=8.30e-05 tr_loss=0.5876 (ce=0.5784, huber=0.0015, util=0.00249) val_loss=1.4600 (ce=1.4592, huber=0.0013, util=-0.00022) val_trade_auc=0.499 val_dir_auc=0.488 sel=0.743 best=0.787@ep09 supports=[0.343, 0.305, 0.352]\n",
      "[fold 01] ep 15 lr=5.96e-05 tr_loss=0.5744 (ce=0.5653, huber=0.0016, util=0.00256) val_loss=1.4780 (ce=1.4773, huber=0.0013, util=-0.00022) val_trade_auc=0.503 val_dir_auc=0.502 sel=0.754 best=0.787@ep09 supports=[0.343, 0.304, 0.353]\n",
      "[fold 01] ep 16 lr=3.93e-05 tr_loss=0.5503 (ce=0.5411, huber=0.0015, util=0.00264) val_loss=1.5065 (ce=1.5057, huber=0.0013, util=-0.00025) val_trade_auc=0.506 val_dir_auc=0.495 sel=0.753 best=0.787@ep09 supports=[0.343, 0.303, 0.354]\n",
      "[fold 01] chosen thresholds on VAL: thr_trade=0.907 thr_dir=0.550 | val pnl_sum=0.0098 val trade_rate=0.083\n",
      "[fold 01] TEST (fixed thresholds from VAL): trade_auc=0.554 dir_auc=0.617 pnl_sum=0.0453 trade_rate=0.151 trades=171\n",
      "Saved fold bundle: fold_01_meta.json\n",
      "\n",
      "==========================================================================================\n",
      "FOLD 2/4 sizes: train=6782 val=1130 test=1130\n",
      "True trade ratio (val):  0.304\n",
      "True trade ratio (test): 0.463\n",
      "[fold 02] ep 01 lr=9.83e-05 tr_loss=1.0912 (ce=1.0832, huber=0.0022, util=-0.00000) val_loss=1.2628 (ce=1.2619, huber=0.0019, util=-0.00004) val_trade_auc=0.500 val_dir_auc=0.536 sel=0.768 best=0.768@ep01 supports=[0.333, 0.333, 0.333]\n",
      "[fold 02] ep 02 lr=2.34e-04 tr_loss=0.9933 (ce=0.9839, huber=0.0022, util=0.00014) val_loss=1.4090 (ce=1.4082, huber=0.0018, util=-0.00000) val_trade_auc=0.566 val_dir_auc=0.617 sel=0.875 best=0.875@ep02 supports=[0.334, 0.333, 0.334]\n",
      "[fold 02] ep 03 lr=3.00e-04 tr_loss=0.9727 (ce=0.9632, huber=0.0020, util=0.00025) val_loss=1.3707 (ce=1.3699, huber=0.0017, util=0.00001) val_trade_auc=0.584 val_dir_auc=0.604 sel=0.886 best=0.886@ep03 supports=[0.334, 0.332, 0.334]\n",
      "[fold 02] ep 04 lr=2.97e-04 tr_loss=0.9318 (ce=0.9222, huber=0.0019, util=0.00044) val_loss=1.3887 (ce=1.3879, huber=0.0016, util=0.00000) val_trade_auc=0.607 val_dir_auc=0.608 sel=0.911 best=0.911@ep04 supports=[0.335, 0.33, 0.335]\n",
      "[fold 02] ep 05 lr=2.90e-04 tr_loss=0.9100 (ce=0.9005, huber=0.0018, util=0.00063) val_loss=1.3873 (ce=1.3865, huber=0.0015, util=0.00001) val_trade_auc=0.614 val_dir_auc=0.587 sel=0.907 best=0.911@ep04 supports=[0.337, 0.327, 0.337]\n",
      "[fold 02] ep 06 lr=2.77e-04 tr_loss=0.8848 (ce=0.8755, huber=0.0017, util=0.00081) val_loss=1.4065 (ce=1.4058, huber=0.0014, util=-0.00002) val_trade_auc=0.609 val_dir_auc=0.579 sel=0.898 best=0.911@ep04 supports=[0.338, 0.323, 0.339]\n",
      "[fold 02] ep 07 lr=2.61e-04 tr_loss=0.8432 (ce=0.8338, huber=0.0016, util=0.00105) val_loss=1.3871 (ce=1.3864, huber=0.0014, util=-0.00004) val_trade_auc=0.603 val_dir_auc=0.554 sel=0.881 best=0.911@ep04 supports=[0.339, 0.319, 0.341]\n",
      "[fold 02] ep 08 lr=2.40e-04 tr_loss=0.8018 (ce=0.7925, huber=0.0015, util=0.00130) val_loss=1.4146 (ce=1.4139, huber=0.0013, util=-0.00002) val_trade_auc=0.606 val_dir_auc=0.571 sel=0.891 best=0.911@ep04 supports=[0.341, 0.314, 0.345]\n",
      "[fold 02] ep 09 lr=2.17e-04 tr_loss=0.7575 (ce=0.7482, huber=0.0015, util=0.00154) val_loss=1.4356 (ce=1.4349, huber=0.0013, util=-0.00003) val_trade_auc=0.602 val_dir_auc=0.531 sel=0.867 best=0.911@ep04 supports=[0.341, 0.31, 0.349]\n",
      "[fold 02] ep 10 lr=1.91e-04 tr_loss=0.7275 (ce=0.7183, huber=0.0014, util=0.00171) val_loss=1.4605 (ce=1.4598, huber=0.0012, util=-0.00007) val_trade_auc=0.604 val_dir_auc=0.569 sel=0.888 best=0.911@ep04 supports=[0.34, 0.307, 0.352]\n",
      "[fold 02] ep 11 lr=1.64e-04 tr_loss=0.7038 (ce=0.6946, huber=0.0014, util=0.00182) val_loss=1.4997 (ce=1.4990, huber=0.0012, util=-0.00012) val_trade_auc=0.589 val_dir_auc=0.542 sel=0.860 best=0.911@ep04 supports=[0.339, 0.305, 0.355]\n",
      "[fold 02] chosen thresholds on VAL: thr_trade=0.894 thr_dir=0.500 | val pnl_sum=0.1027 val trade_rate=0.177\n",
      "[fold 02] TEST (fixed thresholds from VAL): trade_auc=0.519 dir_auc=0.512 pnl_sum=0.3302 trade_rate=0.352 trades=398\n",
      "Saved fold bundle: fold_02_meta.json\n",
      "\n",
      "==========================================================================================\n",
      "FOLD 3/4 sizes: train=7912 val=1130 test=1130\n",
      "True trade ratio (val):  0.463\n",
      "True trade ratio (test): 0.558\n",
      "[fold 03] ep 01 lr=9.82e-05 tr_loss=1.0451 (ce=1.0366, huber=0.0021, util=0.00001) val_loss=1.1519 (ce=1.1510, huber=0.0019, util=0.00013) val_trade_auc=0.496 val_dir_auc=0.522 sel=0.757 best=0.757@ep01 supports=[0.333, 0.333, 0.333]\n",
      "[fold 03] ep 02 lr=2.34e-04 tr_loss=0.9683 (ce=0.9587, huber=0.0021, util=0.00017) val_loss=1.2162 (ce=1.2153, huber=0.0018, util=0.00021) val_trade_auc=0.496 val_dir_auc=0.516 sel=0.754 best=0.757@ep01 supports=[0.333, 0.333, 0.333]\n",
      "[fold 03] ep 03 lr=3.00e-04 tr_loss=0.9545 (ce=0.9450, huber=0.0019, util=0.00027) val_loss=1.2220 (ce=1.2212, huber=0.0016, util=0.00014) val_trade_auc=0.506 val_dir_auc=0.519 sel=0.765 best=0.765@ep03 supports=[0.334, 0.331, 0.334]\n",
      "[fold 03] ep 04 lr=2.97e-04 tr_loss=0.9251 (ce=0.9156, huber=0.0017, util=0.00046) val_loss=1.2284 (ce=1.2277, huber=0.0015, util=0.00026) val_trade_auc=0.515 val_dir_auc=0.516 sel=0.773 best=0.773@ep04 supports=[0.335, 0.329, 0.335]\n",
      "[fold 03] ep 05 lr=2.90e-04 tr_loss=0.8976 (ce=0.8881, huber=0.0016, util=0.00064) val_loss=1.2418 (ce=1.2411, huber=0.0013, util=0.00022) val_trade_auc=0.518 val_dir_auc=0.542 sel=0.789 best=0.789@ep05 supports=[0.339, 0.322, 0.339]\n",
      "[fold 03] ep 06 lr=2.77e-04 tr_loss=0.8647 (ce=0.8554, huber=0.0014, util=0.00089) val_loss=1.2474 (ce=1.2468, huber=0.0013, util=0.00021) val_trade_auc=0.524 val_dir_auc=0.539 sel=0.794 best=0.794@ep06 supports=[0.34, 0.319, 0.341]\n",
      "[fold 03] ep 07 lr=2.61e-04 tr_loss=0.8213 (ce=0.8120, huber=0.0013, util=0.00114) val_loss=1.2708 (ce=1.2701, huber=0.0012, util=0.00013) val_trade_auc=0.518 val_dir_auc=0.531 sel=0.784 best=0.794@ep06 supports=[0.343, 0.313, 0.344]\n",
      "[fold 03] ep 08 lr=2.40e-04 tr_loss=0.7792 (ce=0.7699, huber=0.0013, util=0.00140) val_loss=1.3120 (ce=1.3114, huber=0.0012, util=0.00020) val_trade_auc=0.518 val_dir_auc=0.512 sel=0.774 best=0.794@ep06 supports=[0.345, 0.308, 0.347]\n",
      "[fold 03] ep 09 lr=2.17e-04 tr_loss=0.7232 (ce=0.7140, huber=0.0012, util=0.00170) val_loss=1.3613 (ce=1.3606, huber=0.0011, util=0.00017) val_trade_auc=0.516 val_dir_auc=0.512 sel=0.771 best=0.794@ep06 supports=[0.346, 0.303, 0.351]\n",
      "[fold 03] ep 10 lr=1.91e-04 tr_loss=0.6979 (ce=0.6888, huber=0.0011, util=0.00187) val_loss=1.3568 (ce=1.3562, huber=0.0010, util=0.00021) val_trade_auc=0.516 val_dir_auc=0.508 sel=0.770 best=0.794@ep06 supports=[0.346, 0.301, 0.353]\n",
      "[fold 03] ep 11 lr=1.64e-04 tr_loss=0.6624 (ce=0.6533, huber=0.0010, util=0.00201) val_loss=1.3738 (ce=1.3732, huber=0.0009, util=0.00013) val_trade_auc=0.511 val_dir_auc=0.513 sel=0.768 best=0.794@ep06 supports=[0.347, 0.297, 0.356]\n",
      "[fold 03] ep 12 lr=1.36e-04 tr_loss=0.6223 (ce=0.6132, huber=0.0010, util=0.00223) val_loss=1.4006 (ce=1.4000, huber=0.0009, util=0.00025) val_trade_auc=0.514 val_dir_auc=0.509 sel=0.768 best=0.794@ep06 supports=[0.347, 0.295, 0.358]\n",
      "[fold 03] ep 13 lr=1.09e-04 tr_loss=0.6066 (ce=0.5976, huber=0.0010, util=0.00230) val_loss=1.4171 (ce=1.4165, huber=0.0008, util=0.00011) val_trade_auc=0.504 val_dir_auc=0.522 sel=0.764 best=0.794@ep06 supports=[0.347, 0.294, 0.36]\n",
      "[fold 03] chosen thresholds on VAL: thr_trade=0.500 thr_dir=0.650 | val pnl_sum=0.3650 val trade_rate=0.409\n",
      "[fold 03] TEST (fixed thresholds from VAL): trade_auc=0.641 dir_auc=0.482 pnl_sum=-0.0037 trade_rate=0.427 trades=483\n",
      "Saved fold bundle: fold_03_meta.json\n",
      "\n",
      "==========================================================================================\n",
      "FOLD 4/4 sizes: train=9042 val=1130 test=1130\n",
      "True trade ratio (val):  0.558\n",
      "True trade ratio (test): 0.506\n",
      "[fold 04] ep 01 lr=9.81e-05 tr_loss=1.0534 (ce=1.0449, huber=0.0019, util=-0.00002) val_loss=1.1001 (ce=1.0992, huber=0.0017, util=-0.00006) val_trade_auc=0.539 val_dir_auc=0.464 sel=0.771 best=0.771@ep01 supports=[0.333, 0.334, 0.333]\n",
      "[fold 04] ep 02 lr=2.34e-04 tr_loss=0.9929 (ce=0.9834, huber=0.0018, util=0.00008) val_loss=1.1182 (ce=1.1174, huber=0.0015, util=-0.00008) val_trade_auc=0.592 val_dir_auc=0.461 sel=0.823 best=0.823@ep02 supports=[0.333, 0.334, 0.333]\n",
      "[fold 04] ep 03 lr=3.00e-04 tr_loss=0.9661 (ce=0.9567, huber=0.0017, util=0.00027) val_loss=1.1253 (ce=1.1245, huber=0.0014, util=-0.00008) val_trade_auc=0.639 val_dir_auc=0.454 sel=0.866 best=0.866@ep03 supports=[0.333, 0.334, 0.333]\n",
      "[fold 04] ep 04 lr=2.97e-04 tr_loss=0.9396 (ce=0.9302, huber=0.0015, util=0.00043) val_loss=1.1549 (ce=1.1542, huber=0.0013, util=-0.00009) val_trade_auc=0.659 val_dir_auc=0.445 sel=0.882 best=0.882@ep04 supports=[0.333, 0.334, 0.333]\n",
      "[fold 04] ep 05 lr=2.90e-04 tr_loss=0.9064 (ce=0.8970, huber=0.0014, util=0.00067) val_loss=1.1602 (ce=1.1595, huber=0.0013, util=-0.00002) val_trade_auc=0.667 val_dir_auc=0.467 sel=0.900 best=0.900@ep05 supports=[0.336, 0.329, 0.336]\n",
      "[fold 04] ep 06 lr=2.77e-04 tr_loss=0.8828 (ce=0.8735, huber=0.0013, util=0.00086) val_loss=1.1773 (ce=1.1766, huber=0.0012, util=-0.00009) val_trade_auc=0.669 val_dir_auc=0.454 sel=0.896 best=0.900@ep05 supports=[0.338, 0.324, 0.338]\n",
      "[fold 04] ep 07 lr=2.61e-04 tr_loss=0.8499 (ce=0.8406, huber=0.0012, util=0.00102) val_loss=1.2069 (ce=1.2062, huber=0.0011, util=-0.00012) val_trade_auc=0.656 val_dir_auc=0.440 sel=0.876 best=0.900@ep05 supports=[0.341, 0.318, 0.341]\n",
      "[fold 04] ep 08 lr=2.40e-04 tr_loss=0.8067 (ce=0.7975, huber=0.0012, util=0.00128) val_loss=1.2682 (ce=1.2676, huber=0.0011, util=-0.00007) val_trade_auc=0.646 val_dir_auc=0.444 sel=0.868 best=0.900@ep05 supports=[0.343, 0.315, 0.343]\n",
      "[fold 04] ep 09 lr=2.17e-04 tr_loss=0.7715 (ce=0.7624, huber=0.0011, util=0.00152) val_loss=1.2901 (ce=1.2894, huber=0.0010, util=-0.00016) val_trade_auc=0.648 val_dir_auc=0.432 sel=0.864 best=0.900@ep05 supports=[0.345, 0.31, 0.345]\n",
      "[fold 04] ep 10 lr=1.91e-04 tr_loss=0.7317 (ce=0.7226, huber=0.0010, util=0.00173) val_loss=1.3293 (ce=1.3286, huber=0.0010, util=-0.00009) val_trade_auc=0.641 val_dir_auc=0.450 sel=0.866 best=0.900@ep05 supports=[0.347, 0.305, 0.347]\n",
      "[fold 04] ep 11 lr=1.64e-04 tr_loss=0.6929 (ce=0.6838, huber=0.0010, util=0.00192) val_loss=1.3529 (ce=1.3523, huber=0.0009, util=-0.00016) val_trade_auc=0.636 val_dir_auc=0.443 sel=0.857 best=0.900@ep05 supports=[0.349, 0.302, 0.349]\n",
      "[fold 04] ep 12 lr=1.36e-04 tr_loss=0.6743 (ce=0.6652, huber=0.0009, util=0.00203) val_loss=1.4024 (ce=1.4018, huber=0.0009, util=-0.00017) val_trade_auc=0.639 val_dir_auc=0.442 sel=0.860 best=0.900@ep05 supports=[0.35, 0.3, 0.35]\n",
      "[fold 04] chosen thresholds on VAL: thr_trade=0.500 thr_dir=0.650 | val pnl_sum=0.1049 val trade_rate=0.391\n",
      "[fold 04] TEST (fixed thresholds from VAL): trade_auc=0.596 dir_auc=0.506 pnl_sum=0.2648 trade_rate=0.405 trades=458\n",
      "Saved fold bundle: fold_04_meta.json\n",
      "\n",
      "Saved overall best bundle as: overall_best\n",
      "\n",
      "==========================================================================================\n",
      "CV summary (multi-task; TEST uses thresholds selected on VAL):\n",
      "   fold  val_trade_auc  val_dir_auc  val_loss  test_trade_auc  test_dir_auc  \\\n",
      "0     1       0.520568     0.533172  1.320702        0.554191      0.616690   \n",
      "1     2       0.607403     0.607916  1.388726        0.519097      0.511700   \n",
      "2     3       0.523866     0.539455  1.247449        0.641200      0.481555   \n",
      "3     4       0.667003     0.466772  1.160217        0.596032      0.506378   \n",
      "\n",
      "   test_loss  thr_trade  thr_dir  test_trade_rate_pred  test_pnl_sum  \\\n",
      "0   1.402372   0.906828     0.55              0.151327      0.045270   \n",
      "1   1.231504   0.894001     0.50              0.352212      0.330198   \n",
      "2   1.152467   0.500000     0.65              0.427434     -0.003732   \n",
      "3   1.205582   0.500000     0.65              0.405310      0.264821   \n",
      "\n",
      "   test_pnl_mean  test_n_trades  best_sel  \n",
      "0       0.000040            171  0.787154  \n",
      "1       0.000292            398  0.911361  \n",
      "2      -0.000003            483  0.793593  \n",
      "3       0.000234            458  0.900389  \n",
      "\n",
      "Means:\n",
      "fold                      2.500000\n",
      "val_trade_auc             0.579710\n",
      "val_dir_auc               0.536829\n",
      "val_loss                  1.279274\n",
      "test_trade_auc            0.577630\n",
      "test_dir_auc              0.529080\n",
      "test_loss                 1.247981\n",
      "thr_trade                 0.700207\n",
      "thr_dir                   0.587500\n",
      "test_trade_rate_pred      0.334071\n",
      "test_pnl_sum              0.159139\n",
      "test_pnl_mean             0.000141\n",
      "test_n_trades           377.500000\n",
      "best_sel                  0.848124\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# Step 12: Walk-forward CV run + saving per-fold bundles + overall best\n",
    "# ======================================================================\n",
    "\n",
    "def run_walk_forward_cv_multitask() -> Tuple[pd.DataFrame, List[Dict[str, Any]], str]:\n",
    "    fold_artifacts: List[Dict[str, Any]] = []\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "\n",
    "    best_overall_sel = -1e18\n",
    "    best_overall_name = None\n",
    "\n",
    "    for fi, (idx_tr, idx_va, idx_te) in enumerate(walk_splits, 1):\n",
    "        print(\"\\n\" + \"=\" * 90)\n",
    "        print(f\"FOLD {fi}/{len(walk_splits)} sizes: train={len(idx_tr)} val={len(idx_va)} test={len(idx_te)}\")\n",
    "        print(f\"True trade ratio (val):  {split_trade_ratio(idx_va, sample_t, y_trade):.3f}\")\n",
    "        print(f\"True trade ratio (test): {split_trade_ratio(idx_te, sample_t, y_trade):.3f}\")\n",
    "\n",
    "        # Fold scaling (fit only on fold train timeline)\n",
    "        X_scaled, node_params = fit_scale_nodes_train_only(X_node_raw, sample_t, idx_tr, max_abs=CFG[\"max_abs_feat\"])\n",
    "        if bool(CFG.get(\"edge_scale\", True)):\n",
    "            edge_scaled, edge_params = fit_scale_edges_train_only(edge_feat, sample_t, idx_tr, max_abs=CFG[\"max_abs_edge\"])\n",
    "        else:\n",
    "            edge_scaled = edge_feat.astype(np.float32)\n",
    "            edge_params = None\n",
    "\n",
    "        artifact = train_one_fold_multitask(\n",
    "            fold_id=fi,\n",
    "            X_scaled=X_scaled,\n",
    "            edge_scaled=edge_scaled,\n",
    "            idx_train=idx_tr,\n",
    "            idx_val=idx_va,\n",
    "            idx_test=idx_te,\n",
    "            node_scaler_params=node_params,\n",
    "            edge_scaler_params=edge_params,\n",
    "            cfg=CFG,\n",
    "        )\n",
    "\n",
    "        # Save fold bundle\n",
    "        fold_name = f\"fold_{fi:02d}\"\n",
    "        extra_meta = {\n",
    "            \"kind\": \"fold_best\",\n",
    "            \"fold\": fi,\n",
    "            \"best_epoch\": artifact[\"best_epoch\"],\n",
    "            \"best_sel\": artifact[\"best_sel\"],\n",
    "            \"thr_trade\": artifact[\"thr_trade\"],\n",
    "            \"thr_dir\": artifact[\"thr_dir\"],\n",
    "            \"idx_train\": artifact[\"idx_train\"].tolist(),\n",
    "            \"idx_val\": artifact[\"idx_val\"].tolist(),\n",
    "            \"idx_test\": artifact[\"idx_test\"].tolist(),\n",
    "        }\n",
    "        saved = save_bundle(\n",
    "            bundle_dir=ART_DIR,\n",
    "            name=fold_name,\n",
    "            model_state=artifact[\"model_state\"],\n",
    "            cfg=CFG,\n",
    "            node_scaler_params=artifact[\"node_scaler_params\"],\n",
    "            edge_scaler_params=artifact[\"edge_scaler_params\"],\n",
    "            extra_meta=extra_meta,\n",
    "        )\n",
    "        print(\"Saved fold bundle:\", saved[\"meta\"].name)\n",
    "\n",
    "        if float(artifact[\"best_sel\"]) > best_overall_sel:\n",
    "            best_overall_sel = float(artifact[\"best_sel\"])\n",
    "            best_overall_name = fold_name\n",
    "\n",
    "        fold_artifacts.append(artifact)\n",
    "\n",
    "        rows.append({\n",
    "            \"fold\": fi,\n",
    "            \"val_trade_auc\": artifact[\"val_eval\"][\"trade_auc\"],\n",
    "            \"val_dir_auc\": artifact[\"val_eval\"][\"dir_auc\"],\n",
    "            \"val_loss\": artifact[\"val_eval\"][\"loss\"],\n",
    "            \"test_trade_auc\": artifact[\"test_eval\"][\"trade_auc\"],\n",
    "            \"test_dir_auc\": artifact[\"test_eval\"][\"dir_auc\"],\n",
    "            \"test_loss\": artifact[\"test_eval\"][\"loss\"],\n",
    "            \"thr_trade\": artifact[\"thr_trade\"],\n",
    "            \"thr_dir\": artifact[\"thr_dir\"],\n",
    "            \"test_trade_rate_pred\": artifact[\"pnl_test\"][\"trade_rate\"],\n",
    "            \"test_pnl_sum\": artifact[\"pnl_test\"][\"pnl_sum\"],\n",
    "            \"test_pnl_mean\": artifact[\"pnl_test\"][\"pnl_mean\"],\n",
    "            \"test_n_trades\": artifact[\"pnl_test\"][\"n_trades\"],\n",
    "            \"best_sel\": artifact[\"best_sel\"],\n",
    "        })\n",
    "\n",
    "    cv_summary = pd.DataFrame(rows)\n",
    "\n",
    "    # Create an \"overall_best\" alias by copying the best fold files\n",
    "    assert best_overall_name is not None\n",
    "    overall_name = \"overall_best\"\n",
    "    best_bundle = load_bundle(ART_DIR, best_overall_name)\n",
    "\n",
    "    # Copy weights + scalers by re-saving under overall_best\n",
    "    extra_meta = {\n",
    "        \"kind\": \"overall_best\",\n",
    "        \"source_name\": best_overall_name,\n",
    "        \"source_fold\": best_bundle[\"meta\"].get(\"fold\", None),\n",
    "        \"thr_trade\": best_bundle[\"meta\"][\"thr_trade\"],\n",
    "        \"thr_dir\": best_bundle[\"meta\"][\"thr_dir\"],\n",
    "        \"idx_train\": best_bundle[\"meta\"][\"idx_train\"],\n",
    "        \"idx_val\": best_bundle[\"meta\"][\"idx_val\"],\n",
    "        \"idx_test\": best_bundle[\"meta\"][\"idx_test\"],\n",
    "    }\n",
    "    save_bundle(\n",
    "        bundle_dir=ART_DIR,\n",
    "        name=overall_name,\n",
    "        model_state=best_bundle[\"state\"],\n",
    "        cfg=CFG,\n",
    "        node_scaler_params=best_bundle[\"node_scaler_params\"],\n",
    "        edge_scaler_params=best_bundle[\"edge_scaler_params\"],\n",
    "        extra_meta=extra_meta,\n",
    "    )\n",
    "    print(\"\\nSaved overall best bundle as:\", overall_name)\n",
    "\n",
    "    return cv_summary, fold_artifacts, overall_name\n",
    "\n",
    "\n",
    "cv_summary_mt, fold_artifacts_mt, overall_best_name = run_walk_forward_cv_multitask()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"CV summary (multi-task; TEST uses thresholds selected on VAL):\")\n",
    "print(cv_summary_mt)\n",
    "print(\"\\nMeans:\")\n",
    "print(cv_summary_mt.mean(numeric_only=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================================================\n",
      "FINAL HOLDOUT (10%) using overall_best\n",
      "bundle: overall_best\n",
      "trade_auc=0.536 | dir_auc=0.500 | val/test loss=1.1690\n",
      "soft util mean=-0.000278 | pos_abs mean=0.3416\n",
      "pnl_sum=-0.6337 | trade_rate=0.494 | trades=621\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# Step 13: Evaluate a saved bundle on arbitrary indices (e.g., FINAL holdout)\n",
    "# ======================================================================\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_bundle_on_indices(bundle_dir: Path, name: str, indices: np.ndarray, label: str) -> Dict[str, Any]:\n",
    "    bundle = load_bundle(bundle_dir, name)\n",
    "\n",
    "    # rebuild model\n",
    "    cfg = bundle[\"meta\"][\"cfg\"]\n",
    "    model = GraphWaveNetMultiTask(\n",
    "        node_in=int(X_node_raw.shape[-1]),\n",
    "        edge_dim=int(edge_feat.shape[-1]),\n",
    "        cfg=cfg,\n",
    "        n_nodes=len(ASSETS),\n",
    "        target_node=TARGET_NODE,\n",
    "    ).to(DEVICE)\n",
    "    model.load_state_dict(bundle[\"state\"])\n",
    "    model.eval()\n",
    "\n",
    "    # apply scalers\n",
    "    X_scaled = apply_scaler_params(X_node_raw.astype(np.float32), bundle[\"node_scaler_params\"])\n",
    "    if bundle[\"edge_scaler_params\"] is not None:\n",
    "        E_scaled = apply_scaler_params(edge_feat.astype(np.float32), bundle[\"edge_scaler_params\"])\n",
    "    else:\n",
    "        E_scaled = edge_feat.astype(np.float32)\n",
    "\n",
    "    # CE weights from saved train split labels (approximate)\n",
    "    idx_train_saved = np.asarray(bundle[\"meta\"][\"idx_train\"], dtype=np.int64)\n",
    "    t_train = sample_t[idx_train_saved]\n",
    "    y_train = y_tb[t_train].astype(np.int64)\n",
    "    ce_w = make_ce_weights_3class(y_train)\n",
    "    ce_loss_fn = nn.CrossEntropyLoss(weight=ce_w, label_smoothing=float(CFG.get(\"label_smoothing\", 0.0)))\n",
    "\n",
    "    ev = eval_multitask_on_indices(model, X_scaled, E_scaled, indices.astype(np.int64), ce_loss_fn, cfg)\n",
    "\n",
    "    thr_trade = float(bundle[\"meta\"][\"thr_trade\"])\n",
    "    thr_dir = float(bundle[\"meta\"][\"thr_dir\"])\n",
    "    pnl = pnl_from_probs_3class(ev[\"prob3\"], ev[\"er\"], thr_trade, thr_dir, float(cfg[\"cost_bps\"]))\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 90)\n",
    "    print(label)\n",
    "    print(f\"bundle: {name}\")\n",
    "    print(f\"trade_auc={ev['trade_auc']:.3f} | dir_auc={ev['dir_auc']:.3f} | val/test loss={ev['loss']:.4f}\")\n",
    "    print(f\"soft util mean={ev['util_mean']:.6f} | pos_abs mean={ev['pos_abs_mean']:.4f}\")\n",
    "    print(f\"pnl_sum={pnl['pnl_sum']:.4f} | trade_rate={pnl['trade_rate']:.3f} | trades={pnl['n_trades']}\")\n",
    "    return {\"eval\": ev, \"pnl\": pnl}\n",
    "\n",
    "\n",
    "# Evaluate overall best on FINAL holdout (10%) without refit\n",
    "holdout_indices = idx_final_test.astype(np.int64)\n",
    "_ = evaluate_bundle_on_indices(ART_DIR, overall_best_name, holdout_indices, label=\"FINAL HOLDOUT (10%) using overall_best\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================================================\n",
      "PRODUCTION FIT: train on CV(90%) -> select thresholds on val_final -> eval on FINAL holdout(10%)\n",
      "Sizes:\n",
      "  train_final: 10175\n",
      "  val_final  : 1130\n",
      "  holdout    : 1256\n",
      "True trade ratio (val_final): 0.504\n",
      "True trade ratio (holdout):   0.576\n",
      "[fold 99] ep 01 lr=9.80e-05 tr_loss=1.0672 (ce=1.0589, huber=0.0020, util=0.00002) val_loss=1.1575 (ce=1.1567, huber=0.0018, util=0.00005) val_trade_auc=0.572 val_dir_auc=0.513 sel=0.828 best=0.828@ep01 supports=[0.333, 0.333, 0.333]\n",
      "[fold 99] ep 02 lr=2.34e-04 tr_loss=1.0018 (ce=0.9924, huber=0.0019, util=0.00012) val_loss=1.1577 (ce=1.1569, huber=0.0016, util=0.00005) val_trade_auc=0.598 val_dir_auc=0.531 sel=0.863 best=0.863@ep02 supports=[0.334, 0.332, 0.334]\n",
      "[fold 99] ep 03 lr=3.00e-04 tr_loss=0.9750 (ce=0.9657, huber=0.0017, util=0.00028) val_loss=1.1783 (ce=1.1776, huber=0.0014, util=0.00010) val_trade_auc=0.611 val_dir_auc=0.536 sel=0.879 best=0.879@ep03 supports=[0.334, 0.332, 0.334]\n",
      "[fold 99] ep 04 lr=2.97e-04 tr_loss=0.9454 (ce=0.9361, huber=0.0015, util=0.00043) val_loss=1.1821 (ce=1.1814, huber=0.0012, util=0.00014) val_trade_auc=0.621 val_dir_auc=0.539 sel=0.890 best=0.890@ep04 supports=[0.335, 0.33, 0.335]\n",
      "[fold 99] ep 05 lr=2.90e-04 tr_loss=0.9296 (ce=0.9205, huber=0.0013, util=0.00058) val_loss=1.1884 (ce=1.1877, huber=0.0011, util=0.00012) val_trade_auc=0.625 val_dir_auc=0.530 sel=0.890 best=0.890@ep04 supports=[0.337, 0.327, 0.337]\n",
      "[fold 99] ep 06 lr=2.77e-04 tr_loss=0.9106 (ce=0.9015, huber=0.0012, util=0.00075) val_loss=1.1913 (ce=1.1907, huber=0.0009, util=0.00019) val_trade_auc=0.622 val_dir_auc=0.540 sel=0.891 best=0.891@ep06 supports=[0.339, 0.323, 0.339]\n",
      "[fold 99] ep 07 lr=2.61e-04 tr_loss=0.8649 (ce=0.8558, huber=0.0011, util=0.00102) val_loss=1.2166 (ce=1.2160, huber=0.0008, util=0.00001) val_trade_auc=0.617 val_dir_auc=0.515 sel=0.875 best=0.891@ep06 supports=[0.342, 0.317, 0.342]\n",
      "[fold 99] ep 08 lr=2.40e-04 tr_loss=0.8338 (ce=0.8249, huber=0.0009, util=0.00129) val_loss=1.2422 (ce=1.2416, huber=0.0008, util=0.00002) val_trade_auc=0.606 val_dir_auc=0.497 sel=0.854 best=0.891@ep06 supports=[0.344, 0.312, 0.344]\n",
      "[fold 99] ep 09 lr=2.17e-04 tr_loss=0.7897 (ce=0.7807, huber=0.0009, util=0.00154) val_loss=1.2720 (ce=1.2715, huber=0.0007, util=0.00026) val_trade_auc=0.611 val_dir_auc=0.551 sel=0.886 best=0.891@ep06 supports=[0.346, 0.307, 0.346]\n",
      "[fold 99] ep 10 lr=1.91e-04 tr_loss=0.7470 (ce=0.7381, huber=0.0008, util=0.00177) val_loss=1.3141 (ce=1.3135, huber=0.0007, util=-0.00000) val_trade_auc=0.606 val_dir_auc=0.486 sel=0.849 best=0.891@ep06 supports=[0.348, 0.304, 0.348]\n",
      "[fold 99] ep 11 lr=1.64e-04 tr_loss=0.7241 (ce=0.7152, huber=0.0008, util=0.00193) val_loss=1.3405 (ce=1.3400, huber=0.0006, util=0.00012) val_trade_auc=0.604 val_dir_auc=0.517 sel=0.862 best=0.891@ep06 supports=[0.349, 0.302, 0.349]\n",
      "[fold 99] ep 12 lr=1.36e-04 tr_loss=0.6915 (ce=0.6827, huber=0.0007, util=0.00211) val_loss=1.3415 (ce=1.3410, huber=0.0006, util=0.00004) val_trade_auc=0.598 val_dir_auc=0.505 sel=0.851 best=0.891@ep06 supports=[0.35, 0.299, 0.35]\n",
      "[fold 99] ep 13 lr=1.09e-04 tr_loss=0.6673 (ce=0.6585, huber=0.0007, util=0.00221) val_loss=1.3601 (ce=1.3595, huber=0.0006, util=0.00008) val_trade_auc=0.601 val_dir_auc=0.516 sel=0.859 best=0.891@ep06 supports=[0.351, 0.298, 0.351]\n",
      "[fold 99] chosen thresholds on VAL: thr_trade=0.700 thr_dir=0.600 | val pnl_sum=0.3500 val trade_rate=0.596\n",
      "[fold 99] TEST (fixed thresholds from VAL): trade_auc=0.505 dir_auc=0.517 pnl_sum=-0.0174 trade_rate=0.583 trades=732\n",
      "\n",
      "Saved production bundle as: production_best\n",
      "\n",
      "==========================================================================================\n",
      "TEST-ONLY FROM PRODUCTION BUNDLE (holdout)\n",
      "bundle: production_best\n",
      "trade_auc=0.505 | dir_auc=0.517 | val/test loss=1.2173\n",
      "soft util mean=-0.000137 | pos_abs mean=0.3833\n",
      "pnl_sum=-0.0174 | trade_rate=0.583 | trades=732\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# Step 14: Production fit on CV(90%) -> select thresholds on val_final -> eval holdout(10%)\n",
    "# ======================================================================\n",
    "\n",
    "def production_fit_and_save() -> str:\n",
    "    print(\"\\n\" + \"=\" * 90)\n",
    "    print(\"PRODUCTION FIT: train on CV(90%) -> select thresholds on val_final -> eval on FINAL holdout(10%)\")\n",
    "\n",
    "    val_w = max(1, int(CFG[\"val_window_frac\"] * n_samples_cv))\n",
    "    train_end = n_samples_cv - val_w\n",
    "\n",
    "    idx_train_final = np.arange(0, train_end, dtype=np.int64)\n",
    "    idx_val_final = np.arange(train_end, n_samples_cv, dtype=np.int64)\n",
    "    idx_holdout = idx_final_test.astype(np.int64)\n",
    "\n",
    "    print(\"Sizes:\")\n",
    "    print(\"  train_final:\", len(idx_train_final))\n",
    "    print(\"  val_final  :\", len(idx_val_final))\n",
    "    print(\"  holdout    :\", len(idx_holdout))\n",
    "    print(f\"True trade ratio (val_final): {split_trade_ratio(idx_val_final, sample_t, y_trade):.3f}\")\n",
    "    print(f\"True trade ratio (holdout):   {split_trade_ratio(idx_holdout, sample_t, y_trade):.3f}\")\n",
    "\n",
    "    X_scaled, node_params = fit_scale_nodes_train_only(X_node_raw, sample_t, idx_train_final, max_abs=CFG[\"max_abs_feat\"])\n",
    "    if bool(CFG.get(\"edge_scale\", True)):\n",
    "        edge_scaled, edge_params = fit_scale_edges_train_only(edge_feat, sample_t, idx_train_final, max_abs=CFG[\"max_abs_edge\"])\n",
    "    else:\n",
    "        edge_scaled = edge_feat.astype(np.float32)\n",
    "        edge_params = None\n",
    "\n",
    "    artifact = train_one_fold_multitask(\n",
    "        fold_id=99,\n",
    "        X_scaled=X_scaled,\n",
    "        edge_scaled=edge_scaled,\n",
    "        idx_train=idx_train_final,\n",
    "        idx_val=idx_val_final,\n",
    "        idx_test=idx_holdout,\n",
    "        node_scaler_params=node_params,\n",
    "        edge_scaler_params=edge_params,\n",
    "        cfg=CFG,\n",
    "    )\n",
    "\n",
    "    prod_name = \"production_best\"\n",
    "    extra_meta = {\n",
    "        \"kind\": \"production_best\",\n",
    "        \"fold\": 99,\n",
    "        \"best_epoch\": artifact[\"best_epoch\"],\n",
    "        \"best_sel\": artifact[\"best_sel\"],\n",
    "        \"thr_trade\": artifact[\"thr_trade\"],\n",
    "        \"thr_dir\": artifact[\"thr_dir\"],\n",
    "        \"idx_train\": artifact[\"idx_train\"].tolist(),\n",
    "        \"idx_val\": artifact[\"idx_val\"].tolist(),\n",
    "        \"idx_test\": artifact[\"idx_test\"].tolist(),  # holdout\n",
    "    }\n",
    "    save_bundle(\n",
    "        bundle_dir=ART_DIR,\n",
    "        name=prod_name,\n",
    "        model_state=artifact[\"model_state\"],\n",
    "        cfg=CFG,\n",
    "        node_scaler_params=artifact[\"node_scaler_params\"],\n",
    "        edge_scaler_params=artifact[\"edge_scaler_params\"],\n",
    "        extra_meta=extra_meta,\n",
    "    )\n",
    "    print(\"\\nSaved production bundle as:\", prod_name)\n",
    "    return prod_name\n",
    "\n",
    "\n",
    "production_name = production_fit_and_save()\n",
    "\n",
    "# Evaluate production bundle on its saved idx_test (holdout)\n",
    "prod_bundle = load_bundle(ART_DIR, production_name)\n",
    "idx_eval = np.asarray(prod_bundle[\"meta\"][\"idx_test\"], dtype=np.int64)\n",
    "_ = evaluate_bundle_on_indices(ART_DIR, production_name, idx_eval, label=\"TEST-ONLY FROM PRODUCTION BUNDLE (holdout)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single sample shapes:\n",
      "  x_seq: (240, 3, 15) (L,N,F)\n",
      "  e_seq: (240, 9, 20) (L,E,D)\n",
      "Forward outputs:\n",
      "  logits: (1, 3) expected (1,3)\n",
      "  ret_hat: (1,) expected (1,)\n",
      "  y_tb: 0 | exit_ret: -0.008445847779512405\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# Step 15: Sanity check: load a bundle and do a single forward pass\n",
    "# ======================================================================\n",
    "\n",
    "bundle_dbg = load_bundle(ART_DIR, overall_best_name)\n",
    "cfg_dbg = bundle_dbg[\"meta\"][\"cfg\"]\n",
    "\n",
    "model_dbg = GraphWaveNetMultiTask(\n",
    "    node_in=int(X_node_raw.shape[-1]),\n",
    "    edge_dim=int(edge_feat.shape[-1]),\n",
    "    cfg=cfg_dbg,\n",
    "    n_nodes=len(ASSETS),\n",
    "    target_node=TARGET_NODE,\n",
    ").to(DEVICE)\n",
    "model_dbg.load_state_dict(bundle_dbg[\"state\"])\n",
    "model_dbg.eval()\n",
    "\n",
    "X_dbg = apply_scaler_params(X_node_raw.astype(np.float32), bundle_dbg[\"node_scaler_params\"])\n",
    "E_dbg = apply_scaler_params(edge_feat.astype(np.float32), bundle_dbg[\"edge_scaler_params\"]) if bundle_dbg[\"edge_scaler_params\"] is not None else edge_feat.astype(np.float32)\n",
    "\n",
    "ds_dbg = LobGraphSequenceDataset3Class(X_dbg, E_dbg, y_tb, exit_ret, sample_t, np.arange(0, 2, dtype=np.int64), CFG[\"lookback\"])\n",
    "x_seq, e_seq, y0, er0, sidx0 = ds_dbg[0]\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits, ret_hat = model_dbg(x_seq.unsqueeze(0).to(DEVICE).float(), e_seq.unsqueeze(0).to(DEVICE).float(), return_aux=False)\n",
    "\n",
    "print(\"Single sample shapes:\")\n",
    "print(\"  x_seq:\", tuple(x_seq.shape), \"(L,N,F)\")\n",
    "print(\"  e_seq:\", tuple(e_seq.shape), \"(L,E,D)\")\n",
    "print(\"Forward outputs:\")\n",
    "print(\"  logits:\", tuple(logits.shape), \"expected (1,3)\")\n",
    "print(\"  ret_hat:\", tuple(ret_hat.shape), \"expected (1,)\")\n",
    "print(\"  y_tb:\", int(y0.item()), \"| exit_ret:\", float(er0.item()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recbole_new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
