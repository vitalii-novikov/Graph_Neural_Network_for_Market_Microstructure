{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nGraph WaveNet / MTGNN-style model (adaptive adjacency + dilated gated TCN)\\nTwo-head setup: trade? -> direction? + fixed-horizon return head.\\n\\nRequested changes preserved:\\n1) Fixed-horizon return for regression + soft utility (TB labels remain as ground truth source).\\n2) Stronger influence of utility (scaling + higher weight).\\n3) Selection metric: sel = soft_utility_scaled_mean + b * dir_auc (utility emphasized).\\n4) Correct lead-lag edges for 1–10 minutes.\\n\\nLogged each epoch:\\n- val_trade_auc and val_dir_auc\\n- losses and soft utility\\n\\nArtifacts:\\n- .pt stores ONLY model weights (state_dict)\\n- scalers stored as .npz (no pickle)\\n- meta stored as .json\\n'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Graph WaveNet / MTGNN-style model (adaptive adjacency + dilated gated TCN)\n",
    "Two-head setup: trade? -> direction? + fixed-horizon return head.\n",
    "\n",
    "Requested changes preserved:\n",
    "1) Fixed-horizon return for regression + soft utility (TB labels remain as ground truth source).\n",
    "2) Stronger influence of utility (scaling + higher weight).\n",
    "3) Selection metric: sel = soft_utility_scaled_mean + b * dir_auc (utility emphasized).\n",
    "4) Correct lead-lag edges for 1–10 minutes.\n",
    "\n",
    "Logged each epoch:\n",
    "- val_trade_auc and val_dir_auc\n",
    "- losses and soft utility\n",
    "\n",
    "Artifacts:\n",
    "- .pt stores ONLY model weights (state_dict)\n",
    "- scalers stored as .npz (no pickle)\n",
    "- meta stored as .json\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cpu\n",
      "Assets: ['ADA', 'BTC', 'ETH'] | Target: ETH\n",
      "Artifacts dir: /Users/vitalii/Desktop/Model_Market_Microstructure/Graph_Neural_Network_for_Market_Microstructure/TGNN2026/artifacts_MTGNN1m_heads&util\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# Step 0: Imports, seed, config (single unified block)\n",
    "# ======================================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "def seed_everything(seed: int = 1234) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "seed_everything(100)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "\n",
    "torch.set_num_threads(max(1, os.cpu_count() or 4))\n",
    "\n",
    "CFG: Dict[str, Any] = {\n",
    "    # data\n",
    "    \"freq\": \"1min\",\n",
    "    \"data_dir\": \"../dataset\",\n",
    "    \"final_test_frac\": 0.10,\n",
    "\n",
    "    # order book\n",
    "    \"book_levels\": 15,\n",
    "    \"top_levels\": 5,\n",
    "    \"near_levels\": 5,\n",
    "\n",
    "    # walk-forward windows (in sample-space)\n",
    "    \"train_min_frac\": 0.50,\n",
    "    \"val_window_frac\": 0.10,\n",
    "    \"test_window_frac\": 0.10,\n",
    "    \"step_window_frac\": 0.10,\n",
    "\n",
    "    # scaling\n",
    "    \"max_abs_feat\": 10.0,\n",
    "    \"max_abs_edge\": 6.0,\n",
    "\n",
    "    # correlations / graph\n",
    "    \"corr_windows\": [6 * 5, 12 * 5, 24 * 5, 48 * 5, 84 * 5],  # 30m,1h,2h,4h,7h\n",
    "    \"corr_lags\": list(range(0, 11)),  # lead-lag horizons: 0..10 minutes\n",
    "    \"edges_mode\": \"all_pairs\",        # \"manual\" | \"all_pairs\"\n",
    "    \"edges\": [(\"ADA\", \"BTC\"), (\"ADA\", \"ETH\"), (\"ETH\", \"BTC\")],  # used if edges_mode=\"manual\"\n",
    "    \"add_self_loops\": True,\n",
    "    \"edge_transform\": \"fisher\",       # \"none\" | \"fisher\"\n",
    "    \"edge_scale\": True,\n",
    "\n",
    "    # triple-barrier labels (source of ground truth)\n",
    "    \"tb_horizon\": 1 * 30,\n",
    "    \"lookback\": 4 * 12 * 5,\n",
    "    \"tb_pt_mult\": 1.2,\n",
    "    \"tb_sl_mult\": 1.1,\n",
    "    \"tb_min_barrier\": 0.001,\n",
    "    \"tb_max_barrier\": 0.006,\n",
    "\n",
    "    # fixed-horizon return for regression + utility\n",
    "    \"fixed_horizon\": 30,              # minutes: r_H(t) = sum lr_{t+1..t+H}\n",
    "    \"fixed_ret_clip\": 0.02,           # clip future return for ret/utility stability\n",
    "\n",
    "    # training\n",
    "    \"batch_size\": 128,\n",
    "    \"epochs\": 20,\n",
    "    \"lr\": 3e-4,\n",
    "    \"weight_decay\": 5e-4,\n",
    "    \"grad_clip\": 1.0,\n",
    "    \"dropout\": 0.15,\n",
    "\n",
    "    # stability tricks\n",
    "    \"use_weighted_sampler\": True,\n",
    "    \"use_onecycle\": True,\n",
    "\n",
    "    # model channels\n",
    "    \"gwn_residual_channels\": 64,\n",
    "    \"gwn_dilation_channels\": 64,\n",
    "    \"gwn_skip_channels\": 128,\n",
    "    \"gwn_end_channels\": 128,\n",
    "    \"gwn_blocks\": 3,\n",
    "    \"gwn_layers_per_block\": 2,\n",
    "    \"gwn_kernel_size\": 2,\n",
    "\n",
    "    # adaptive adjacency\n",
    "    \"adj_emb_dim\": 8,\n",
    "    \"adj_temperature\": 1.0,\n",
    "    \"adaptive_topk\": 3,\n",
    "\n",
    "    # adjacency regularization\n",
    "    \"adj_l1_lambda\": 1e-3,\n",
    "    \"adj_prior_lambda\": 1e-2,\n",
    "\n",
    "    # prior adjacency from edge_attr (last timestep)\n",
    "    \"prior_use_abs\": False,\n",
    "    \"prior_diag_boost\": 1.0,\n",
    "    \"prior_row_normalize\": True,\n",
    "\n",
    "    # trading eval\n",
    "    \"cost_bps\": 1.0,\n",
    "\n",
    "    # threshold sweep grids (val only)\n",
    "    \"thr_trade_grid\": [0.50, 0.55, 0.60, 0.65, 0.70, 0.75],\n",
    "    \"thr_dir_grid\":   [0.50, 0.55, 0.60, 0.65, 0.70],\n",
    "    \"eval_min_trades\": 50,\n",
    "    \"max_trade_rate_val\": 0.65,\n",
    "    \"trade_rate_penalty\": 0.10,\n",
    "    \"thr_objective\": \"pnl_sum\",\n",
    "    \"proxy_target_trades\": [50, 100, 200],\n",
    "\n",
    "    # selection metric: sel = soft_utility_scaled_mean + b * dir_auc\n",
    "    \"sel_b_dir_auc\": 0.30,\n",
    "\n",
    "    # optional anti-overtrading penalty on mean p_trade\n",
    "    \"trade_prob_penalty\": 0.05,\n",
    "\n",
    "    # loss weights (stronger utility)\n",
    "    \"loss_w_trade\": 0.35,              # BCE trade\n",
    "    \"loss_w_dir\": 0.35,                # BCE direction (only on true trades)\n",
    "    \"loss_w_ret\": 0.25,               # Huber fixed-horizon\n",
    "    \"loss_w_utility\": 2.00,           # utility weight increased\n",
    "\n",
    "    # regression / utility stability\n",
    "    \"ret_huber_delta\": 0.01,\n",
    "    \"utility_k\": 3.0,\n",
    "    \"utility_scale\": 500.0,\n",
    "\n",
    "    # artifact saving\n",
    "    \"artifact_dir\": \"./artifacts_MTGNN1m_heads&util\",\n",
    "}\n",
    "\n",
    "ASSETS = [\"ADA\", \"BTC\", \"ETH\"]\n",
    "ASSET2IDX = {a: i for i, a in enumerate(ASSETS)}\n",
    "TARGET_ASSET = \"ETH\"\n",
    "TARGET_NODE = ASSET2IDX[TARGET_ASSET]\n",
    "\n",
    "ART_DIR = Path(CFG[\"artifact_dir\"])\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Assets:\", ASSETS, \"| Target:\", TARGET_ASSET)\n",
    "print(\"Artifacts dir:\", str(ART_DIR.resolve()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EDGE_LIST: ['ADA->BTC', 'ADA->ETH', 'BTC->ADA', 'BTC->ETH', 'ETH->ADA', 'ETH->BTC', 'ADA->ADA', 'BTC->BTC', 'ETH->ETH']\n",
      "EDGE_INDEX: [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1], [0, 0], [1, 1], [2, 2]]\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# Step 0.1: Graph edges\n",
    "# ======================================================================\n",
    "\n",
    "def build_edge_list(cfg: Dict[str, Any], assets: List[str]) -> List[Tuple[str, str]]:\n",
    "    mode = str(cfg.get(\"edges_mode\", \"manual\"))\n",
    "    if mode == \"manual\":\n",
    "        edges = list(cfg[\"edges\"])\n",
    "    elif mode == \"all_pairs\":\n",
    "        edges = [(s, t) for s in assets for t in assets if s != t]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown edges_mode={mode}\")\n",
    "\n",
    "    if bool(cfg.get(\"add_self_loops\", True)):\n",
    "        edges = edges + [(a, a) for a in assets]\n",
    "    return edges\n",
    "\n",
    "\n",
    "EDGE_LIST = build_edge_list(CFG, ASSETS)\n",
    "EDGE_NAMES = [f\"{s}->{t}\" for (s, t) in EDGE_LIST]\n",
    "EDGE_INDEX = torch.tensor([[ASSET2IDX[s], ASSET2IDX[t]] for (s, t) in EDGE_LIST], dtype=torch.long)\n",
    "\n",
    "print(\"EDGE_LIST:\", EDGE_NAMES)\n",
    "print(\"EDGE_INDEX:\", EDGE_INDEX.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded df: (12831, 106)\n",
      "Time range: 2021-04-07 11:34:00+00:00 -> 2021-04-16 10:15:00+00:00\n",
      "                  timestamp      ADA  spread_ADA      buys_ADA      sells_ADA  \\\n",
      "0 2021-04-07 11:34:00+00:00  1.16205      0.0001  56936.467913  258248.957367   \n",
      "1 2021-04-07 11:35:00+00:00  1.16800      0.0022  56491.336799   78665.286640   \n",
      "\n",
      "   bids_vol_ADA_0  bids_vol_ADA_1  bids_vol_ADA_2  bids_vol_ADA_3  \\\n",
      "0      876.869995     5984.169922        5.810000       18.240000   \n",
      "1    33769.671875    23137.169922      550.299988      550.299988   \n",
      "\n",
      "   bids_vol_ADA_4  ...  asks_vol_ETH_8  asks_vol_ETH_9  asks_vol_ETH_10  \\\n",
      "0    19844.640625  ...      373.700012      196.699997      2059.709961   \n",
      "1    19012.320312  ...     3873.709961     1954.630005       197.039993   \n",
      "\n",
      "   asks_vol_ETH_11  asks_vol_ETH_12  asks_vol_ETH_13  asks_vol_ETH_14  \\\n",
      "0      3874.989990      5901.209961       178.289993     28512.160156   \n",
      "1     12661.990234     20006.970703     28562.310547      3874.379883   \n",
      "\n",
      "     lr_ADA    lr_BTC    lr_ETH  \n",
      "0  0.000000  0.000000  0.000000  \n",
      "1  0.005107  0.000937  0.001931  \n",
      "\n",
      "[2 rows x 106 columns]\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# Step 1: Data loading\n",
    "# ======================================================================\n",
    "\n",
    "def load_asset(asset: str, freq: str, data_dir: Path, book_levels: int, part: Tuple[int, int] = (0, 80)) -> pd.DataFrame:\n",
    "    path = data_dir / f\"{asset}_{freq}.csv\"\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.iloc[int(len(df) * part[0] / 100): int(len(df) * part[1] / 100)]\n",
    "\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"system_time\"]).dt.round(\"min\")\n",
    "    df = df.sort_values(\"timestamp\").set_index(\"timestamp\")\n",
    "\n",
    "    bid_cols = [f\"bids_notional_{i}\" for i in range(book_levels)]\n",
    "    ask_cols = [f\"asks_notional_{i}\" for i in range(book_levels)]\n",
    "\n",
    "    needed = [\"midpoint\", \"spread\", \"buys\", \"sells\"] + bid_cols + ask_cols\n",
    "    missing = [c for c in needed if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"{asset}: missing columns in CSV: {missing[:10]}{'...' if len(missing) > 10 else ''}\")\n",
    "\n",
    "    return df[needed]\n",
    "\n",
    "\n",
    "def load_all_assets() -> pd.DataFrame:\n",
    "    freq = CFG[\"freq\"]\n",
    "    data_dir = Path(CFG[\"data_dir\"])\n",
    "    book_levels = CFG[\"book_levels\"]\n",
    "\n",
    "    def rename_cols(df_one: pd.DataFrame, asset: str) -> pd.DataFrame:\n",
    "        rename_map = {\n",
    "            \"midpoint\": asset,\n",
    "            \"buys\": f\"buys_{asset}\",\n",
    "            \"sells\": f\"sells_{asset}\",\n",
    "            \"spread\": f\"spread_{asset}\",\n",
    "        }\n",
    "        for i in range(book_levels):\n",
    "            rename_map[f\"bids_notional_{i}\"] = f\"bids_vol_{asset}_{i}\"\n",
    "            rename_map[f\"asks_notional_{i}\"] = f\"asks_vol_{asset}_{i}\"\n",
    "        return df_one.rename(columns=rename_map)\n",
    "\n",
    "    df_ada = rename_cols(load_asset(\"ADA\", freq, data_dir, book_levels, part=(0, 75)), \"ADA\")\n",
    "    df_btc = rename_cols(load_asset(\"BTC\", freq, data_dir, book_levels, part=(0, 75)), \"BTC\")\n",
    "    df_eth = rename_cols(load_asset(\"ETH\", freq, data_dir, book_levels, part=(0, 75)), \"ETH\")\n",
    "\n",
    "    df = df_ada.join(df_btc).join(df_eth).reset_index()\n",
    "    return df\n",
    "\n",
    "\n",
    "df = load_all_assets()\n",
    "for a in ASSETS:\n",
    "    df[f\"lr_{a}\"] = np.log(df[a]).diff().fillna(0.0)\n",
    "\n",
    "print(\"Loaded df:\", df.shape)\n",
    "print(\"Time range:\", df[\"timestamp\"].min(), \"->\", df[\"timestamp\"].max())\n",
    "print(df.head(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edge_feat shape: (12831, 9, 55) (T,E,edge_dim)\n",
      "edge_dim = 55\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# Step 2: Edge features (rolling corr with lead-lag 0..10 minutes)\n",
    "# ======================================================================\n",
    "\n",
    "def _fisher_z(x: np.ndarray, eps: float = 1e-6) -> np.ndarray:\n",
    "    x = np.clip(x, -0.999, 0.999)\n",
    "    return 0.5 * np.log((1.0 + x + eps) / (1.0 - x + eps))\n",
    "\n",
    "\n",
    "def build_corr_array(\n",
    "    df_: pd.DataFrame,\n",
    "    corr_windows: List[int],\n",
    "    edges: List[Tuple[str, str]],\n",
    "    lags: List[int],\n",
    "    transform: str = \"fisher\",\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Edge features per time:\n",
    "      for edge s->t:\n",
    "        for lag in lags:\n",
    "          corr(lr_s.shift(lag), lr_t) over rolling window\n",
    "    No leakage: shift(lag>0) uses past of source.\n",
    "    Self-loop edges a->a: constant 1.0.\n",
    "    \"\"\"\n",
    "    T_ = len(df_)\n",
    "    E_ = len(edges)\n",
    "    W_ = len(corr_windows)\n",
    "    Lg = len(lags)\n",
    "    out = np.zeros((T_, E_, W_ * Lg), dtype=np.float32)\n",
    "\n",
    "    lr_map = {a: df_[f\"lr_{a}\"].astype(float) for a in ASSETS}\n",
    "\n",
    "    for ei, (s, t) in enumerate(edges):\n",
    "        if s == t:\n",
    "            out[:, ei, :] = 1.0\n",
    "            continue\n",
    "\n",
    "        src0 = lr_map[s]\n",
    "        dst0 = lr_map[t]\n",
    "\n",
    "        feat_idx = 0\n",
    "        for lag in lags:\n",
    "            src = src0.shift(int(lag)) if int(lag) > 0 else src0\n",
    "            for w in corr_windows:\n",
    "                r = src.rolling(int(w), min_periods=1).corr(dst0)\n",
    "                r = np.nan_to_num(r.to_numpy(dtype=np.float32), nan=0.0, posinf=0.0, neginf=0.0)\n",
    "                if transform == \"fisher\":\n",
    "                    r = _fisher_z(r).astype(np.float32)\n",
    "                out[:, ei, feat_idx] = r\n",
    "                feat_idx += 1\n",
    "\n",
    "    return out.astype(np.float32)\n",
    "\n",
    "\n",
    "edge_feat = build_corr_array(\n",
    "    df,\n",
    "    CFG[\"corr_windows\"],\n",
    "    EDGE_LIST,\n",
    "    CFG[\"corr_lags\"],\n",
    "    transform=str(CFG.get(\"edge_transform\", \"fisher\")),\n",
    ")\n",
    "\n",
    "print(\"edge_feat shape:\", edge_feat.shape, \"(T,E,edge_dim)\")\n",
    "print(\"edge_dim =\", edge_feat.shape[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TB dist [down,flat,up]: [2875 7413 2543]\n",
      "True trade ratio: 0.42225859247135844\n",
      "fixed_ret stats: mean= 0.000453889777418226 std= 0.005364810582250357\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# Step 3: Labels (TB for trade/dir) + fixed-horizon return\n",
    "# ======================================================================\n",
    "\n",
    "EPS = 1e-6\n",
    "\n",
    "\n",
    "def triple_barrier_labels_from_lr(\n",
    "    lr: pd.Series,\n",
    "    horizon: int,\n",
    "    vol_window: int,\n",
    "    pt_mult: float,\n",
    "    sl_mult: float,\n",
    "    min_barrier: float,\n",
    "    max_barrier: float,\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      y_tb: {0=down, 1=flat/no-trade, 2=up}\n",
    "      exit_ret: realized log-return to exit (tp/sl/timeout)\n",
    "      exit_t: exit index\n",
    "      thr: barrier per t\n",
    "    \"\"\"\n",
    "    lr = lr.astype(float).copy()\n",
    "    T = len(lr)\n",
    "\n",
    "    vol = lr.rolling(vol_window, min_periods=max(10, vol_window // 10)).std().shift(1)\n",
    "    thr = (vol * np.sqrt(horizon)).clip(lower=min_barrier, upper=max_barrier)\n",
    "\n",
    "    y = np.ones(T, dtype=np.int64)\n",
    "    exit_ret = np.zeros(T, dtype=np.float32)\n",
    "    exit_t = np.arange(T, dtype=np.int64)\n",
    "\n",
    "    lr_np = lr.fillna(0.0).to_numpy(dtype=np.float64)\n",
    "    thr_np = thr.fillna(min_barrier).to_numpy(dtype=np.float64)\n",
    "\n",
    "    for t in range(T - horizon - 1):\n",
    "        up = pt_mult * thr_np[t]\n",
    "        dn = -sl_mult * thr_np[t]\n",
    "\n",
    "        cum = 0.0\n",
    "        hit = 1\n",
    "        et = t + horizon\n",
    "        er = 0.0\n",
    "\n",
    "        for dt in range(1, horizon + 1):\n",
    "            cum += lr_np[t + dt]\n",
    "            if cum >= up:\n",
    "                hit, et, er = 2, t + dt, cum\n",
    "                break\n",
    "            if cum <= dn:\n",
    "                hit, et, er = 0, t + dt, cum\n",
    "                break\n",
    "\n",
    "        if hit == 1:\n",
    "            er = float(np.sum(lr_np[t + 1: t + horizon + 1]))\n",
    "            et = t + horizon\n",
    "\n",
    "        y[t] = hit\n",
    "        exit_ret[t] = er\n",
    "        exit_t[t] = et\n",
    "\n",
    "    return y, exit_ret, exit_t, thr_np\n",
    "\n",
    "\n",
    "def fixed_horizon_future_return(lr: np.ndarray, H: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    r_H(t) = sum_{i=1..H} lr[t+i]\n",
    "    For last H timesteps, return 0.0 (not used by sampling).\n",
    "    \"\"\"\n",
    "    lr = np.asarray(lr, dtype=np.float64)\n",
    "    T = lr.shape[0]\n",
    "    out = np.zeros(T, dtype=np.float32)\n",
    "    if H <= 0:\n",
    "        return out\n",
    "    for t in range(0, T - H - 1):\n",
    "        out[t] = float(lr[t + 1: t + H + 1].sum())\n",
    "    return out\n",
    "\n",
    "\n",
    "y_tb, exit_ret, exit_t, tb_thr = triple_barrier_labels_from_lr(\n",
    "    df[\"lr_ETH\"],\n",
    "    horizon=int(CFG[\"tb_horizon\"]),\n",
    "    vol_window=int(CFG[\"lookback\"]),\n",
    "    pt_mult=float(CFG[\"tb_pt_mult\"]),\n",
    "    sl_mult=float(CFG[\"tb_sl_mult\"]),\n",
    "    min_barrier=float(CFG[\"tb_min_barrier\"]),\n",
    "    max_barrier=float(CFG[\"tb_max_barrier\"]),\n",
    ")\n",
    "\n",
    "y_trade = (y_tb != 1).astype(np.int64)                 # 1 if trade (SHORT or LONG)\n",
    "y_dir = (y_tb == 2).astype(np.int64)                   # 1 if LONG, 0 if SHORT (meaningful only when y_trade=1)\n",
    "\n",
    "fixed_ret = fixed_horizon_future_return(df[\"lr_ETH\"].to_numpy(dtype=np.float64), int(CFG[\"fixed_horizon\"]))\n",
    "\n",
    "dist = np.bincount(y_tb, minlength=3)\n",
    "print(\"TB dist [down,flat,up]:\", dist)\n",
    "print(\"True trade ratio:\", float(y_trade.mean()))\n",
    "print(\"fixed_ret stats: mean=\", float(np.mean(fixed_ret)), \"std=\", float(np.std(fixed_ret)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_node_raw: (12831, 3, 15) edge_feat: (12831, 9, 55)\n",
      "n_samples: 12561 | t range: 239 -> 12799\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# Step 4: Node features\n",
    "# ======================================================================\n",
    "\n",
    "def safe_log1p(x: np.ndarray) -> np.ndarray:\n",
    "    return np.log1p(np.maximum(x, 0.0))\n",
    "\n",
    "\n",
    "def build_node_tensor(df_: pd.DataFrame) -> Tuple[np.ndarray, List[str]]:\n",
    "    \"\"\"\n",
    "    Features per asset:\n",
    "      lr, spread,\n",
    "      log_buys, log_sells, ofi,\n",
    "      DI_15,\n",
    "      DI_L0..DI_L4,\n",
    "      near_ratio_bid, near_ratio_ask,\n",
    "      di_near, di_far\n",
    "    \"\"\"\n",
    "    book_levels = int(CFG[\"book_levels\"])\n",
    "    top_k = int(CFG[\"top_levels\"])\n",
    "    near_k = int(CFG[\"near_levels\"])\n",
    "\n",
    "    if near_k >= book_levels:\n",
    "        raise ValueError(\"CFG['near_levels'] must be < CFG['book_levels']\")\n",
    "\n",
    "    feat_names = [\n",
    "        \"lr\", \"spread\",\n",
    "        \"log_buys\", \"log_sells\", \"ofi\",\n",
    "        \"DI_15\",\n",
    "        \"DI_L0\", \"DI_L1\", \"DI_L2\", \"DI_L3\", \"DI_L4\",\n",
    "        \"near_ratio_bid\", \"near_ratio_ask\",\n",
    "        \"di_near\", \"di_far\",\n",
    "    ]\n",
    "\n",
    "    feats_all = []\n",
    "    for a in ASSETS:\n",
    "        lr = df_[f\"lr_{a}\"].values.astype(np.float32)\n",
    "        spread = df_[f\"spread_{a}\"].values.astype(np.float32)\n",
    "\n",
    "        buys = df_[f\"buys_{a}\"].values.astype(np.float32)\n",
    "        sells = df_[f\"sells_{a}\"].values.astype(np.float32)\n",
    "\n",
    "        log_buys = safe_log1p(buys).astype(np.float32)\n",
    "        log_sells = safe_log1p(sells).astype(np.float32)\n",
    "\n",
    "        ofi = ((buys - sells) / (buys + sells + EPS)).astype(np.float32)\n",
    "\n",
    "        bids_lvls = np.stack([df_[f\"bids_vol_{a}_{i}\"].values.astype(np.float32) for i in range(book_levels)], axis=1)\n",
    "        asks_lvls = np.stack([df_[f\"asks_vol_{a}_{i}\"].values.astype(np.float32) for i in range(book_levels)], axis=1)\n",
    "\n",
    "        bid_sum = bids_lvls.sum(axis=1)\n",
    "        ask_sum = asks_lvls.sum(axis=1)\n",
    "        di_15 = ((bid_sum - ask_sum) / (bid_sum + ask_sum + EPS)).astype(np.float32)\n",
    "\n",
    "        di_levels = []\n",
    "        for i in range(top_k):\n",
    "            b = bids_lvls[:, i]\n",
    "            s = asks_lvls[:, i]\n",
    "            di_levels.append(((b - s) / (b + s + EPS)).astype(np.float32))\n",
    "        di_l0_4 = np.stack(di_levels, axis=1)\n",
    "\n",
    "        bid_near = bids_lvls[:, :near_k].sum(axis=1)\n",
    "        ask_near = asks_lvls[:, :near_k].sum(axis=1)\n",
    "        bid_far = bids_lvls[:, near_k:].sum(axis=1)\n",
    "        ask_far = asks_lvls[:, near_k:].sum(axis=1)\n",
    "\n",
    "        near_ratio_bid = (bid_near / (bid_far + EPS)).astype(np.float32)\n",
    "        near_ratio_ask = (ask_near / (ask_far + EPS)).astype(np.float32)\n",
    "\n",
    "        di_near = ((bid_near - ask_near) / (bid_near + ask_near + EPS)).astype(np.float32)\n",
    "        di_far = ((bid_far - ask_far) / (bid_far + ask_far + EPS)).astype(np.float32)\n",
    "\n",
    "        Xa = np.column_stack([\n",
    "            lr, spread,\n",
    "            log_buys, log_sells, ofi,\n",
    "            di_15,\n",
    "            di_l0_4[:, 0], di_l0_4[:, 1], di_l0_4[:, 2], di_l0_4[:, 3], di_l0_4[:, 4],\n",
    "            near_ratio_bid, near_ratio_ask,\n",
    "            di_near, di_far,\n",
    "        ]).astype(np.float32)\n",
    "\n",
    "        feats_all.append(Xa)\n",
    "\n",
    "    X = np.stack(feats_all, axis=1).astype(np.float32)  # (T,N,F)\n",
    "    return X, feat_names\n",
    "\n",
    "\n",
    "X_node_raw, node_feat_names = build_node_tensor(df)\n",
    "\n",
    "T = len(df)\n",
    "L = int(CFG[\"lookback\"])\n",
    "H_tb = int(CFG[\"tb_horizon\"])\n",
    "H_fixed = int(CFG[\"fixed_horizon\"])\n",
    "\n",
    "t_min = L - 1\n",
    "t_max = T - max(H_tb, H_fixed) - 2\n",
    "sample_t = np.arange(t_min, t_max + 1)\n",
    "n_samples = len(sample_t)\n",
    "\n",
    "print(\"X_node_raw:\", X_node_raw.shape, \"edge_feat:\", edge_feat.shape)\n",
    "print(\"n_samples:\", n_samples, \"| t range:\", int(sample_t[0]), \"->\", int(sample_t[-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holdout split:\n",
      "  n_samples total: 12561\n",
      "  n_samples CV   : 11305\n",
      "  n_samples FINAL: 1256\n",
      "\n",
      "Walk-forward folds: 4\n",
      "  fold 1: train=5652 | val=1130 | test=1130\n",
      "  fold 2: train=6782 | val=1130 | test=1130\n",
      "  fold 3: train=7912 | val=1130 | test=1130\n",
      "  fold 4: train=9042 | val=1130 | test=1130\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# Step 5: Splits\n",
    "# ======================================================================\n",
    "\n",
    "def make_final_holdout_split(n_samples_: int, final_test_frac: float) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    if not (0.0 < final_test_frac < 0.5):\n",
    "        raise ValueError(\"final_test_frac should be in (0, 0.5)\")\n",
    "    n_final = max(1, int(round(final_test_frac * n_samples_)))\n",
    "    n_cv = n_samples_ - n_final\n",
    "    if n_cv <= 50:\n",
    "        raise ValueError(\"Too few samples left for CV after holdout split.\")\n",
    "    idx_cv = np.arange(0, n_cv, dtype=np.int64)\n",
    "    idx_final = np.arange(n_cv, n_samples_, dtype=np.int64)\n",
    "    return idx_cv, idx_final\n",
    "\n",
    "\n",
    "def make_walk_forward_splits(\n",
    "    n_samples_: int,\n",
    "    train_min_frac: float,\n",
    "    val_window_frac: float,\n",
    "    test_window_frac: float,\n",
    "    step_window_frac: float,\n",
    ") -> List[Tuple[np.ndarray, np.ndarray, np.ndarray]]:\n",
    "    train_min = int(train_min_frac * n_samples_)\n",
    "    val_w = max(1, int(val_window_frac * n_samples_))\n",
    "    test_w = max(1, int(test_window_frac * n_samples_))\n",
    "    step_w = max(1, int(step_window_frac * n_samples_))\n",
    "\n",
    "    splits = []\n",
    "    start = train_min\n",
    "    while True:\n",
    "        tr_end = start\n",
    "        va_end = tr_end + val_w\n",
    "        te_end = va_end + test_w\n",
    "        if te_end > n_samples_:\n",
    "            break\n",
    "\n",
    "        idx_train = np.arange(0, tr_end, dtype=np.int64)\n",
    "        idx_val = np.arange(tr_end, va_end, dtype=np.int64)\n",
    "        idx_test = np.arange(va_end, te_end, dtype=np.int64)\n",
    "        splits.append((idx_train, idx_val, idx_test))\n",
    "\n",
    "        start += step_w\n",
    "\n",
    "    return splits\n",
    "\n",
    "\n",
    "idx_cv_all, idx_final_test = make_final_holdout_split(n_samples, float(CFG[\"final_test_frac\"]))\n",
    "n_samples_cv = len(idx_cv_all)\n",
    "\n",
    "walk_splits = make_walk_forward_splits(\n",
    "    n_samples_=n_samples_cv,\n",
    "    train_min_frac=float(CFG[\"train_min_frac\"]),\n",
    "    val_window_frac=float(CFG[\"val_window_frac\"]),\n",
    "    test_window_frac=float(CFG[\"test_window_frac\"]),\n",
    "    step_window_frac=float(CFG[\"step_window_frac\"]),\n",
    ")\n",
    "\n",
    "print(\"Holdout split:\")\n",
    "print(f\"  n_samples total: {n_samples}\")\n",
    "print(f\"  n_samples CV   : {len(idx_cv_all)}\")\n",
    "print(f\"  n_samples FINAL: {len(idx_final_test)}\")\n",
    "print(\"\\nWalk-forward folds:\", len(walk_splits))\n",
    "for i, (a, b, c) in enumerate(walk_splits, 1):\n",
    "    print(f\"  fold {i}: train={len(a)} | val={len(b)} | test={len(c)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# Step 6: Dataset, scaling helpers, sampler\n",
    "# ======================================================================\n",
    "\n",
    "class LobGraphSequenceDatasetTwoHeadFixedH(Dataset):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      x_seq:      (L,N,F)\n",
    "      e_seq:      (L,E,D)\n",
    "      y_trade:    scalar in {0,1}\n",
    "      y_dir:      scalar in {0,1} (meaningful only when y_trade=1)\n",
    "      exit_ret:   scalar (TB realized return to exit) - for threshold sweep / PnL eval\n",
    "      fixed_ret:  scalar (fixed-horizon future return) - for regression + utility\n",
    "      sidx:       scalar sample index\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        X_node: np.ndarray,\n",
    "        E_feat: np.ndarray,\n",
    "        y_trade_arr: np.ndarray,\n",
    "        y_dir_arr: np.ndarray,\n",
    "        exit_ret_arr: np.ndarray,\n",
    "        fixed_ret_arr: np.ndarray,\n",
    "        sample_t_: np.ndarray,\n",
    "        indices: np.ndarray,\n",
    "        lookback: int,\n",
    "    ):\n",
    "        self.X_node = X_node\n",
    "        self.E_feat = E_feat\n",
    "        self.y_trade = y_trade_arr\n",
    "        self.y_dir = y_dir_arr\n",
    "        self.exit_ret = exit_ret_arr\n",
    "        self.fixed_ret = fixed_ret_arr\n",
    "        self.sample_t = sample_t_\n",
    "        self.indices = indices.astype(np.int64)\n",
    "        self.L = int(lookback)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return int(len(self.indices))\n",
    "\n",
    "    def __getitem__(self, i: int):\n",
    "        sidx = int(self.indices[i])\n",
    "        t = int(self.sample_t[sidx])\n",
    "        t0 = t - self.L + 1\n",
    "\n",
    "        x_seq = self.X_node[t0:t + 1]\n",
    "        e_seq = self.E_feat[t0:t + 1]\n",
    "\n",
    "        yt = int(self.y_trade[t])\n",
    "        yd = int(self.y_dir[t])\n",
    "\n",
    "        er_exit = float(self.exit_ret[t])\n",
    "        er_fixed = float(self.fixed_ret[t])\n",
    "\n",
    "        return (\n",
    "            torch.from_numpy(x_seq),\n",
    "            torch.from_numpy(e_seq),\n",
    "            torch.tensor(yt, dtype=torch.float32),\n",
    "            torch.tensor(yd, dtype=torch.float32),\n",
    "            torch.tensor(er_exit, dtype=torch.float32),\n",
    "            torch.tensor(er_fixed, dtype=torch.float32),\n",
    "            torch.tensor(sidx, dtype=torch.long),\n",
    "        )\n",
    "\n",
    "\n",
    "def collate_fn_twohead(batch):\n",
    "    xs, es, ytr, ydir, er_exit, er_fixed, sidxs = zip(*batch)\n",
    "    return (\n",
    "        torch.stack(xs, 0),\n",
    "        torch.stack(es, 0),\n",
    "        torch.stack(ytr, 0),\n",
    "        torch.stack(ydir, 0),\n",
    "        torch.stack(er_exit, 0),\n",
    "        torch.stack(er_fixed, 0),\n",
    "        torch.stack(sidxs, 0),\n",
    "    )\n",
    "\n",
    "\n",
    "def make_weighted_sampler_trade(y_trade_np: np.ndarray) -> WeightedRandomSampler:\n",
    "    \"\"\"\n",
    "    Weighted sampler for binary trade label.\n",
    "    \"\"\"\n",
    "    y = np.asarray(y_trade_np, dtype=np.int64)\n",
    "    counts = np.bincount(y, minlength=2).astype(np.float64)\n",
    "    counts = np.maximum(counts, 1.0)\n",
    "    w = counts.sum() / (2.0 * counts)\n",
    "    sample_w = w[y].astype(np.float64)\n",
    "    return WeightedRandomSampler(torch.tensor(sample_w, dtype=torch.double), num_samples=len(sample_w), replacement=True)\n",
    "\n",
    "\n",
    "def fit_scale_nodes_train_only(X_node_raw_: np.ndarray, sample_t_: np.ndarray, idx_train: np.ndarray, max_abs: float) -> Tuple[np.ndarray, Dict[str, Any]]:\n",
    "    last_train_t = int(sample_t_[int(idx_train[-1])])\n",
    "    train_time_mask = np.arange(0, last_train_t + 1)\n",
    "\n",
    "    X_train_time = X_node_raw_[train_time_mask]\n",
    "    _, _, Fdim = X_train_time.shape\n",
    "\n",
    "    scaler = RobustScaler(with_centering=True, with_scaling=True, quantile_range=(5.0, 95.0))\n",
    "    scaler.fit(X_train_time.reshape(-1, Fdim))\n",
    "\n",
    "    X_scaled = scaler.transform(X_node_raw_.reshape(-1, Fdim)).reshape(X_node_raw_.shape).astype(np.float32)\n",
    "    X_scaled = np.clip(X_scaled, -max_abs, max_abs).astype(np.float32)\n",
    "    X_scaled = np.nan_to_num(X_scaled, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n",
    "\n",
    "    params = {\"center_\": scaler.center_.astype(np.float32), \"scale_\": scaler.scale_.astype(np.float32), \"max_abs\": float(max_abs)}\n",
    "    return X_scaled, params\n",
    "\n",
    "\n",
    "def fit_scale_edges_train_only(E_raw_: np.ndarray, sample_t_: np.ndarray, idx_train: np.ndarray, max_abs: float) -> Tuple[np.ndarray, Dict[str, Any]]:\n",
    "    last_train_t = int(sample_t_[int(idx_train[-1])])\n",
    "    train_time_mask = np.arange(0, last_train_t + 1)\n",
    "\n",
    "    E_train_time = E_raw_[train_time_mask]\n",
    "    _, _, D = E_train_time.shape\n",
    "\n",
    "    scaler = RobustScaler(with_centering=True, with_scaling=True, quantile_range=(5.0, 95.0))\n",
    "    scaler.fit(E_train_time.reshape(-1, D))\n",
    "\n",
    "    E_scaled = scaler.transform(E_raw_.reshape(-1, D)).reshape(E_raw_.shape).astype(np.float32)\n",
    "    E_scaled = np.clip(E_scaled, -max_abs, max_abs).astype(np.float32)\n",
    "    E_scaled = np.nan_to_num(E_scaled, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n",
    "\n",
    "    params = {\"center_\": scaler.center_.astype(np.float32), \"scale_\": scaler.scale_.astype(np.float32), \"max_abs\": float(max_abs)}\n",
    "    return E_scaled, params\n",
    "\n",
    "\n",
    "def apply_scaler_params(X: np.ndarray, params: Dict[str, Any]) -> np.ndarray:\n",
    "    center = np.asarray(params[\"center_\"], dtype=np.float32)\n",
    "    scale = np.asarray(params[\"scale_\"], dtype=np.float32)\n",
    "    max_abs = float(params[\"max_abs\"])\n",
    "    X2 = (X - center) / (scale + 1e-12)\n",
    "    X2 = np.clip(X2, -max_abs, max_abs)\n",
    "    return np.nan_to_num(X2, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n",
    "\n",
    "\n",
    "def split_trade_ratio(indices: np.ndarray, sample_t_: np.ndarray, y_trade_arr: np.ndarray) -> float:\n",
    "    tt = sample_t_[indices]\n",
    "    return float(y_trade_arr[tt].mean()) if len(tt) else float(\"nan\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# Step 7: Graph WaveNet model (trade logit + dir logit + fixed_ret_hat)\n",
    "# ======================================================================\n",
    "\n",
    "def build_static_adjacency_from_edges(edge_index: torch.Tensor, n_nodes: int, eps: float = 1e-8) -> torch.Tensor:\n",
    "    A = torch.zeros((n_nodes, n_nodes), dtype=torch.float32)\n",
    "    src = edge_index[:, 0].long()\n",
    "    dst = edge_index[:, 1].long()\n",
    "    A[src, dst] = 1.0\n",
    "    A = A / (A.sum(dim=-1, keepdim=True) + eps)\n",
    "    return A\n",
    "\n",
    "\n",
    "def build_adj_prior_from_edge_attr(\n",
    "    edge_attr_last: torch.Tensor,\n",
    "    edge_index: torch.Tensor,\n",
    "    n_nodes: int,\n",
    "    use_abs: bool,\n",
    "    diag_boost: float,\n",
    "    row_normalize: bool,\n",
    "    eps: float = 1e-8,\n",
    ") -> torch.Tensor:\n",
    "    edge_attr_last = torch.nan_to_num(edge_attr_last, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    B, E, D = edge_attr_last.shape\n",
    "    r = edge_attr_last.mean(dim=-1)\n",
    "    if use_abs:\n",
    "        r = r.abs()\n",
    "    w = torch.sigmoid(r)\n",
    "\n",
    "    A = torch.zeros((B, n_nodes, n_nodes), device=edge_attr_last.device, dtype=edge_attr_last.dtype)\n",
    "    src = edge_index[:, 0].to(edge_attr_last.device)\n",
    "    dst = edge_index[:, 1].to(edge_attr_last.device)\n",
    "    A[:, src, dst] = w\n",
    "\n",
    "    diag = torch.arange(n_nodes, device=edge_attr_last.device)\n",
    "    A[:, diag, diag] = torch.maximum(A[:, diag, diag], torch.full_like(A[:, diag, diag], float(diag_boost)))\n",
    "\n",
    "    if row_normalize:\n",
    "        A = A / (A.sum(dim=-1, keepdim=True) + eps)\n",
    "\n",
    "    return torch.nan_to_num(A, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "\n",
    "class AdaptiveAdjacency(nn.Module):\n",
    "    def __init__(self, n_nodes: int, cfg: Dict[str, Any]):\n",
    "        super().__init__()\n",
    "        self.n = int(n_nodes)\n",
    "        k = int(cfg.get(\"adj_emb_dim\", 8))\n",
    "        self.E1 = nn.Parameter(0.01 * torch.randn(self.n, k))\n",
    "        self.E2 = nn.Parameter(0.01 * torch.randn(self.n, k))\n",
    "        temp = float(cfg.get(\"adj_temperature\", 1.0))\n",
    "        self.temp = max(temp, 1e-3)\n",
    "        self.topk = int(cfg.get(\"adaptive_topk\", self.n))\n",
    "\n",
    "    def forward(self) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        logits = (self.E1 @ self.E2.t())\n",
    "        logits = F.relu(logits) / self.temp\n",
    "\n",
    "        if 0 < self.topk < self.n:\n",
    "            vals, idx = torch.topk(logits, k=self.topk, dim=-1)\n",
    "            mask = torch.full_like(logits, fill_value=float(\"-inf\"))\n",
    "            mask.scatter_(-1, idx, vals)\n",
    "            logits = mask\n",
    "\n",
    "        A = torch.softmax(logits, dim=-1)\n",
    "        sparsity_proxy = torch.sigmoid(logits)\n",
    "        return A, sparsity_proxy, logits\n",
    "\n",
    "\n",
    "class LearnableSupportMix(nn.Module):\n",
    "    def __init__(self, n_supports: int = 3):\n",
    "        super().__init__()\n",
    "        self.w_logits = nn.Parameter(torch.zeros(n_supports, dtype=torch.float32))\n",
    "\n",
    "    def forward(self) -> torch.Tensor:\n",
    "        return torch.softmax(self.w_logits, dim=0)\n",
    "\n",
    "\n",
    "class CausalConv2dTime(nn.Module):\n",
    "    def __init__(self, in_ch: int, out_ch: int, kernel_size: int, dilation: int):\n",
    "        super().__init__()\n",
    "        self.k = int(kernel_size)\n",
    "        self.d = int(dilation)\n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, kernel_size=(1, self.k), dilation=(1, self.d))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        pad_left = (self.k - 1) * self.d\n",
    "        x = F.pad(x, (pad_left, 0, 0, 0))\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "def graph_message_passing(x: torch.Tensor, A: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.einsum(\"bcnt,bnm->bcmt\", x, A)\n",
    "\n",
    "\n",
    "class GraphWaveNetBlock(nn.Module):\n",
    "    def __init__(self, residual_ch: int, dilation_ch: int, skip_ch: int, kernel_size: int, dilation: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.filter_conv = CausalConv2dTime(residual_ch, dilation_ch, kernel_size=kernel_size, dilation=dilation)\n",
    "        self.gate_conv = CausalConv2dTime(residual_ch, dilation_ch, kernel_size=kernel_size, dilation=dilation)\n",
    "\n",
    "        self.residual_conv = nn.Conv2d(dilation_ch, residual_ch, kernel_size=(1, 1))\n",
    "        self.skip_conv = nn.Conv2d(dilation_ch, skip_ch, kernel_size=(1, 1))\n",
    "\n",
    "        self.dropout = nn.Dropout(float(dropout))\n",
    "        self.bn = nn.BatchNorm2d(residual_ch)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, A: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        x = torch.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        A = torch.nan_to_num(A, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "        residual = x\n",
    "\n",
    "        f = torch.tanh(self.filter_conv(x))\n",
    "        g = torch.sigmoid(self.gate_conv(x))\n",
    "        z = f * g\n",
    "        z = self.dropout(z)\n",
    "\n",
    "        skip = self.skip_conv(z)\n",
    "        out = self.residual_conv(z)\n",
    "        out = graph_message_passing(out, A)\n",
    "        out = out + residual\n",
    "        out = self.bn(out)\n",
    "\n",
    "        out = torch.nan_to_num(out, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        skip = torch.nan_to_num(skip, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        return out, skip\n",
    "\n",
    "\n",
    "class GraphWaveNetTwoHeadFixedH(nn.Module):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "      x_seq: (B,L,N,F)\n",
    "      e_seq: (B,L,E,D) used only for building A_prior from last step\n",
    "\n",
    "    Output (target node):\n",
    "      trade_logit: (B,) trade probability logit\n",
    "      dir_logit:   (B,) direction logit (prob LONG given trade)\n",
    "      fixed_hat:   (B,) predicted fixed-horizon future return\n",
    "    \"\"\"\n",
    "    def __init__(self, node_in: int, edge_dim: int, cfg: Dict[str, Any], n_nodes: int, target_node: int):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.n_nodes = int(n_nodes)\n",
    "        self.target_node = int(target_node)\n",
    "\n",
    "        residual_ch = int(cfg[\"gwn_residual_channels\"])\n",
    "        dilation_ch = int(cfg[\"gwn_dilation_channels\"])\n",
    "        skip_ch = int(cfg[\"gwn_skip_channels\"])\n",
    "        end_ch = int(cfg[\"gwn_end_channels\"])\n",
    "        k = int(cfg[\"gwn_kernel_size\"])\n",
    "        blocks = int(cfg[\"gwn_blocks\"])\n",
    "        layers_per_block = int(cfg[\"gwn_layers_per_block\"])\n",
    "        drop = float(cfg.get(\"dropout\", 0.0))\n",
    "\n",
    "        self.in_proj = nn.Linear(int(node_in), residual_ch)\n",
    "\n",
    "        A_static = build_static_adjacency_from_edges(EDGE_INDEX, n_nodes=self.n_nodes)\n",
    "        self.register_buffer(\"A_static\", A_static)\n",
    "\n",
    "        self.adapt = AdaptiveAdjacency(n_nodes=self.n_nodes, cfg=cfg)\n",
    "        self.support_mix = LearnableSupportMix(n_supports=3)\n",
    "\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for _b in range(blocks):\n",
    "            for l in range(layers_per_block):\n",
    "                dilation = 2 ** l\n",
    "                self.blocks.append(GraphWaveNetBlock(\n",
    "                    residual_ch=residual_ch,\n",
    "                    dilation_ch=dilation_ch,\n",
    "                    skip_ch=skip_ch,\n",
    "                    kernel_size=k,\n",
    "                    dilation=dilation,\n",
    "                    dropout=drop,\n",
    "                ))\n",
    "\n",
    "        self.end1 = nn.Conv2d(skip_ch, end_ch, kernel_size=(1, 1))\n",
    "        self.trade_head = nn.Linear(end_ch, 1)\n",
    "        self.dir_head = nn.Linear(end_ch, 1)\n",
    "        self.fixed_head = nn.Linear(end_ch, 1)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def _compute_supports(self, e_seq: torch.Tensor) -> Tuple[torch.Tensor, Dict[str, Any]]:\n",
    "        B, L_, E, D = e_seq.shape\n",
    "        e_last = e_seq[:, -1, :, :]\n",
    "\n",
    "        A_prior = build_adj_prior_from_edge_attr(\n",
    "            edge_attr_last=e_last,\n",
    "            edge_index=EDGE_INDEX.to(e_seq.device),\n",
    "            n_nodes=self.n_nodes,\n",
    "            use_abs=bool(self.cfg.get(\"prior_use_abs\", False)),\n",
    "            diag_boost=float(self.cfg.get(\"prior_diag_boost\", 1.0)),\n",
    "            row_normalize=bool(self.cfg.get(\"prior_row_normalize\", True)),\n",
    "        )\n",
    "\n",
    "        A_adapt_base, sparsity_proxy, _ = self.adapt()\n",
    "        A_adapt = A_adapt_base.unsqueeze(0).expand(B, -1, -1)\n",
    "\n",
    "        w = self.support_mix()\n",
    "        A_static = self.A_static.to(e_seq.device).to(e_seq.dtype).unsqueeze(0).expand(B, -1, -1)\n",
    "\n",
    "        A_mix = w[0] * A_static + w[1] * A_prior + w[2] * A_adapt\n",
    "        A_mix = A_mix / (A_mix.sum(dim=-1, keepdim=True) + 1e-8)\n",
    "\n",
    "        N = self.n_nodes\n",
    "        offdiag = (1.0 - torch.eye(N, device=e_seq.device, dtype=e_seq.dtype))\n",
    "        l1_off = (sparsity_proxy.to(e_seq.dtype) * offdiag).abs().mean()\n",
    "        mse_prior = ((A_adapt - A_prior) ** 2 * offdiag).mean()\n",
    "\n",
    "        aux = {\n",
    "            \"support_w\": w.detach().cpu().numpy().tolist(),\n",
    "            \"_l1_off_t\": l1_off,\n",
    "            \"_mse_prior_t\": mse_prior,\n",
    "        }\n",
    "        return A_mix, aux\n",
    "\n",
    "    def forward(self, x_seq: torch.Tensor, e_seq: torch.Tensor, return_aux: bool = False):\n",
    "        x_seq = torch.nan_to_num(x_seq, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        e_seq = torch.nan_to_num(e_seq, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "        B, L_, N, Fdim = x_seq.shape\n",
    "        assert N == self.n_nodes\n",
    "\n",
    "        x = self.in_proj(x_seq)                 # (B,L,N,C)\n",
    "        x = x.permute(0, 3, 2, 1).contiguous()  # (B,C,N,T)\n",
    "\n",
    "        A_mix, aux = self._compute_supports(e_seq)\n",
    "\n",
    "        skip_sum = None\n",
    "        for blk in self.blocks:\n",
    "            x, skip = blk(x, A_mix)\n",
    "            skip_sum = skip if skip_sum is None else (skip_sum + skip)\n",
    "\n",
    "        y = F.relu(skip_sum)\n",
    "        y_end = F.relu(self.end1(y))            # (B,end_ch,N,T)\n",
    "        feat = y_end[:, :, self.target_node, -1]  # (B,end_ch)\n",
    "\n",
    "        trade_logit = self.trade_head(feat).squeeze(-1)\n",
    "        dir_logit = self.dir_head(feat).squeeze(-1)\n",
    "        fixed_hat = self.fixed_head(feat).squeeze(-1)\n",
    "\n",
    "        trade_logit = torch.nan_to_num(trade_logit, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        dir_logit = torch.nan_to_num(dir_logit, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        fixed_hat = torch.nan_to_num(fixed_hat, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "        if return_aux:\n",
    "            return trade_logit, dir_logit, fixed_hat, aux\n",
    "        return trade_logit, dir_logit, fixed_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# Step 8: Metrics, thresholding, losses (two-head + fixed-horizon utility)\n",
    "# ======================================================================\n",
    "\n",
    "def _safe_auc_binary(y_true: np.ndarray, score: np.ndarray) -> float:\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    score = np.asarray(score, dtype=np.float64)\n",
    "    if y_true.size == 0 or len(np.unique(y_true)) < 2:\n",
    "        return float(\"nan\")\n",
    "    return float(roc_auc_score(y_true, score))\n",
    "\n",
    "\n",
    "def probs3_from_twohead(p_trade: np.ndarray, p_dir: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert two-head outputs to a 3-class distribution:\n",
    "      p_flat = 1 - p_trade\n",
    "      p_long = p_trade * p_dir\n",
    "      p_short = p_trade * (1 - p_dir)\n",
    "    \"\"\"\n",
    "    p_trade = np.asarray(p_trade, dtype=np.float64)\n",
    "    p_dir = np.asarray(p_dir, dtype=np.float64)\n",
    "    p_flat = 1.0 - p_trade\n",
    "    p_long = p_trade * p_dir\n",
    "    p_short = p_trade * (1.0 - p_dir)\n",
    "    prob3 = np.stack([p_short, p_flat, p_long], axis=1)\n",
    "    prob3 = np.clip(prob3, 1e-12, 1.0)\n",
    "    prob3 = prob3 / prob3.sum(axis=1, keepdims=True)\n",
    "    return prob3\n",
    "\n",
    "\n",
    "def compute_trade_dir_auc_from_twohead(y_trade_true: np.ndarray, y_tb_true: np.ndarray, p_trade: np.ndarray, p_dir: np.ndarray) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    trade_auc: y_trade_true vs p_trade\n",
    "    dir_auc: LONG vs SHORT only on true trades; score = p_dir\n",
    "    \"\"\"\n",
    "    y_trade_true = np.asarray(y_trade_true, dtype=np.int64)\n",
    "    y_tb_true = np.asarray(y_tb_true, dtype=np.int64)\n",
    "    p_trade = np.asarray(p_trade, dtype=np.float64)\n",
    "    p_dir = np.asarray(p_dir, dtype=np.float64)\n",
    "\n",
    "    trade_auc = _safe_auc_binary(y_trade_true, p_trade)\n",
    "\n",
    "    mask_trade = (y_tb_true != 1)\n",
    "    y_dir_bin = (y_tb_true[mask_trade] == 2).astype(np.int64)\n",
    "    dir_auc = _safe_auc_binary(y_dir_bin, p_dir[mask_trade])\n",
    "\n",
    "    return trade_auc, dir_auc\n",
    "\n",
    "\n",
    "def pnl_from_probs_3class(prob3: np.ndarray, exit_ret_arr: np.ndarray, thr_trade: float, thr_dir: float, cost_bps: float) -> Dict[str, Any]:\n",
    "    prob3 = np.asarray(prob3, dtype=np.float64)\n",
    "    exit_ret_arr = np.asarray(exit_ret_arr, dtype=np.float64)\n",
    "\n",
    "    p_short = prob3[:, 0]\n",
    "    p_flat = prob3[:, 1]\n",
    "    p_long = prob3[:, 2]\n",
    "\n",
    "    trade_conf = 1.0 - p_flat\n",
    "    dir_prob = p_long / (p_long + p_short + 1e-12)\n",
    "    dir_conf = np.maximum(dir_prob, 1.0 - dir_prob)\n",
    "\n",
    "    mask = (trade_conf >= float(thr_trade)) & (dir_conf >= float(thr_dir))\n",
    "\n",
    "    action = np.zeros_like(exit_ret_arr, dtype=np.float64)\n",
    "    action[mask] = np.where(dir_prob[mask] >= 0.5, 1.0, -1.0)\n",
    "\n",
    "    cost = (float(cost_bps) * 1e-4) * mask.astype(np.float64)\n",
    "    pnl = action * exit_ret_arr - cost\n",
    "\n",
    "    n = int(len(exit_ret_arr))\n",
    "    n_tr = int(mask.sum())\n",
    "\n",
    "    return {\n",
    "        \"n\": n,\n",
    "        \"n_trades\": n_tr,\n",
    "        \"trade_rate\": float(n_tr / max(1, n)),\n",
    "        \"pnl_sum\": float(pnl.sum()),\n",
    "        \"pnl_mean\": float(pnl.mean()) if n else float(\"nan\"),\n",
    "        \"pnl_per_trade\": float(pnl.sum() / max(1, n_tr)),\n",
    "    }\n",
    "\n",
    "\n",
    "def build_trade_threshold_grid(p_trade: np.ndarray, base_grid: Optional[List[float]], target_trades_list: Optional[List[int]]) -> List[float]:\n",
    "    p_trade = np.asarray(p_trade, dtype=np.float64)\n",
    "    p_trade = p_trade[np.isfinite(p_trade)]\n",
    "    if p_trade.size == 0:\n",
    "        return base_grid or [0.5]\n",
    "\n",
    "    thrs = set(float(t) for t in (base_grid or []))\n",
    "\n",
    "    if target_trades_list:\n",
    "        N = int(p_trade.size)\n",
    "        for k in target_trades_list:\n",
    "            k = int(k)\n",
    "            if k <= 0:\n",
    "                continue\n",
    "            if k >= N:\n",
    "                thr = float(np.min(p_trade))\n",
    "            else:\n",
    "                q = 1.0 - (k / N)\n",
    "                thr = float(np.quantile(p_trade, q))\n",
    "            thrs.add(float(np.clip(thr, 0.01, 0.99)))\n",
    "\n",
    "    out = sorted(thrs)\n",
    "    cleaned = []\n",
    "    for t in out:\n",
    "        if not cleaned or abs(t - cleaned[-1]) > 1e-6:\n",
    "            cleaned.append(float(t))\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def sweep_thresholds_3class(prob3: np.ndarray, exit_ret_arr: np.ndarray, cfg: Dict[str, Any], min_trades: int, target_trade_rate: Optional[float]) -> pd.DataFrame:\n",
    "    prob3 = np.asarray(prob3, dtype=np.float64)\n",
    "    p_trade = 1.0 - prob3[:, 1]\n",
    "\n",
    "    thr_trade_grid = build_trade_threshold_grid(\n",
    "        p_trade=p_trade,\n",
    "        base_grid=cfg.get(\"thr_trade_grid\", [0.5]),\n",
    "        target_trades_list=cfg.get(\"proxy_target_trades\", None),\n",
    "    )\n",
    "    thr_dir_grid = cfg.get(\"thr_dir_grid\", [0.5])\n",
    "\n",
    "    obj = str(cfg.get(\"thr_objective\", \"pnl_sum\"))\n",
    "    max_rate = cfg.get(\"max_trade_rate_val\", None)\n",
    "    penalty = float(cfg.get(\"trade_rate_penalty\", 0.0))\n",
    "\n",
    "    rows = []\n",
    "    for thr_t in thr_trade_grid:\n",
    "        for thr_d in thr_dir_grid:\n",
    "            m = pnl_from_probs_3class(prob3, exit_ret_arr, thr_t, thr_d, cfg[\"cost_bps\"])\n",
    "            if int(m[\"n_trades\"]) < int(min_trades):\n",
    "                continue\n",
    "            if max_rate is not None and float(m[\"trade_rate\"]) > float(max_rate):\n",
    "                continue\n",
    "\n",
    "            base = float(m.get(obj, np.nan))\n",
    "            if not np.isfinite(base):\n",
    "                continue\n",
    "\n",
    "            if target_trade_rate is not None:\n",
    "                score = base - penalty * abs(float(m[\"trade_rate\"]) - float(target_trade_rate))\n",
    "            else:\n",
    "                score = base - penalty * float(m[\"trade_rate\"])\n",
    "\n",
    "            rows.append({\"thr_trade\": float(thr_t), \"thr_dir\": float(thr_d), \"score\": float(score), **m})\n",
    "\n",
    "    if not rows:\n",
    "        return sweep_thresholds_3class(prob3, exit_ret_arr, cfg, min_trades=1, target_trade_rate=target_trade_rate)\n",
    "\n",
    "    return pd.DataFrame(rows).sort_values([\"score\", \"pnl_sum\"], ascending=False)\n",
    "\n",
    "\n",
    "def total_loss_with_adj_reg(loss: torch.Tensor, aux: Dict[str, Any], cfg: Dict[str, Any]) -> torch.Tensor:\n",
    "    lam_l1 = float(cfg.get(\"adj_l1_lambda\", 0.0))\n",
    "    lam_pr = float(cfg.get(\"adj_prior_lambda\", 0.0))\n",
    "    reg = 0.0\n",
    "    if lam_l1 > 0:\n",
    "        reg = reg + lam_l1 * aux[\"_l1_off_t\"]\n",
    "    if lam_pr > 0:\n",
    "        reg = reg + lam_pr * aux[\"_mse_prior_t\"]\n",
    "    return loss + reg\n",
    "\n",
    "\n",
    "def compute_pos_weights_binary(y: np.ndarray) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    pos_weight for BCEWithLogitsLoss:\n",
    "      pos_weight = n_neg / n_pos\n",
    "    \"\"\"\n",
    "    y = np.asarray(y, dtype=np.int64)\n",
    "    n_pos = float((y == 1).sum())\n",
    "    n_neg = float((y == 0).sum())\n",
    "    n_pos = max(n_pos, 1.0)\n",
    "    return torch.tensor([n_neg / n_pos], dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "def multitask_loss_twohead_fixedH(\n",
    "    trade_logit: torch.Tensor,      # (B,)\n",
    "    dir_logit: torch.Tensor,        # (B,)\n",
    "    fixed_hat: torch.Tensor,        # (B,)\n",
    "    y_trade: torch.Tensor,          # (B,) {0,1}\n",
    "    y_dir: torch.Tensor,            # (B,) {0,1} meaningful only if y_trade==1\n",
    "    fixed_ret: torch.Tensor,        # (B,)\n",
    "    bce_trade_fn: nn.Module,\n",
    "    bce_dir_fn: nn.Module,\n",
    "    cfg: Dict[str, Any],\n",
    ") -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n",
    "\n",
    "    wT = float(cfg[\"loss_w_trade\"])\n",
    "    wD = float(cfg[\"loss_w_dir\"])\n",
    "    wR = float(cfg[\"loss_w_ret\"])\n",
    "    wU = float(cfg[\"loss_w_utility\"])\n",
    "\n",
    "    # ---- Trade BCE (all samples)\n",
    "    bceT = bce_trade_fn(trade_logit, y_trade.float())\n",
    "\n",
    "    # ---- Dir BCE (ONLY on true trades)\n",
    "    maskT = (y_trade == 1)\n",
    "    if maskT.any():\n",
    "        bceD = bce_dir_fn(dir_logit[maskT], y_dir[maskT].float())\n",
    "    else:\n",
    "        bceD = trade_logit.new_tensor(0.0)\n",
    "\n",
    "    # ---- Regression Huber (all samples, clipped)\n",
    "    clip_val = float(cfg.get(\"fixed_ret_clip\", 0.0))\n",
    "    fr = torch.clamp(fixed_ret, -clip_val, clip_val) if clip_val and clip_val > 0 else fixed_ret\n",
    "    delta = float(cfg.get(\"ret_huber_delta\", 0.01))\n",
    "    huber = F.huber_loss(fixed_hat, fr, delta=delta)\n",
    "\n",
    "    # ---- UTILITY (masked: ONLY on true trades)\n",
    "    p_trade = torch.sigmoid(trade_logit)         # (B,)\n",
    "    p_dir = torch.sigmoid(dir_logit)             # (B,)\n",
    "    signed_dir = 2.0 * p_dir - 1.0               # [-1,1]\n",
    "    k = float(cfg.get(\"utility_k\", 2.0))\n",
    "    pos = p_trade * torch.tanh(k * signed_dir)   # [-1,1], soft position\n",
    "\n",
    "    fee = float(cfg.get(\"cost_bps\", 0.0)) * 1e-4\n",
    "    utility_vec = pos * fr - fee * torch.abs(pos)\n",
    "\n",
    "    if maskT.any():\n",
    "        util = utility_vec[maskT].mean()\n",
    "        pos_abs = torch.abs(pos[maskT]).mean()\n",
    "        pT_mean = p_trade[maskT].mean()\n",
    "    else:\n",
    "        util = trade_logit.new_tensor(0.0)\n",
    "        pos_abs = trade_logit.new_tensor(0.0)\n",
    "        pT_mean = trade_logit.new_tensor(0.0)\n",
    "\n",
    "    util_scale = float(cfg.get(\"utility_scale\", 1.0))\n",
    "    utilS = util_scale * util\n",
    "    util_loss = -utilS  # maximize utilS\n",
    "\n",
    "    total = wT * bceT + wD * bceD + wR * huber + wU * util_loss\n",
    "\n",
    "    parts = {\n",
    "        # names used across notebook\n",
    "        \"bce_trade\": bceT.detach(),\n",
    "        \"bce_dir\": bceD.detach(),\n",
    "        \"huber\": huber.detach(),\n",
    "        \"util\": util.detach(),\n",
    "        \"util_scaled\": utilS.detach(),\n",
    "        \"pos_abs\": pos_abs.detach(),\n",
    "        \"p_trade_mean\": pT_mean.detach(),\n",
    "\n",
    "        # optional backward-compatible aliases (can remove later)\n",
    "        \"bceT\": bceT.detach(),\n",
    "        \"bceD\": bceD.detach(),\n",
    "        \"utilS\": utilS.detach(),\n",
    "        \"posAbs\": pos_abs.detach(),\n",
    "        \"pT\": pT_mean.detach(),\n",
    "    }\n",
    "\n",
    "    return total, parts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# Step 9: Evaluation (two-head -> prob3 for thresholds, log AUCs + utility)\n",
    "# ======================================================================\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_twohead_on_indices(\n",
    "    model: nn.Module,\n",
    "    X_scaled: np.ndarray,\n",
    "    edge_scaled: np.ndarray,\n",
    "    indices: np.ndarray,\n",
    "    bce_trade: nn.Module,\n",
    "    bce_dir: nn.Module,\n",
    "    cfg: Dict[str, Any],\n",
    ") -> Dict[str, Any]:\n",
    "    ds = LobGraphSequenceDatasetTwoHeadFixedH(\n",
    "        X_node=X_scaled,\n",
    "        E_feat=edge_scaled,\n",
    "        y_trade_arr=y_trade,\n",
    "        y_dir_arr=y_dir,\n",
    "        exit_ret_arr=exit_ret,\n",
    "        fixed_ret_arr=fixed_ret,\n",
    "        sample_t_=sample_t,\n",
    "        indices=indices.astype(np.int64),\n",
    "        lookback=int(cfg[\"lookback\"]),\n",
    "    )\n",
    "    loader = DataLoader(ds, batch_size=int(cfg[\"batch_size\"]), shuffle=False, collate_fn=collate_fn_twohead, num_workers=0)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    tot_loss = 0.0\n",
    "    tot_tr = 0.0\n",
    "    tot_dr = 0.0\n",
    "    tot_huber = 0.0\n",
    "    tot_util = 0.0\n",
    "    tot_util_s = 0.0\n",
    "    tot_pos_abs = 0.0\n",
    "    tot_ptr = 0.0\n",
    "    n = 0\n",
    "\n",
    "    p_trade_all = []\n",
    "    p_dir_all = []\n",
    "    y_trade_all = []\n",
    "    y_tb_all = []\n",
    "    exit_all = []\n",
    "\n",
    "    for x, e, yt, yd, er_exit, er_fixed, _sidx in loader:\n",
    "        x = x.to(DEVICE).float()\n",
    "        e = e.to(DEVICE).float()\n",
    "        yt = yt.to(DEVICE).float()\n",
    "        yd = yd.to(DEVICE).float()\n",
    "        er_fixed = er_fixed.to(DEVICE).float()\n",
    "\n",
    "        trade_logit, dir_logit, fixed_hat, aux = model(x, e, return_aux=True)\n",
    "        loss, parts = multitask_loss_twohead_fixedH(trade_logit, dir_logit, fixed_hat, yt, yd, er_fixed, bce_trade, bce_dir, cfg)\n",
    "        loss = total_loss_with_adj_reg(loss, aux, cfg)\n",
    "\n",
    "        B = int(yt.size(0))\n",
    "        tot_loss += float(loss.item()) * B\n",
    "        tot_tr += float(parts[\"bce_trade\"].item()) * B\n",
    "        tot_dr += float(parts[\"bce_dir\"].item()) * B\n",
    "        tot_huber += float(parts[\"huber\"].item()) * B\n",
    "        tot_util += float(parts[\"util\"].item()) * B\n",
    "        tot_util_s += float(parts[\"util_scaled\"].item()) * B\n",
    "        tot_pos_abs += float(parts[\"pos_abs\"].item()) * B\n",
    "        tot_ptr += float(parts[\"p_trade_mean\"].item()) * B\n",
    "        n += B\n",
    "\n",
    "        p_trade = torch.sigmoid(trade_logit).detach().cpu().numpy()\n",
    "        p_dir = torch.sigmoid(dir_logit).detach().cpu().numpy()\n",
    "\n",
    "        p_trade_all.append(p_trade)\n",
    "        p_dir_all.append(p_dir)\n",
    "\n",
    "        y_trade_all.append(yt.detach().cpu().numpy())\n",
    "\n",
    "        # TB label reconstructed from y_trade/y_dir is not possible for flat; keep original via index\n",
    "        # Here we recover TB labels by mapping sidx->t->y_tb outside the loader:\n",
    "        # We use exit_ret tensor order and dataset indices; easiest: compute y_tb in dataset using sample_t.\n",
    "        # To keep eval simple, recompute y_tb from current indices and loader batch positions:\n",
    "        # We'll store TB labels from dataset inside the batch by reconstructing using sidx in the dataset.\n",
    "        # (Here we hack: use er_exit length to slice TB labels by advancing pointer.)\n",
    "        # Instead, we do an exact method by re-evaluating TB labels from the same indices outside loop later.\n",
    "        exit_all.append(er_exit.detach().cpu().numpy())\n",
    "\n",
    "    p_trade_np = np.concatenate(p_trade_all, axis=0) if p_trade_all else np.zeros((0,), dtype=np.float64)\n",
    "    p_dir_np = np.concatenate(p_dir_all, axis=0) if p_dir_all else np.zeros((0,), dtype=np.float64)\n",
    "    y_trade_np = (np.concatenate(y_trade_all, axis=0) > 0.5).astype(np.int64) if y_trade_all else np.zeros((0,), dtype=np.int64)\n",
    "    er_exit_np = np.concatenate(exit_all, axis=0) if exit_all else np.zeros((0,), dtype=np.float64)\n",
    "\n",
    "    # Exact TB labels for these indices:\n",
    "    t_idx = sample_t[indices.astype(np.int64)]\n",
    "    y_tb_np = y_tb[t_idx].astype(np.int64)\n",
    "\n",
    "    trade_auc, dir_auc = compute_trade_dir_auc_from_twohead(y_trade_np, y_tb_np, p_trade_np, p_dir_np)\n",
    "\n",
    "    prob3 = probs3_from_twohead(p_trade_np, p_dir_np)\n",
    "\n",
    "    out = {\n",
    "        \"loss\": float(tot_loss / max(1, n)),\n",
    "        \"loss_trade\": float(tot_tr / max(1, n)),\n",
    "        \"loss_dir\": float(tot_dr / max(1, n)),\n",
    "        \"loss_huber\": float(tot_huber / max(1, n)),\n",
    "        \"soft_util_mean\": float(tot_util / max(1, n)),\n",
    "        \"soft_util_scaled_mean\": float(tot_util_s / max(1, n)),\n",
    "        \"pos_abs_mean\": float(tot_pos_abs / max(1, n)),\n",
    "        \"p_trade_mean\": float(tot_ptr / max(1, n)),\n",
    "        \"trade_auc\": float(trade_auc) if np.isfinite(trade_auc) else float(\"nan\"),\n",
    "        \"dir_auc\": float(dir_auc) if np.isfinite(dir_auc) else float(\"nan\"),\n",
    "        \"p_trade\": p_trade_np,\n",
    "        \"p_dir\": p_dir_np,\n",
    "        \"prob3\": prob3,\n",
    "        \"y_tb\": y_tb_np,\n",
    "        \"exit_ret\": er_exit_np,\n",
    "    }\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# Step 10: Artifact saving/loading (weights .pt + scaler .npz + meta .json)\n",
    "# ======================================================================\n",
    "\n",
    "def _to_jsonable_cfg(cfg: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    out = {}\n",
    "    for k, v in cfg.items():\n",
    "        out[k] = str(v) if isinstance(v, Path) else v\n",
    "    return out\n",
    "\n",
    "\n",
    "def save_scaler_npz(path: Path, params: Dict[str, Any]) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    np.savez_compressed(\n",
    "        str(path),\n",
    "        center_=np.asarray(params[\"center_\"], dtype=np.float32),\n",
    "        scale_=np.asarray(params[\"scale_\"], dtype=np.float32),\n",
    "        max_abs=np.asarray([float(params[\"max_abs\"])], dtype=np.float32),\n",
    "    )\n",
    "\n",
    "\n",
    "def load_scaler_npz(path: Path) -> Dict[str, Any]:\n",
    "    data = np.load(str(path))\n",
    "    return {\n",
    "        \"center_\": data[\"center_\"].astype(np.float32),\n",
    "        \"scale_\": data[\"scale_\"].astype(np.float32),\n",
    "        \"max_abs\": float(data[\"max_abs\"][0]),\n",
    "    }\n",
    "\n",
    "\n",
    "def save_bundle(\n",
    "    bundle_dir: Path,\n",
    "    name: str,\n",
    "    model_state: Dict[str, torch.Tensor],\n",
    "    cfg: Dict[str, Any],\n",
    "    node_scaler_params: Dict[str, Any],\n",
    "    edge_scaler_params: Optional[Dict[str, Any]],\n",
    "    extra_meta: Dict[str, Any],\n",
    ") -> Dict[str, Path]:\n",
    "    bundle_dir.mkdir(parents=True, exist_ok=True)\n",
    "    weights_path = bundle_dir / f\"{name}_weights.pt\"\n",
    "    node_scaler_path = bundle_dir / f\"{name}_node_scaler.npz\"\n",
    "    edge_scaler_path = bundle_dir / f\"{name}_edge_scaler.npz\"\n",
    "    meta_path = bundle_dir / f\"{name}_meta.json\"\n",
    "\n",
    "    torch.save(model_state, str(weights_path))\n",
    "\n",
    "    save_scaler_npz(node_scaler_path, node_scaler_params)\n",
    "    edge_scaler_file = None\n",
    "    if edge_scaler_params is not None:\n",
    "        save_scaler_npz(edge_scaler_path, edge_scaler_params)\n",
    "        edge_scaler_file = edge_scaler_path.name\n",
    "\n",
    "    meta = {\n",
    "        \"name\": name,\n",
    "        \"weights_file\": weights_path.name,\n",
    "        \"node_scaler_file\": node_scaler_path.name,\n",
    "        \"edge_scaler_file\": edge_scaler_file,\n",
    "        \"cfg\": _to_jsonable_cfg(cfg),\n",
    "        **extra_meta,\n",
    "    }\n",
    "    with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(meta, f, indent=2)\n",
    "\n",
    "    return {\n",
    "        \"weights\": weights_path,\n",
    "        \"node_scaler\": node_scaler_path,\n",
    "        \"edge_scaler\": edge_scaler_path if edge_scaler_params is not None else None,\n",
    "        \"meta\": meta_path,\n",
    "    }\n",
    "\n",
    "\n",
    "def load_bundle(bundle_dir: Path, name: str) -> Dict[str, Any]:\n",
    "    meta_path = bundle_dir / f\"{name}_meta.json\"\n",
    "    if not meta_path.exists():\n",
    "        raise FileNotFoundError(str(meta_path))\n",
    "\n",
    "    with open(meta_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        meta = json.load(f)\n",
    "\n",
    "    weights_path = bundle_dir / meta[\"weights_file\"]\n",
    "    node_scaler_path = bundle_dir / meta[\"node_scaler_file\"]\n",
    "    edge_scaler_file = meta.get(\"edge_scaler_file\", None)\n",
    "    edge_scaler_path = (bundle_dir / edge_scaler_file) if edge_scaler_file else None\n",
    "\n",
    "    state = torch.load(str(weights_path), map_location=\"cpu\", weights_only=True)\n",
    "    node_scaler_params = load_scaler_npz(node_scaler_path)\n",
    "    edge_scaler_params = load_scaler_npz(edge_scaler_path) if edge_scaler_path else None\n",
    "\n",
    "    return {\n",
    "        \"meta\": meta,\n",
    "        \"state\": state,\n",
    "        \"node_scaler_params\": node_scaler_params,\n",
    "        \"edge_scaler_params\": edge_scaler_params,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# Step 11: Train one fold (two-head)\n",
    "# Selection metric: sel = soft_util_scaled_mean + b * dir_auc\n",
    "# Still log trade_auc and dir_auc each epoch\n",
    "# ======================================================================\n",
    "\n",
    "def train_one_fold_twohead_fixedH(\n",
    "    fold_id: int,\n",
    "    X_scaled: np.ndarray,\n",
    "    edge_scaled: np.ndarray,\n",
    "    idx_train: np.ndarray,\n",
    "    idx_val: np.ndarray,\n",
    "    idx_test: np.ndarray,\n",
    "    node_scaler_params: Dict[str, Any],\n",
    "    edge_scaler_params: Optional[Dict[str, Any]],\n",
    "    cfg: Dict[str, Any],\n",
    ") -> Dict[str, Any]:\n",
    "    t_train = sample_t[idx_train]\n",
    "    ytr_train = y_trade[t_train].astype(np.int64)\n",
    "    ytb_train = y_tb[t_train].astype(np.int64)\n",
    "\n",
    "    tr_ds = LobGraphSequenceDatasetTwoHeadFixedH(X_scaled, edge_scaled, y_trade, y_dir, exit_ret, fixed_ret, sample_t, idx_train, int(cfg[\"lookback\"]))\n",
    "    va_ds = LobGraphSequenceDatasetTwoHeadFixedH(X_scaled, edge_scaled, y_trade, y_dir, exit_ret, fixed_ret, sample_t, idx_val,   int(cfg[\"lookback\"]))\n",
    "    te_ds = LobGraphSequenceDatasetTwoHeadFixedH(X_scaled, edge_scaled, y_trade, y_dir, exit_ret, fixed_ret, sample_t, idx_test,  int(cfg[\"lookback\"]))\n",
    "\n",
    "    sampler = None\n",
    "    shuffle = True\n",
    "    if bool(cfg.get(\"use_weighted_sampler\", True)):\n",
    "        sampler = make_weighted_sampler_trade(ytr_train)\n",
    "        shuffle = False\n",
    "\n",
    "    tr_loader = DataLoader(tr_ds, batch_size=int(cfg[\"batch_size\"]), shuffle=shuffle, sampler=sampler, collate_fn=collate_fn_twohead, num_workers=0)\n",
    "    va_loader = DataLoader(va_ds, batch_size=int(cfg[\"batch_size\"]), shuffle=False, collate_fn=collate_fn_twohead, num_workers=0)\n",
    "    te_loader = DataLoader(te_ds, batch_size=int(cfg[\"batch_size\"]), shuffle=False, collate_fn=collate_fn_twohead, num_workers=0)\n",
    "\n",
    "    model = GraphWaveNetTwoHeadFixedH(\n",
    "        node_in=int(X_scaled.shape[-1]),\n",
    "        edge_dim=int(edge_scaled.shape[-1]),\n",
    "        cfg=cfg,\n",
    "        n_nodes=len(ASSETS),\n",
    "        target_node=TARGET_NODE,\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    # BCE losses with pos_weight for class imbalance\n",
    "    pos_w_trade = compute_pos_weights_binary(ytr_train)\n",
    "    bce_trade = nn.BCEWithLogitsLoss(pos_weight=pos_w_trade)\n",
    "\n",
    "    # Direction pos_weight computed on trade samples only\n",
    "    mask_tr = (ytb_train != 1)\n",
    "    ydir_train = (ytb_train[mask_tr] == 2).astype(np.int64)\n",
    "    pos_w_dir = compute_pos_weights_binary(ydir_train) if ydir_train.size else torch.tensor([1.0], device=DEVICE)\n",
    "    bce_dir = nn.BCEWithLogitsLoss(pos_weight=pos_w_dir)\n",
    "\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=float(cfg[\"lr\"]), weight_decay=float(cfg[\"weight_decay\"]))\n",
    "\n",
    "    use_onecycle = bool(cfg.get(\"use_onecycle\", True))\n",
    "    if use_onecycle:\n",
    "        sch = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            opt,\n",
    "            max_lr=float(cfg[\"lr\"]),\n",
    "            epochs=int(cfg[\"epochs\"]),\n",
    "            steps_per_epoch=max(1, len(tr_loader)),\n",
    "            pct_start=0.15,\n",
    "            div_factor=10.0,\n",
    "            final_div_factor=50.0,\n",
    "        )\n",
    "    else:\n",
    "        sch = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"max\", factor=0.5, patience=3)\n",
    "\n",
    "    b_dir = float(cfg.get(\"sel_b_dir_auc\", 0.10))\n",
    "    trade_pen = float(cfg.get(\"trade_prob_penalty\", 0.0))\n",
    "\n",
    "    best_sel = -1e18\n",
    "    best_state = None\n",
    "    best_epoch = -1\n",
    "    patience = 7\n",
    "    bad = 0\n",
    "\n",
    "    for ep in range(1, int(cfg[\"epochs\"]) + 1):\n",
    "        model.train()\n",
    "\n",
    "        tot = 0.0\n",
    "        tot_tr = 0.0\n",
    "        tot_dr = 0.0\n",
    "        tot_h = 0.0\n",
    "        tot_u = 0.0\n",
    "        tot_us = 0.0\n",
    "        n = 0\n",
    "\n",
    "        for x, e, yt, yd, _er_exit, er_fixed, _sidx in tr_loader:\n",
    "            x = x.to(DEVICE).float()\n",
    "            e = e.to(DEVICE).float()\n",
    "            yt = yt.to(DEVICE).float()\n",
    "            yd = yd.to(DEVICE).float()\n",
    "            er_fixed = er_fixed.to(DEVICE).float()\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "\n",
    "            trade_logit, dir_logit, fixed_hat, aux = model(x, e, return_aux=True)\n",
    "            loss_mt, parts = multitask_loss_twohead_fixedH(\n",
    "                trade_logit, dir_logit, fixed_hat,\n",
    "                yt, yd, er_fixed,\n",
    "                bce_trade, bce_dir,\n",
    "                cfg\n",
    "            )\n",
    "\n",
    "            if trade_pen > 0:\n",
    "                loss_mt = loss_mt + trade_pen * parts[\"p_trade_mean\"]\n",
    "\n",
    "            loss = total_loss_with_adj_reg(loss_mt, aux, cfg)\n",
    "            if not torch.isfinite(loss):\n",
    "                continue\n",
    "\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), float(cfg[\"grad_clip\"]))\n",
    "            opt.step()\n",
    "            if use_onecycle:\n",
    "                sch.step()\n",
    "\n",
    "            B = int(yt.size(0))\n",
    "            tot += float(loss.item()) * B\n",
    "            tot_tr += float(parts[\"bce_trade\"].item()) * B\n",
    "            tot_dr += float(parts[\"bce_dir\"].item()) * B\n",
    "            tot_h += float(parts[\"huber\"].item()) * B\n",
    "            tot_u += float(parts[\"util\"].item()) * B\n",
    "            tot_us += float(parts[\"util_scaled\"].item()) * B\n",
    "            n += B\n",
    "\n",
    "        tr_loss = tot / max(1, n)\n",
    "        tr_tr = tot_tr / max(1, n)\n",
    "        tr_dr = tot_dr / max(1, n)\n",
    "        tr_h = tot_h / max(1, n)\n",
    "        tr_u = tot_u / max(1, n)\n",
    "        tr_us = tot_us / max(1, n)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        v_tot = 0.0\n",
    "        v_tr = 0.0\n",
    "        v_dr = 0.0\n",
    "        v_h = 0.0\n",
    "        v_u = 0.0\n",
    "        v_us = 0.0\n",
    "        v_pa = 0.0\n",
    "        v_ptr = 0.0\n",
    "        v_n = 0\n",
    "\n",
    "        p_trade_list = []\n",
    "        p_dir_list = []\n",
    "        y_trade_list = []\n",
    "        exit_list = []\n",
    "\n",
    "        for x, e, yt, yd, er_exit, er_fixed, _sidx in va_loader:\n",
    "            x = x.to(DEVICE).float()\n",
    "            e = e.to(DEVICE).float()\n",
    "            yt = yt.to(DEVICE).float()\n",
    "            yd = yd.to(DEVICE).float()\n",
    "            er_fixed = er_fixed.to(DEVICE).float()\n",
    "\n",
    "            trade_logit, dir_logit, fixed_hat, aux = model(x, e, return_aux=True)\n",
    "            loss_mt, parts = multitask_loss_twohead_fixedH(\n",
    "                trade_logit, dir_logit, fixed_hat,\n",
    "                yt, yd, er_fixed,\n",
    "                bce_trade, bce_dir,\n",
    "                cfg\n",
    "            )\n",
    "            loss_val = total_loss_with_adj_reg(loss_mt, aux, cfg)\n",
    "\n",
    "            B = int(yt.size(0))\n",
    "            v_tot += float(loss_val.item()) * B\n",
    "            v_tr += float(parts[\"bce_trade\"].item()) * B\n",
    "            v_dr += float(parts[\"bce_dir\"].item()) * B\n",
    "            v_h += float(parts[\"huber\"].item()) * B\n",
    "            v_u += float(parts[\"util\"].item()) * B\n",
    "            v_us += float(parts[\"util_scaled\"].item()) * B\n",
    "            v_pa += float(parts[\"pos_abs\"].item()) * B\n",
    "            v_ptr += float(parts[\"p_trade_mean\"].item()) * B\n",
    "            v_n += B\n",
    "\n",
    "            p_trade_list.append(torch.sigmoid(trade_logit).detach().cpu().numpy())\n",
    "            p_dir_list.append(torch.sigmoid(dir_logit).detach().cpu().numpy())\n",
    "            y_trade_list.append((yt.detach().cpu().numpy() > 0.5).astype(np.int64))\n",
    "            exit_list.append(er_exit.detach().cpu().numpy())\n",
    "\n",
    "        p_trade_np = np.concatenate(p_trade_list, axis=0) if p_trade_list else np.zeros((0,), dtype=np.float64)\n",
    "        p_dir_np = np.concatenate(p_dir_list, axis=0) if p_dir_list else np.zeros((0,), dtype=np.float64)\n",
    "        y_trade_np = np.concatenate(y_trade_list, axis=0) if y_trade_list else np.zeros((0,), dtype=np.int64)\n",
    "        er_exit_np = np.concatenate(exit_list, axis=0) if exit_list else np.zeros((0,), dtype=np.float64)\n",
    "\n",
    "        t_val = sample_t[idx_val]\n",
    "        y_tb_val = y_tb[t_val].astype(np.int64)\n",
    "        trade_auc, dir_auc = compute_trade_dir_auc_from_twohead(y_trade_np, y_tb_val, p_trade_np, p_dir_np)\n",
    "\n",
    "        val_loss = v_tot / max(1, v_n)\n",
    "        val_tr = v_tr / max(1, v_n)\n",
    "        val_dr = v_dr / max(1, v_n)\n",
    "        val_h = v_h / max(1, v_n)\n",
    "        val_u = v_u / max(1, v_n)\n",
    "        val_us = v_us / max(1, v_n)\n",
    "        val_pa = v_pa / max(1, v_n)\n",
    "        val_ptr = v_ptr / max(1, v_n)\n",
    "\n",
    "        sel = float(val_us) + b_dir * (float(dir_auc) if np.isfinite(dir_auc) else 0.0)\n",
    "\n",
    "        if sel > best_sel:\n",
    "            best_sel = sel\n",
    "            best_epoch = ep\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            bad = 0\n",
    "        else:\n",
    "            bad += 1\n",
    "\n",
    "        if not use_onecycle:\n",
    "            sch.step(sel)\n",
    "\n",
    "        lr_now = opt.param_groups[0][\"lr\"]\n",
    "        w_support = model.support_mix().detach().cpu().numpy().tolist()\n",
    "\n",
    "        print(\n",
    "            f\"[fold {fold_id:02d}] ep {ep:02d} lr={lr_now:.2e} \"\n",
    "            f\"tr_loss={tr_loss:.4f} (bceT={tr_tr:.4f}, bceD={tr_dr:.4f}, huber={tr_h:.4f}, util={tr_u:.5f}, utilS={tr_us:.5f}) \"\n",
    "            f\"val_loss={val_loss:.4f} (bceT={val_tr:.4f}, bceD={val_dr:.4f}, huber={val_h:.4f}, util={val_u:.5f}, utilS={val_us:.5f}, posAbs={val_pa:.3f}, pT={val_ptr:.3f}) \"\n",
    "            f\"val_trade_auc={trade_auc:.3f} val_dir_auc={dir_auc:.3f} sel={sel:.5f} \"\n",
    "            f\"best={best_sel:.5f}@ep{best_epoch:02d} supports={np.round(w_support, 3).tolist()}\"\n",
    "        )\n",
    "\n",
    "        if bad >= patience:\n",
    "            break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    # Eval with best epoch model\n",
    "    val_eval = eval_twohead_on_indices(model, X_scaled, edge_scaled, idx_val, bce_trade, bce_dir, cfg)\n",
    "    test_eval = eval_twohead_on_indices(model, X_scaled, edge_scaled, idx_test, bce_trade, bce_dir, cfg)\n",
    "\n",
    "    true_val_trade_rate = split_trade_ratio(idx_val, sample_t, y_trade)\n",
    "    sweep_val = sweep_thresholds_3class(\n",
    "        prob3=val_eval[\"prob3\"],\n",
    "        exit_ret_arr=val_eval[\"exit_ret\"],\n",
    "        cfg=cfg,\n",
    "        min_trades=int(cfg[\"eval_min_trades\"]),\n",
    "        target_trade_rate=float(true_val_trade_rate),\n",
    "    )\n",
    "    best_thr = sweep_val.iloc[0].to_dict()\n",
    "    thr_trade = float(best_thr[\"thr_trade\"])\n",
    "    thr_dir = float(best_thr[\"thr_dir\"])\n",
    "\n",
    "    pnl_val = pnl_from_probs_3class(val_eval[\"prob3\"], val_eval[\"exit_ret\"], thr_trade, thr_dir, cfg[\"cost_bps\"])\n",
    "    pnl_test = pnl_from_probs_3class(test_eval[\"prob3\"], test_eval[\"exit_ret\"], thr_trade, thr_dir, cfg[\"cost_bps\"])\n",
    "\n",
    "    print(\n",
    "        f\"[fold {fold_id:02d}] chosen thresholds on VAL: thr_trade={thr_trade:.3f} thr_dir={thr_dir:.3f} \"\n",
    "        f\"| val pnl_sum={pnl_val['pnl_sum']:.4f} val trade_rate={pnl_val['trade_rate']:.3f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"[fold {fold_id:02d}] TEST (fixed thresholds from VAL): \"\n",
    "        f\"trade_auc={test_eval['trade_auc']:.3f} dir_auc={test_eval['dir_auc']:.3f} \"\n",
    "        f\"soft_utilS={test_eval['soft_util_scaled_mean']:.5f} pnl_sum={pnl_test['pnl_sum']:.4f} \"\n",
    "        f\"trade_rate={pnl_test['trade_rate']:.3f} trades={pnl_test['n_trades']}\"\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"fold\": int(fold_id),\n",
    "        \"model_state\": {k: v.detach().cpu().clone() for k, v in model.state_dict().items()},\n",
    "        \"node_scaler_params\": node_scaler_params,\n",
    "        \"edge_scaler_params\": edge_scaler_params,\n",
    "        \"idx_train\": idx_train,\n",
    "        \"idx_val\": idx_val,\n",
    "        \"idx_test\": idx_test,\n",
    "        \"best_epoch\": int(best_epoch),\n",
    "        \"best_sel\": float(best_sel),\n",
    "        \"val_eval\": val_eval,\n",
    "        \"test_eval\": test_eval,\n",
    "        \"thr_trade\": thr_trade,\n",
    "        \"thr_dir\": thr_dir,\n",
    "        \"pnl_val\": pnl_val,\n",
    "        \"pnl_test\": pnl_test,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================================================\n",
      "FOLD 1/4 sizes: train=5652 val=1130 test=1130\n",
      "True trade ratio (val):  0.365\n",
      "True trade ratio (test): 0.304\n",
      "[fold 01] ep 01 lr=9.84e-05 tr_loss=0.6717 (bceT=0.9970, bceD=0.7946, huber=0.0048, util=-0.00002, utilS=-0.00991) val_loss=0.5750 (bceT=0.8840, bceD=0.8300, huber=0.0038, util=0.00003, utilS=0.01314, posAbs=0.098, pT=0.489) val_trade_auc=0.505 val_dir_auc=0.579 sel=0.18672 best=0.18672@ep01 supports=[0.333, 0.334, 0.333]\n",
      "[fold 01] ep 02 lr=2.34e-04 tr_loss=0.4497 (bceT=0.9443, bceD=0.7793, huber=0.0042, util=0.00018, utilS=0.09047) val_loss=0.5863 (bceT=0.9029, bceD=0.8019, huber=0.0031, util=0.00001, utilS=0.00578, posAbs=0.228, pT=0.584) val_trade_auc=0.500 val_dir_auc=0.530 sel=0.16483 best=0.18672@ep01 supports=[0.333, 0.334, 0.333]\n",
      "[fold 01] ep 03 lr=3.00e-04 tr_loss=0.2246 (bceT=0.9220, bceD=0.7802, huber=0.0029, util=0.00041, utilS=0.20278) val_loss=0.7800 (bceT=1.0273, bceD=0.8456, huber=0.0018, util=-0.00012, utilS=-0.06184, posAbs=0.361, pT=0.721) val_trade_auc=0.498 val_dir_auc=0.513 sel=0.09202 best=0.18672@ep01 supports=[0.333, 0.334, 0.333]\n",
      "[fold 01] ep 04 lr=2.97e-04 tr_loss=-0.2402 (bceT=1.0010, bceD=0.7566, huber=0.0020, util=0.00090, utilS=0.44788) val_loss=0.8121 (bceT=1.1757, bceD=0.8553, huber=0.0016, util=-0.00010, utilS=-0.05023, posAbs=0.448, pT=0.798) val_trade_auc=0.509 val_dir_auc=0.514 sel=0.10411 best=0.18672@ep01 supports=[0.334, 0.332, 0.334]\n",
      "[fold 01] ep 05 lr=2.90e-04 tr_loss=-0.7494 (bceT=1.1267, bceD=0.7425, huber=0.0019, util=0.00145, utilS=0.72359) val_loss=1.0630 (bceT=1.3681, bceD=0.9533, huber=0.0017, util=-0.00025, utilS=-0.12486, posAbs=0.540, pT=0.855) val_trade_auc=0.497 val_dir_auc=0.495 sel=0.02358 best=0.18672@ep01 supports=[0.334, 0.332, 0.334]\n",
      "[fold 01] ep 06 lr=2.77e-04 tr_loss=-1.0559 (bceT=1.2286, bceD=0.7419, huber=0.0020, util=0.00179, utilS=0.89559) val_loss=1.1656 (bceT=1.4313, bceD=0.9853, huber=0.0017, util=-0.00032, utilS=-0.15952, posAbs=0.577, pT=0.867) val_trade_auc=0.496 val_dir_auc=0.501 sel=-0.00916 best=0.18672@ep01 supports=[0.335, 0.329, 0.335]\n",
      "[fold 01] ep 07 lr=2.61e-04 tr_loss=-1.2365 (bceT=1.2817, bceD=0.7173, huber=0.0021, util=0.00198, utilS=0.99117) val_loss=0.7274 (bceT=1.4972, bceD=0.8872, huber=0.0018, util=0.00011, utilS=0.05399, posAbs=0.529, pT=0.880) val_trade_auc=0.498 val_dir_auc=0.522 sel=0.21072 best=0.21072@ep07 supports=[0.335, 0.329, 0.336]\n",
      "[fold 01] ep 08 lr=2.40e-04 tr_loss=-1.6607 (bceT=1.3269, bceD=0.6700, huber=0.0021, util=0.00241, utilS=1.20317) val_loss=0.9514 (bceT=1.6112, bceD=0.9613, huber=0.0018, util=-0.00005, utilS=-0.02507, posAbs=0.590, pT=0.899) val_trade_auc=0.496 val_dir_auc=0.512 sel=0.12862 best=0.21072@ep07 supports=[0.336, 0.328, 0.337]\n",
      "[fold 01] ep 09 lr=2.16e-04 tr_loss=-1.9732 (bceT=1.4354, bceD=0.6523, huber=0.0021, util=0.00275, utilS=1.37562) val_loss=0.8311 (bceT=1.6735, bceD=0.9499, huber=0.0018, util=0.00009, utilS=0.04399, posAbs=0.620, pT=0.910) val_trade_auc=0.496 val_dir_auc=0.548 sel=0.20828 best=0.21072@ep07 supports=[0.336, 0.326, 0.338]\n",
      "[fold 01] ep 10 lr=1.91e-04 tr_loss=-1.9444 (bceT=1.4647, bceD=0.6492, huber=0.0021, util=0.00273, utilS=1.36591) val_loss=0.8286 (bceT=1.6733, bceD=0.9845, huber=0.0018, util=0.00010, utilS=0.05119, posAbs=0.650, pT=0.911) val_trade_auc=0.500 val_dir_auc=0.554 sel=0.21747 best=0.21747@ep10 supports=[0.337, 0.325, 0.339]\n",
      "[fold 01] ep 11 lr=1.64e-04 tr_loss=-2.3054 (bceT=1.4660, bceD=0.6305, huber=0.0021, util=0.00309, utilS=1.54342) val_loss=0.9130 (bceT=1.7095, bceD=1.0651, huber=0.0018, util=0.00006, utilS=0.02948, posAbs=0.694, pT=0.917) val_trade_auc=0.502 val_dir_auc=0.568 sel=0.19998 best=0.21747@ep10 supports=[0.337, 0.323, 0.339]\n",
      "[fold 01] ep 12 lr=1.36e-04 tr_loss=-2.6234 (bceT=1.4904, bceD=0.5752, huber=0.0021, util=0.00339, utilS=1.69715) val_loss=0.8525 (bceT=1.7621, bceD=1.0367, huber=0.0018, util=0.00013, utilS=0.06395, posAbs=0.690, pT=0.923) val_trade_auc=0.498 val_dir_auc=0.547 sel=0.22808 best=0.22808@ep12 supports=[0.337, 0.323, 0.34]\n",
      "[fold 01] ep 13 lr=1.09e-04 tr_loss=-2.5911 (bceT=1.4945, bceD=0.5751, huber=0.0020, util=0.00336, utilS=1.68179) val_loss=0.8208 (bceT=1.8038, bceD=1.0095, huber=0.0018, util=0.00016, utilS=0.08233, posAbs=0.690, pT=0.928) val_trade_auc=0.496 val_dir_auc=0.560 sel=0.25027 best=0.25027@ep13 supports=[0.337, 0.322, 0.341]\n",
      "[fold 01] ep 14 lr=8.30e-05 tr_loss=-2.7897 (bceT=1.5039, bceD=0.5796, huber=0.0020, util=0.00357, utilS=1.78361) val_loss=0.9944 (bceT=1.8295, bceD=1.0740, huber=0.0018, util=0.00002, utilS=0.01131, posAbs=0.694, pT=0.931) val_trade_auc=0.497 val_dir_auc=0.552 sel=0.17687 best=0.25027@ep13 supports=[0.337, 0.321, 0.341]\n",
      "[fold 01] ep 15 lr=5.96e-05 tr_loss=-2.8464 (bceT=1.5435, bceD=0.5519, huber=0.0020, util=0.00363, utilS=1.81407) val_loss=0.7786 (bceT=1.8359, bceD=1.0144, huber=0.0018, util=0.00022, utilS=0.10994, posAbs=0.678, pT=0.932) val_trade_auc=0.499 val_dir_auc=0.570 sel=0.28095 best=0.28095@ep15 supports=[0.337, 0.321, 0.342]\n",
      "[fold 01] ep 16 lr=3.93e-05 tr_loss=-2.9693 (bceT=1.5514, bceD=0.5235, huber=0.0020, util=0.00374, utilS=1.87198) val_loss=0.9152 (bceT=1.8535, bceD=1.0663, huber=0.0018, util=0.00011, utilS=0.05380, posAbs=0.702, pT=0.934) val_trade_auc=0.500 val_dir_auc=0.570 sel=0.22469 best=0.28095@ep15 supports=[0.337, 0.321, 0.342]\n",
      "[fold 01] ep 17 lr=2.27e-05 tr_loss=-2.9782 (bceT=1.5570, bceD=0.5462, huber=0.0020, util=0.00376, utilS=1.88138) val_loss=0.8850 (bceT=1.8531, bceD=1.0635, huber=0.0018, util=0.00014, utilS=0.06834, posAbs=0.703, pT=0.934) val_trade_auc=0.500 val_dir_auc=0.571 sel=0.23963 best=0.28095@ep15 supports=[0.337, 0.32, 0.342]\n",
      "[fold 01] ep 18 lr=1.05e-05 tr_loss=-3.0233 (bceT=1.5749, bceD=0.5481, huber=0.0020, util=0.00381, utilS=1.90740) val_loss=0.8321 (bceT=1.8468, bceD=1.0502, huber=0.0018, util=0.00018, utilS=0.09135, posAbs=0.697, pT=0.934) val_trade_auc=0.500 val_dir_auc=0.574 sel=0.26361 best=0.28095@ep15 supports=[0.337, 0.32, 0.342]\n",
      "[fold 01] ep 19 lr=3.04e-06 tr_loss=-2.9723 (bceT=1.5442, bceD=0.5336, huber=0.0020, util=0.00375, utilS=1.87398) val_loss=0.8041 (bceT=1.8430, bceD=1.0450, huber=0.0018, util=0.00021, utilS=0.10378, posAbs=0.697, pT=0.933) val_trade_auc=0.501 val_dir_auc=0.579 sel=0.27733 best=0.28095@ep15 supports=[0.337, 0.32, 0.342]\n",
      "[fold 01] ep 20 lr=6.01e-07 tr_loss=-2.9277 (bceT=1.5652, bceD=0.5400, huber=0.0020, util=0.00371, utilS=1.85651) val_loss=0.8431 (bceT=1.8572, bceD=1.0542, huber=0.0018, util=0.00018, utilS=0.08836, posAbs=0.700, pT=0.935) val_trade_auc=0.500 val_dir_auc=0.575 sel=0.26084 best=0.28095@ep15 supports=[0.337, 0.32, 0.342]\n",
      "[fold 01] chosen thresholds on VAL: thr_trade=0.964 thr_dir=0.700 | val pnl_sum=0.0110 val trade_rate=0.100\n",
      "[fold 01] TEST (fixed thresholds from VAL): trade_auc=0.587 dir_auc=0.595 soft_utilS=0.25721 pnl_sum=-0.0056 trade_rate=0.126 trades=142\n",
      "Saved fold bundle: fold_01_meta.json\n",
      "\n",
      "==========================================================================================\n",
      "FOLD 2/4 sizes: train=6782 val=1130 test=1130\n",
      "True trade ratio (val):  0.304\n",
      "True trade ratio (test): 0.463\n",
      "[fold 02] ep 01 lr=9.83e-05 tr_loss=0.5159 (bceT=1.0030, bceD=0.7607, huber=0.0024, util=0.00012, utilS=0.06244) val_loss=0.3218 (bceT=0.8392, bceD=0.7396, huber=0.0018, util=0.00023, utilS=0.11580, posAbs=0.255, pT=0.491) val_trade_auc=0.563 val_dir_auc=0.484 sel=0.26089 best=0.26089@ep01 supports=[0.333, 0.333, 0.333]\n",
      "[fold 02] ep 02 lr=2.34e-04 tr_loss=0.4055 (bceT=0.9228, bceD=0.7556, huber=0.0022, util=0.00021, utilS=0.10524) val_loss=0.2914 (bceT=0.9096, bceD=0.7420, huber=0.0018, util=0.00029, utilS=0.14377, posAbs=0.284, pT=0.621) val_trade_auc=0.586 val_dir_auc=0.520 sel=0.29970 best=0.29970@ep02 supports=[0.333, 0.335, 0.333]\n",
      "[fold 02] ep 03 lr=3.00e-04 tr_loss=0.1420 (bceT=0.9151, bceD=0.7358, huber=0.0021, util=0.00047, utilS=0.23531) val_loss=0.5774 (bceT=1.1030, bceD=0.7696, huber=0.0019, util=0.00008, utilS=0.03942, posAbs=0.339, pT=0.753) val_trade_auc=0.604 val_dir_auc=0.588 sel=0.21583 best=0.29970@ep02 supports=[0.332, 0.336, 0.332]\n",
      "[fold 02] ep 04 lr=2.97e-04 tr_loss=-0.1947 (bceT=0.9872, bceD=0.7103, huber=0.0021, util=0.00083, utilS=0.41443) val_loss=0.7490 (bceT=1.2911, bceD=0.7818, huber=0.0018, util=-0.00002, utilS=-0.01130, posAbs=0.388, pT=0.825) val_trade_auc=0.607 val_dir_auc=0.622 sel=0.17528 best=0.29970@ep02 supports=[0.333, 0.335, 0.333]\n",
      "[fold 02] ep 05 lr=2.90e-04 tr_loss=-0.5652 (bceT=1.1156, bceD=0.6916, huber=0.0021, util=0.00124, utilS=0.62056) val_loss=0.7170 (bceT=1.4967, bceD=0.7919, huber=0.0018, util=0.00008, utilS=0.04243, posAbs=0.423, pT=0.875) val_trade_auc=0.609 val_dir_auc=0.600 sel=0.22243 best=0.29970@ep02 supports=[0.333, 0.334, 0.333]\n",
      "[fold 02] ep 06 lr=2.77e-04 tr_loss=-0.9897 (bceT=1.2254, bceD=0.6681, huber=0.0020, util=0.00170, utilS=0.84891) val_loss=1.0760 (bceT=1.6846, bceD=0.8310, huber=0.0018, util=-0.00019, utilS=-0.09736, posAbs=0.536, pT=0.903) val_trade_auc=0.608 val_dir_auc=0.618 sel=0.08789 best=0.29970@ep02 supports=[0.334, 0.331, 0.334]\n",
      "[fold 02] ep 07 lr=2.61e-04 tr_loss=-1.1562 (bceT=1.3019, bceD=0.6593, huber=0.0019, util=0.00189, utilS=0.94449) val_loss=0.6619 (bceT=1.7382, bceD=0.8021, huber=0.0016, util=0.00023, utilS=0.11401, posAbs=0.516, pT=0.909) val_trade_auc=0.603 val_dir_auc=0.604 sel=0.29507 best=0.29970@ep02 supports=[0.336, 0.328, 0.336]\n",
      "[fold 02] ep 08 lr=2.40e-04 tr_loss=-1.4573 (bceT=1.3290, bceD=0.6351, huber=0.0019, util=0.00219, utilS=1.09568) val_loss=0.4748 (bceT=1.8105, bceD=0.8225, huber=0.0016, util=0.00045, utilS=0.22377, posAbs=0.614, pT=0.919) val_trade_auc=0.606 val_dir_auc=0.610 sel=0.40665 best=0.40665@ep08 supports=[0.337, 0.327, 0.337]\n",
      "[fold 02] ep 09 lr=2.17e-04 tr_loss=-1.8184 (bceT=1.3751, bceD=0.6074, huber=0.0017, util=0.00256, utilS=1.27974) val_loss=1.2365 (bceT=1.8865, bceD=0.8796, huber=0.0015, util=-0.00027, utilS=-0.13381, posAbs=0.589, pT=0.926) val_trade_auc=0.605 val_dir_auc=0.577 sel=0.03916 best=0.40665@ep08 supports=[0.338, 0.324, 0.338]\n",
      "[fold 02] ep 10 lr=1.91e-04 tr_loss=-1.9318 (bceT=1.4456, bceD=0.6098, huber=0.0017, util=0.00270, utilS=1.34934) val_loss=0.9157 (bceT=1.9309, bceD=0.8790, huber=0.0015, util=0.00007, utilS=0.03425, posAbs=0.646, pT=0.929) val_trade_auc=0.601 val_dir_auc=0.593 sel=0.21221 best=0.40665@ep08 supports=[0.339, 0.322, 0.339]\n",
      "[fold 02] ep 11 lr=1.64e-04 tr_loss=-2.1885 (bceT=1.4897, bceD=0.5726, huber=0.0017, util=0.00296, utilS=1.47906) val_loss=0.8711 (bceT=1.9591, bceD=0.8720, huber=0.0014, util=0.00012, utilS=0.06024, posAbs=0.654, pT=0.931) val_trade_auc=0.595 val_dir_auc=0.573 sel=0.23222 best=0.40665@ep08 supports=[0.34, 0.32, 0.34]\n",
      "[fold 02] ep 12 lr=1.36e-04 tr_loss=-2.3844 (bceT=1.5163, bceD=0.5467, huber=0.0016, util=0.00315, utilS=1.57719) val_loss=0.7791 (bceT=2.0035, bceD=0.8937, huber=0.0014, util=0.00024, utilS=0.11780, posAbs=0.685, pT=0.935) val_trade_auc=0.596 val_dir_auc=0.562 sel=0.28634 best=0.40665@ep08 supports=[0.341, 0.319, 0.341]\n",
      "[fold 02] ep 13 lr=1.09e-04 tr_loss=-2.4179 (bceT=1.5300, bceD=0.5620, huber=0.0016, util=0.00320, utilS=1.59904) val_loss=0.8957 (bceT=1.9963, bceD=0.9322, huber=0.0014, util=0.00013, utilS=0.06501, posAbs=0.719, pT=0.934) val_trade_auc=0.594 val_dir_auc=0.558 sel=0.23249 best=0.40665@ep08 supports=[0.341, 0.318, 0.341]\n",
      "[fold 02] ep 14 lr=8.31e-05 tr_loss=-2.6169 (bceT=1.5196, bceD=0.5317, huber=0.0016, util=0.00338, utilS=1.69142) val_loss=0.9668 (bceT=2.0411, bceD=0.9475, huber=0.0014, util=0.00008, utilS=0.03996, posAbs=0.723, pT=0.939) val_trade_auc=0.593 val_dir_auc=0.564 sel=0.20904 best=0.40665@ep08 supports=[0.341, 0.317, 0.341]\n",
      "[fold 02] ep 15 lr=5.97e-05 tr_loss=-2.7821 (bceT=1.5538, bceD=0.5136, huber=0.0015, util=0.00355, utilS=1.77693) val_loss=1.0046 (bceT=2.0664, bceD=0.9692, huber=0.0013, util=0.00006, utilS=0.02928, posAbs=0.726, pT=0.941) val_trade_auc=0.593 val_dir_auc=0.554 sel=0.19534 best=0.40665@ep08 supports=[0.342, 0.317, 0.342]\n",
      "[fold 02] chosen thresholds on VAL: thr_trade=0.965 thr_dir=0.550 | val pnl_sum=0.1035 val trade_rate=0.082\n",
      "[fold 02] TEST (fixed thresholds from VAL): trade_auc=0.549 dir_auc=0.499 soft_utilS=0.72012 pnl_sum=0.3617 trade_rate=0.309 trades=349\n",
      "Saved fold bundle: fold_02_meta.json\n",
      "\n",
      "==========================================================================================\n",
      "FOLD 3/4 sizes: train=7912 val=1130 test=1130\n",
      "True trade ratio (val):  0.463\n",
      "True trade ratio (test): 0.558\n",
      "[fold 03] ep 01 lr=9.82e-05 tr_loss=0.6188 (bceT=0.9847, bceD=0.7653, huber=0.0024, util=0.00002, utilS=0.01008) val_loss=0.6282 (bceT=0.9584, bceD=0.8045, huber=0.0022, util=-0.00001, utilS=-0.00515, posAbs=0.133, pT=0.515) val_trade_auc=0.490 val_dir_auc=0.520 sel=0.15081 best=0.15081@ep01 supports=[0.333, 0.334, 0.333]\n",
      "[fold 03] ep 02 lr=2.34e-04 tr_loss=0.4062 (bceT=0.9464, bceD=0.7542, huber=0.0019, util=0.00022, utilS=0.10920) val_loss=0.1461 (bceT=0.9304, bceD=0.7856, huber=0.0017, util=0.00046, utilS=0.22768, posAbs=0.275, pT=0.634) val_trade_auc=0.508 val_dir_auc=0.522 sel=0.38434 best=0.38434@ep02 supports=[0.333, 0.334, 0.333]\n",
      "[fold 03] ep 03 lr=3.00e-04 tr_loss=0.0108 (bceT=0.9674, bceD=0.7481, huber=0.0018, util=0.00063, utilS=0.31357) val_loss=-0.3690 (bceT=1.1321, bceD=0.8162, huber=0.0018, util=0.00105, utilS=0.52589, posAbs=0.530, pT=0.825) val_trade_auc=0.525 val_dir_auc=0.526 sel=0.68366 best=0.68366@ep03 supports=[0.333, 0.334, 0.333]\n",
      "[fold 03] ep 04 lr=2.97e-04 tr_loss=-0.3971 (bceT=1.0717, bceD=0.7517, huber=0.0018, util=0.00108, utilS=0.53911) val_loss=-0.1781 (bceT=1.3428, bceD=0.9102, huber=0.0017, util=0.00097, utilS=0.48374, posAbs=0.624, pT=0.887) val_trade_auc=0.534 val_dir_auc=0.525 sel=0.64120 best=0.68366@ep03 supports=[0.332, 0.337, 0.332]\n",
      "[fold 03] ep 05 lr=2.90e-04 tr_loss=-0.6050 (bceT=1.1893, bceD=0.7533, huber=0.0017, util=0.00133, utilS=0.66473) val_loss=0.1514 (bceT=1.4319, bceD=0.9392, huber=0.0016, util=0.00068, utilS=0.33961, posAbs=0.624, pT=0.901) val_trade_auc=0.526 val_dir_auc=0.529 sel=0.49845 best=0.68366@ep03 supports=[0.332, 0.336, 0.332]\n",
      "[fold 03] ep 06 lr=2.77e-04 tr_loss=-1.1091 (bceT=1.2434, bceD=0.7200, huber=0.0017, util=0.00184, utilS=0.92114) val_loss=0.4497 (bceT=1.6101, bceD=1.0427, huber=0.0016, util=0.00048, utilS=0.23979, posAbs=0.694, pT=0.930) val_trade_auc=0.538 val_dir_auc=0.542 sel=0.40252 best=0.68366@ep03 supports=[0.332, 0.336, 0.333]\n",
      "[fold 03] ep 07 lr=2.61e-04 tr_loss=-1.2681 (bceT=1.3528, bceD=0.7170, huber=0.0016, util=0.00204, utilS=1.01960) val_loss=-0.0542 (bceT=1.6466, bceD=1.0184, huber=0.0015, util=0.00099, utilS=0.49387, posAbs=0.715, pT=0.934) val_trade_auc=0.536 val_dir_auc=0.534 sel=0.65395 best=0.68366@ep03 supports=[0.332, 0.335, 0.333]\n",
      "[fold 03] ep 08 lr=2.40e-04 tr_loss=-1.7185 (bceT=1.3822, bceD=0.6641, huber=0.0016, util=0.00248, utilS=1.24091) val_loss=-0.2879 (bceT=1.7287, bceD=1.0139, huber=0.0015, util=0.00125, utilS=0.62429, posAbs=0.718, pT=0.944) val_trade_auc=0.540 val_dir_auc=0.543 sel=0.78708 best=0.78708@ep08 supports=[0.331, 0.334, 0.334]\n",
      "[fold 03] ep 09 lr=2.17e-04 tr_loss=-1.9556 (bceT=1.4595, bceD=0.6585, huber=0.0015, util=0.00274, utilS=1.37224) val_loss=-0.2373 (bceT=1.7411, bceD=1.0157, huber=0.0015, util=0.00120, utilS=0.60146, posAbs=0.725, pT=0.946) val_trade_auc=0.540 val_dir_auc=0.541 sel=0.76362 best=0.78708@ep08 supports=[0.331, 0.335, 0.335]\n",
      "[fold 03] ep 10 lr=1.91e-04 tr_loss=-1.9908 (bceT=1.4756, bceD=0.6622, huber=0.0015, util=0.00279, utilS=1.39330) val_loss=0.3625 (bceT=1.7557, bceD=1.2266, huber=0.0014, util=0.00068, utilS=0.34101, posAbs=0.787, pT=0.948) val_trade_auc=0.536 val_dir_auc=0.529 sel=0.49966 best=0.78708@ep08 supports=[0.331, 0.333, 0.336]\n",
      "[fold 03] ep 11 lr=1.64e-04 tr_loss=-2.2182 (bceT=1.4860, bceD=0.6159, huber=0.0015, util=0.00300, utilS=1.50081) val_loss=0.3169 (bceT=1.7791, bceD=1.1710, huber=0.0014, util=0.00072, utilS=0.35819, posAbs=0.755, pT=0.950) val_trade_auc=0.542 val_dir_auc=0.532 sel=0.51776 best=0.78708@ep08 supports=[0.33, 0.332, 0.337]\n",
      "[fold 03] ep 12 lr=1.36e-04 tr_loss=-2.4861 (bceT=1.5119, bceD=0.5853, huber=0.0014, util=0.00327, utilS=1.63403) val_loss=0.4434 (bceT=1.7926, bceD=1.2254, huber=0.0013, util=0.00061, utilS=0.30682, posAbs=0.754, pT=0.951) val_trade_auc=0.541 val_dir_auc=0.530 sel=0.46568 best=0.78708@ep08 supports=[0.33, 0.332, 0.338]\n",
      "[fold 03] ep 13 lr=1.09e-04 tr_loss=-2.5477 (bceT=1.5105, bceD=0.5888, huber=0.0014, util=0.00333, utilS=1.66524) val_loss=0.5955 (bceT=1.8066, bceD=1.2546, huber=0.0013, util=0.00048, utilS=0.23830, posAbs=0.746, pT=0.953) val_trade_auc=0.541 val_dir_auc=0.533 sel=0.39809 best=0.78708@ep08 supports=[0.33, 0.331, 0.339]\n",
      "[fold 03] ep 14 lr=8.32e-05 tr_loss=-2.7784 (bceT=1.5389, bceD=0.5569, huber=0.0014, util=0.00356, utilS=1.78007) val_loss=0.3391 (bceT=1.8543, bceD=1.2415, huber=0.0013, util=0.00075, utilS=0.37255, posAbs=0.777, pT=0.957) val_trade_auc=0.542 val_dir_auc=0.531 sel=0.53192 best=0.78708@ep08 supports=[0.329, 0.331, 0.34]\n",
      "[fold 03] ep 15 lr=5.97e-05 tr_loss=-2.7537 (bceT=1.5584, bceD=0.5552, huber=0.0014, util=0.00354, utilS=1.77083) val_loss=0.7594 (bceT=1.8447, bceD=1.3098, huber=0.0013, util=0.00035, utilS=0.17269, posAbs=0.768, pT=0.956) val_trade_auc=0.543 val_dir_auc=0.529 sel=0.33142 best=0.78708@ep08 supports=[0.329, 0.331, 0.34]\n",
      "[fold 03] chosen thresholds on VAL: thr_trade=0.500 thr_dir=0.700 | val pnl_sum=0.3105 val trade_rate=0.598\n",
      "[fold 03] TEST (fixed thresholds from VAL): trade_auc=0.660 dir_auc=0.490 soft_utilS=-0.21153 pnl_sum=0.0410 trade_rate=0.612 trades=692\n",
      "Saved fold bundle: fold_03_meta.json\n",
      "\n",
      "==========================================================================================\n",
      "FOLD 4/4 sizes: train=9042 val=1130 test=1130\n",
      "True trade ratio (val):  0.558\n",
      "True trade ratio (test): 0.506\n",
      "[fold 04] ep 01 lr=9.81e-05 tr_loss=0.5287 (bceT=0.9061, bceD=0.7343, huber=0.0064, util=0.00008, utilS=0.03844) val_loss=0.3964 (bceT=0.9031, bceD=0.7406, huber=0.0057, util=0.00018, utilS=0.09036, posAbs=0.253, pT=0.595) val_trade_auc=0.493 val_dir_auc=0.482 sel=0.23489 best=0.23489@ep01 supports=[0.333, 0.333, 0.333]\n",
      "[fold 04] ep 02 lr=2.34e-04 tr_loss=0.1620 (bceT=0.9051, bceD=0.7452, huber=0.0048, util=0.00045, utilS=0.22510) val_loss=0.0186 (bceT=0.8824, bceD=0.7817, huber=0.0037, util=0.00057, utilS=0.28259, posAbs=0.548, pT=0.761) val_trade_auc=0.571 val_dir_auc=0.503 sel=0.43358 best=0.43358@ep02 supports=[0.333, 0.333, 0.333]\n",
      "[fold 04] ep 03 lr=3.00e-04 tr_loss=-0.0762 (bceT=1.0035, bceD=0.7396, huber=0.0029, util=0.00073, utilS=0.36350) val_loss=0.0249 (bceT=1.0029, bceD=0.8382, huber=0.0024, util=0.00062, utilS=0.31022, posAbs=0.649, pT=0.866) val_trade_auc=0.615 val_dir_auc=0.508 sel=0.46248 best=0.46248@ep03 supports=[0.333, 0.335, 0.333]\n",
      "[fold 04] ep 04 lr=2.97e-04 tr_loss=-0.3983 (bceT=1.1270, bceD=0.7457, huber=0.0024, util=0.00110, utilS=0.54882) val_loss=-0.0551 (bceT=1.0819, bceD=0.8248, huber=0.0023, util=0.00072, utilS=0.36172, posAbs=0.650, pT=0.894) val_trade_auc=0.620 val_dir_auc=0.541 sel=0.52394 best=0.52394@ep04 supports=[0.333, 0.333, 0.333]\n",
      "[fold 04] ep 05 lr=2.90e-04 tr_loss=-0.7909 (bceT=1.2137, bceD=0.7112, huber=0.0023, util=0.00151, utilS=0.75493) val_loss=0.1558 (bceT=1.2093, bceD=0.8452, huber=0.0023, util=0.00056, utilS=0.28210, posAbs=0.664, pT=0.928) val_trade_auc=0.635 val_dir_auc=0.509 sel=0.43476 best=0.52394@ep04 supports=[0.335, 0.33, 0.335]\n",
      "[fold 04] ep 06 lr=2.77e-04 tr_loss=-1.1247 (bceT=1.3162, bceD=0.7043, huber=0.0023, util=0.00188, utilS=0.93925) val_loss=0.0387 (bceT=1.2729, bceD=0.8977, huber=0.0024, util=0.00072, utilS=0.36101, posAbs=0.736, pT=0.940) val_trade_auc=0.649 val_dir_auc=0.514 sel=0.51534 best=0.52394@ep04 supports=[0.335, 0.329, 0.336]\n",
      "[fold 04] ep 07 lr=2.61e-04 tr_loss=-1.4985 (bceT=1.3491, bceD=0.6992, huber=0.0024, util=0.00226, utilS=1.13127) val_loss=0.3198 (bceT=1.3452, bceD=0.9409, huber=0.0025, util=0.00048, utilS=0.24066, posAbs=0.756, pT=0.950) val_trade_auc=0.653 val_dir_auc=0.503 sel=0.39171 best=0.52394@ep04 supports=[0.334, 0.329, 0.337]\n",
      "[fold 04] ep 08 lr=2.40e-04 tr_loss=-1.7776 (bceT=1.4025, bceD=0.6614, huber=0.0023, util=0.00255, utilS=1.27378) val_loss=0.6583 (bceT=1.3725, bceD=0.9977, huber=0.0024, util=0.00017, utilS=0.08612, posAbs=0.775, pT=0.953) val_trade_auc=0.658 val_dir_auc=0.483 sel=0.23100 best=0.52394@ep04 supports=[0.333, 0.328, 0.339]\n",
      "[fold 04] ep 09 lr=2.17e-04 tr_loss=-2.0283 (bceT=1.4353, bceD=0.6481, huber=0.0022, util=0.00281, utilS=1.40265) val_loss=0.6154 (bceT=1.4209, bceD=1.0437, huber=0.0021, util=0.00025, utilS=0.12405, posAbs=0.791, pT=0.959) val_trade_auc=0.665 val_dir_auc=0.490 sel=0.27101 best=0.52394@ep04 supports=[0.332, 0.327, 0.342]\n",
      "[fold 04] ep 10 lr=1.91e-04 tr_loss=-2.1996 (bceT=1.4663, bceD=0.6420, huber=0.0021, util=0.00299, utilS=1.49277) val_loss=0.8106 (bceT=1.4337, bceD=1.0382, huber=0.0020, util=0.00006, utilS=0.02772, posAbs=0.793, pT=0.960) val_trade_auc=0.661 val_dir_auc=0.482 sel=0.17240 best=0.52394@ep04 supports=[0.33, 0.326, 0.344]\n",
      "[fold 04] ep 11 lr=1.64e-04 tr_loss=-2.4683 (bceT=1.4957, bceD=0.6037, huber=0.0021, util=0.00325, utilS=1.62565) val_loss=0.7351 (bceT=1.4419, bceD=1.0595, huber=0.0019, util=0.00014, utilS=0.07064, posAbs=0.789, pT=0.961) val_trade_auc=0.666 val_dir_auc=0.490 sel=0.21758 best=0.52394@ep04 supports=[0.329, 0.325, 0.346]\n",
      "[fold 04] chosen thresholds on VAL: thr_trade=0.500 thr_dir=0.700 | val pnl_sum=0.2575 val trade_rate=0.510\n",
      "[fold 04] TEST (fixed thresholds from VAL): trade_auc=0.540 dir_auc=0.544 soft_utilS=0.11153 pnl_sum=0.3130 trade_rate=0.442 trades=499\n",
      "Saved fold bundle: fold_04_meta.json\n",
      "\n",
      "Saved overall best bundle as: overall_best\n",
      "\n",
      "==========================================================================================\n",
      "CV summary (two-head; utility selection; TEST uses thresholds selected on VAL):\n",
      "   fold  val_trade_auc  val_dir_auc  val_soft_utilS  val_loss  test_trade_auc  \\\n",
      "0     1       0.498671     0.570041        0.109941  0.778558        0.586658   \n",
      "1     2       0.605555     0.609617        0.223769  0.474791        0.549334   \n",
      "2     3       0.539981     0.542642        0.624286 -0.287935        0.659554   \n",
      "3     4       0.619898     0.540745        0.361718 -0.055128        0.539928   \n",
      "\n",
      "   test_dir_auc  test_soft_utilS  test_loss  thr_trade  thr_dir  \\\n",
      "0      0.595083         0.257206   0.536341   0.964005     0.70   \n",
      "1      0.498768         0.720119  -0.577922   0.965040     0.55   \n",
      "2      0.489718        -0.211531   1.319214   0.500000     0.70   \n",
      "3      0.544203         0.111533   0.490145   0.500000     0.70   \n",
      "\n",
      "   test_trade_rate_pred  test_pnl_sum  test_n_trades  best_sel  \n",
      "0              0.125664     -0.005607            142  0.280953  \n",
      "1              0.308850      0.361675            349  0.406654  \n",
      "2              0.612389      0.040979            692  0.787078  \n",
      "3              0.441593      0.312985            499  0.523941  \n",
      "\n",
      "Means:\n",
      "fold                      2.500000\n",
      "val_trade_auc             0.566026\n",
      "val_dir_auc               0.565761\n",
      "val_soft_utilS            0.329928\n",
      "val_loss                  0.227572\n",
      "test_trade_auc            0.583868\n",
      "test_dir_auc              0.531943\n",
      "test_soft_utilS           0.219331\n",
      "test_loss                 0.441945\n",
      "thr_trade                 0.732261\n",
      "thr_dir                   0.662500\n",
      "test_trade_rate_pred      0.372124\n",
      "test_pnl_sum              0.177508\n",
      "test_n_trades           420.500000\n",
      "best_sel                  0.499657\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# Step 12: Walk-forward CV run + saving per-fold bundles + overall best\n",
    "# ======================================================================\n",
    "\n",
    "def run_walk_forward_cv_twohead_fixedH() -> Tuple[pd.DataFrame, List[Dict[str, Any]], str]:\n",
    "    fold_artifacts: List[Dict[str, Any]] = []\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "\n",
    "    best_overall_sel = -1e18\n",
    "    best_overall_name = None\n",
    "\n",
    "    for fi, (idx_tr, idx_va, idx_te) in enumerate(walk_splits, 1):\n",
    "        print(\"\\n\" + \"=\" * 90)\n",
    "        print(f\"FOLD {fi}/{len(walk_splits)} sizes: train={len(idx_tr)} val={len(idx_va)} test={len(idx_te)}\")\n",
    "        print(f\"True trade ratio (val):  {split_trade_ratio(idx_va, sample_t, y_trade):.3f}\")\n",
    "        print(f\"True trade ratio (test): {split_trade_ratio(idx_te, sample_t, y_trade):.3f}\")\n",
    "\n",
    "        X_scaled, node_params = fit_scale_nodes_train_only(X_node_raw, sample_t, idx_tr, max_abs=float(CFG[\"max_abs_feat\"]))\n",
    "        if bool(CFG.get(\"edge_scale\", True)):\n",
    "            edge_scaled, edge_params = fit_scale_edges_train_only(edge_feat, sample_t, idx_tr, max_abs=float(CFG[\"max_abs_edge\"]))\n",
    "        else:\n",
    "            edge_scaled = edge_feat.astype(np.float32)\n",
    "            edge_params = None\n",
    "\n",
    "        artifact = train_one_fold_twohead_fixedH(\n",
    "            fold_id=fi,\n",
    "            X_scaled=X_scaled,\n",
    "            edge_scaled=edge_scaled,\n",
    "            idx_train=idx_tr,\n",
    "            idx_val=idx_va,\n",
    "            idx_test=idx_te,\n",
    "            node_scaler_params=node_params,\n",
    "            edge_scaler_params=edge_params,\n",
    "            cfg=CFG,\n",
    "        )\n",
    "\n",
    "        fold_name = f\"fold_{fi:02d}\"\n",
    "        extra_meta = {\n",
    "            \"kind\": \"fold_best\",\n",
    "            \"fold\": fi,\n",
    "            \"best_epoch\": artifact[\"best_epoch\"],\n",
    "            \"best_sel\": artifact[\"best_sel\"],\n",
    "            \"thr_trade\": artifact[\"thr_trade\"],\n",
    "            \"thr_dir\": artifact[\"thr_dir\"],\n",
    "            \"idx_train\": artifact[\"idx_train\"].tolist(),\n",
    "            \"idx_val\": artifact[\"idx_val\"].tolist(),\n",
    "            \"idx_test\": artifact[\"idx_test\"].tolist(),\n",
    "        }\n",
    "        saved = save_bundle(\n",
    "            bundle_dir=ART_DIR,\n",
    "            name=fold_name,\n",
    "            model_state=artifact[\"model_state\"],\n",
    "            cfg=CFG,\n",
    "            node_scaler_params=artifact[\"node_scaler_params\"],\n",
    "            edge_scaler_params=artifact[\"edge_scaler_params\"],\n",
    "            extra_meta=extra_meta,\n",
    "        )\n",
    "        print(\"Saved fold bundle:\", saved[\"meta\"].name)\n",
    "\n",
    "        if float(artifact[\"best_sel\"]) > best_overall_sel:\n",
    "            best_overall_sel = float(artifact[\"best_sel\"])\n",
    "            best_overall_name = fold_name\n",
    "\n",
    "        fold_artifacts.append(artifact)\n",
    "\n",
    "        rows.append({\n",
    "            \"fold\": fi,\n",
    "            \"val_trade_auc\": artifact[\"val_eval\"][\"trade_auc\"],\n",
    "            \"val_dir_auc\": artifact[\"val_eval\"][\"dir_auc\"],\n",
    "            \"val_soft_utilS\": artifact[\"val_eval\"][\"soft_util_scaled_mean\"],\n",
    "            \"val_loss\": artifact[\"val_eval\"][\"loss\"],\n",
    "            \"test_trade_auc\": artifact[\"test_eval\"][\"trade_auc\"],\n",
    "            \"test_dir_auc\": artifact[\"test_eval\"][\"dir_auc\"],\n",
    "            \"test_soft_utilS\": artifact[\"test_eval\"][\"soft_util_scaled_mean\"],\n",
    "            \"test_loss\": artifact[\"test_eval\"][\"loss\"],\n",
    "            \"thr_trade\": artifact[\"thr_trade\"],\n",
    "            \"thr_dir\": artifact[\"thr_dir\"],\n",
    "            \"test_trade_rate_pred\": artifact[\"pnl_test\"][\"trade_rate\"],\n",
    "            \"test_pnl_sum\": artifact[\"pnl_test\"][\"pnl_sum\"],\n",
    "            \"test_n_trades\": artifact[\"pnl_test\"][\"n_trades\"],\n",
    "            \"best_sel\": artifact[\"best_sel\"],\n",
    "        })\n",
    "\n",
    "    cv_summary = pd.DataFrame(rows)\n",
    "\n",
    "    assert best_overall_name is not None\n",
    "    overall_name = \"overall_best\"\n",
    "\n",
    "    best_bundle = load_bundle(ART_DIR, best_overall_name)\n",
    "    extra_meta = {\n",
    "        \"kind\": \"overall_best\",\n",
    "        \"source_name\": best_overall_name,\n",
    "        \"source_fold\": best_bundle[\"meta\"].get(\"fold\", None),\n",
    "        \"thr_trade\": best_bundle[\"meta\"][\"thr_trade\"],\n",
    "        \"thr_dir\": best_bundle[\"meta\"][\"thr_dir\"],\n",
    "        \"idx_train\": best_bundle[\"meta\"][\"idx_train\"],\n",
    "        \"idx_val\": best_bundle[\"meta\"][\"idx_val\"],\n",
    "        \"idx_test\": best_bundle[\"meta\"][\"idx_test\"],\n",
    "    }\n",
    "    save_bundle(\n",
    "        bundle_dir=ART_DIR,\n",
    "        name=overall_name,\n",
    "        model_state=best_bundle[\"state\"],\n",
    "        cfg=CFG,\n",
    "        node_scaler_params=best_bundle[\"node_scaler_params\"],\n",
    "        edge_scaler_params=best_bundle[\"edge_scaler_params\"],\n",
    "        extra_meta=extra_meta,\n",
    "    )\n",
    "    print(\"\\nSaved overall best bundle as:\", overall_name)\n",
    "\n",
    "    return cv_summary, fold_artifacts, overall_name\n",
    "\n",
    "\n",
    "cv_summary_twohead, fold_artifacts_twohead, overall_best_name = run_walk_forward_cv_twohead_fixedH()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"CV summary (two-head; utility selection; TEST uses thresholds selected on VAL):\")\n",
    "print(cv_summary_twohead)\n",
    "print(\"\\nMeans:\")\n",
    "print(cv_summary_twohead.mean(numeric_only=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================================================\n",
      "FINAL HOLDOUT (10%) using overall_best\n",
      "bundle: overall_best\n",
      "trade_auc=0.503 | dir_auc=0.498 | loss=1.0657\n",
      "soft_util=-0.000145 | soft_utilS=-0.07254 | pos_abs=0.7802 | p_trade_mean=0.958\n",
      "pnl_sum=-0.4391 | trade_rate=0.629 | trades=790\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# Step 13: Evaluate a saved bundle on FINAL holdout\n",
    "# ======================================================================\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_bundle_on_indices(bundle_dir: Path, name: str, indices: np.ndarray, label: str) -> Dict[str, Any]:\n",
    "    bundle = load_bundle(bundle_dir, name)\n",
    "    cfg = bundle[\"meta\"][\"cfg\"]\n",
    "\n",
    "    model = GraphWaveNetTwoHeadFixedH(\n",
    "        node_in=int(X_node_raw.shape[-1]),\n",
    "        edge_dim=int(edge_feat.shape[-1]),\n",
    "        cfg=cfg,\n",
    "        n_nodes=len(ASSETS),\n",
    "        target_node=TARGET_NODE,\n",
    "    ).to(DEVICE)\n",
    "    model.load_state_dict(bundle[\"state\"])\n",
    "    model.eval()\n",
    "\n",
    "    X_scaled = apply_scaler_params(X_node_raw.astype(np.float32), bundle[\"node_scaler_params\"])\n",
    "    if bundle[\"edge_scaler_params\"] is not None:\n",
    "        E_scaled = apply_scaler_params(edge_feat.astype(np.float32), bundle[\"edge_scaler_params\"])\n",
    "    else:\n",
    "        E_scaled = edge_feat.astype(np.float32)\n",
    "\n",
    "    idx_train_saved = np.asarray(bundle[\"meta\"][\"idx_train\"], dtype=np.int64)\n",
    "    t_train = sample_t[idx_train_saved]\n",
    "    ytr_train = y_trade[t_train].astype(np.int64)\n",
    "    ytb_train = y_tb[t_train].astype(np.int64)\n",
    "\n",
    "    pos_w_trade = compute_pos_weights_binary(ytr_train)\n",
    "    bce_trade = nn.BCEWithLogitsLoss(pos_weight=pos_w_trade)\n",
    "\n",
    "    mask_tr = (ytb_train != 1)\n",
    "    ydir_train = (ytb_train[mask_tr] == 2).astype(np.int64)\n",
    "    pos_w_dir = compute_pos_weights_binary(ydir_train) if ydir_train.size else torch.tensor([1.0], device=DEVICE)\n",
    "    bce_dir = nn.BCEWithLogitsLoss(pos_weight=pos_w_dir)\n",
    "\n",
    "    ev = eval_twohead_on_indices(model, X_scaled, E_scaled, indices.astype(np.int64), bce_trade, bce_dir, cfg)\n",
    "\n",
    "    thr_trade = float(bundle[\"meta\"][\"thr_trade\"])\n",
    "    thr_dir = float(bundle[\"meta\"][\"thr_dir\"])\n",
    "    pnl = pnl_from_probs_3class(ev[\"prob3\"], ev[\"exit_ret\"], thr_trade, thr_dir, float(cfg[\"cost_bps\"]))\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 90)\n",
    "    print(label)\n",
    "    print(f\"bundle: {name}\")\n",
    "    print(f\"trade_auc={ev['trade_auc']:.3f} | dir_auc={ev['dir_auc']:.3f} | loss={ev['loss']:.4f}\")\n",
    "    print(f\"soft_util={ev['soft_util_mean']:.6f} | soft_utilS={ev['soft_util_scaled_mean']:.5f} | pos_abs={ev['pos_abs_mean']:.4f} | p_trade_mean={ev['p_trade_mean']:.3f}\")\n",
    "    print(f\"pnl_sum={pnl['pnl_sum']:.4f} | trade_rate={pnl['trade_rate']:.3f} | trades={pnl['n_trades']}\")\n",
    "    return {\"eval\": ev, \"pnl\": pnl}\n",
    "\n",
    "\n",
    "holdout_indices = idx_final_test.astype(np.int64)\n",
    "_ = evaluate_bundle_on_indices(ART_DIR, overall_best_name, holdout_indices, label=\"FINAL HOLDOUT (10%) using overall_best\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================================================\n",
      "PRODUCTION FIT: train on CV(90%) -> select thresholds on val_final -> eval on FINAL holdout(10%)\n",
      "Sizes:\n",
      "  train_final: 10175\n",
      "  val_final  : 1130\n",
      "  holdout    : 1256\n",
      "True trade ratio (val_final): 0.504\n",
      "True trade ratio (holdout):   0.576\n",
      "[fold 99] ep 01 lr=9.80e-05 tr_loss=0.5809 (bceT=1.0744, bceD=0.7324, huber=0.0039, util=0.00007, utilS=0.03489) val_loss=0.5413 (bceT=0.9656, bceD=0.7410, huber=0.0029, util=0.00006, utilS=0.02855, posAbs=0.139, pT=0.416) val_trade_auc=0.506 val_dir_auc=0.521 sel=0.18471 best=0.18471@ep01 supports=[0.333, 0.334, 0.333]\n",
      "[fold 99] ep 02 lr=2.34e-04 tr_loss=0.2438 (bceT=0.8979, bceD=0.7396, huber=0.0025, util=0.00036, utilS=0.17821) val_loss=-0.0948 (bceT=0.8833, bceD=0.7575, huber=0.0019, util=0.00067, utilS=0.33499, posAbs=0.388, pT=0.678) val_trade_auc=0.552 val_dir_auc=0.580 sel=0.50900 best=0.50900@ep02 supports=[0.333, 0.334, 0.333]\n",
      "[fold 99] ep 03 lr=3.00e-04 tr_loss=-0.1070 (bceT=0.9431, bceD=0.7564, huber=0.0022, util=0.00074, utilS=0.37027) val_loss=0.4931 (bceT=1.0571, bceD=0.8292, huber=0.0023, util=0.00017, utilS=0.08403, posAbs=0.536, pT=0.815) val_trade_auc=0.551 val_dir_auc=0.567 sel=0.25402 best=0.50900@ep02 supports=[0.333, 0.333, 0.333]\n",
      "[fold 99] ep 04 lr=2.97e-04 tr_loss=-0.4351 (bceT=1.0590, bceD=0.7643, huber=0.0022, util=0.00112, utilS=0.55819) val_loss=0.4907 (bceT=1.2029, bceD=0.8893, huber=0.0021, util=0.00024, utilS=0.12124, posAbs=0.626, pT=0.880) val_trade_auc=0.577 val_dir_auc=0.566 sel=0.29103 best=0.50900@ep02 supports=[0.332, 0.336, 0.332]\n",
      "[fold 99] ep 05 lr=2.90e-04 tr_loss=-0.8935 (bceT=1.2024, bceD=0.7639, huber=0.0022, util=0.00163, utilS=0.81358) val_loss=0.8989 (bceT=1.2889, bceD=0.9543, huber=0.0021, util=-0.00011, utilS=-0.05646, posAbs=0.674, pT=0.903) val_trade_auc=0.582 val_dir_auc=0.544 sel=0.10663 best=0.50900@ep02 supports=[0.331, 0.337, 0.332]\n",
      "[fold 99] ep 06 lr=2.77e-04 tr_loss=-1.1791 (bceT=1.2424, bceD=0.7425, huber=0.0021, util=0.00192, utilS=0.95995) val_loss=1.2587 (bceT=1.3983, bceD=0.9770, huber=0.0020, util=-0.00043, utilS=-0.21324, posAbs=0.706, pT=0.923) val_trade_auc=0.591 val_dir_auc=0.532 sel=-0.05373 best=0.50900@ep02 supports=[0.33, 0.339, 0.331]\n",
      "[fold 99] ep 07 lr=2.61e-04 tr_loss=-1.4233 (bceT=1.3124, bceD=0.7300, huber=0.0020, util=0.00218, utilS=1.09245) val_loss=1.3301 (bceT=1.4279, bceD=0.9909, huber=0.0019, util=-0.00048, utilS=-0.24131, posAbs=0.706, pT=0.927) val_trade_auc=0.594 val_dir_auc=0.497 sel=-0.09216 best=0.50900@ep02 supports=[0.33, 0.338, 0.332]\n",
      "[fold 99] ep 08 lr=2.40e-04 tr_loss=-1.7610 (bceT=1.3506, bceD=0.7146, huber=0.0020, util=0.00253, utilS=1.26548) val_loss=1.2214 (bceT=1.4594, bceD=0.9422, huber=0.0019, util=-0.00038, utilS=-0.19000, posAbs=0.687, pT=0.933) val_trade_auc=0.601 val_dir_auc=0.532 sel=-0.03032 best=0.50900@ep02 supports=[0.331, 0.336, 0.333]\n",
      "[fold 99] ep 09 lr=2.17e-04 tr_loss=-2.1146 (bceT=1.4185, bceD=0.6705, huber=0.0019, util=0.00289, utilS=1.44663) val_loss=0.9091 (bceT=1.5369, bceD=0.9462, huber=0.0019, util=-0.00004, utilS=-0.01957, posAbs=0.719, pT=0.943) val_trade_auc=0.603 val_dir_auc=0.555 sel=0.14690 best=0.50900@ep02 supports=[0.331, 0.335, 0.334]\n",
      "[fold 99] chosen thresholds on VAL: thr_trade=0.700 thr_dir=0.550 | val pnl_sum=0.5610 val trade_rate=0.421\n",
      "[fold 99] TEST (fixed thresholds from VAL): trade_auc=0.471 dir_auc=0.492 soft_utilS=-0.03569 pnl_sum=-0.6988 trade_rate=0.533 trades=669\n",
      "\n",
      "Saved production bundle as: production_best\n",
      "\n",
      "==========================================================================================\n",
      "TEST-ONLY FROM PRODUCTION BUNDLE (holdout)\n",
      "bundle: production_best\n",
      "trade_auc=0.471 | dir_auc=0.492 | loss=0.6667\n",
      "soft_util=-0.000071 | soft_utilS=-0.03569 | pos_abs=0.4658 | p_trade_mean=0.710\n",
      "pnl_sum=-0.6988 | trade_rate=0.533 | trades=669\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# Step 14: Production fit on CV(90%) -> select thresholds on val_final -> eval holdout(10%)\n",
    "# ======================================================================\n",
    "\n",
    "def production_fit_and_save() -> str:\n",
    "    print(\"\\n\" + \"=\" * 90)\n",
    "    print(\"PRODUCTION FIT: train on CV(90%) -> select thresholds on val_final -> eval on FINAL holdout(10%)\")\n",
    "\n",
    "    val_w = max(1, int(float(CFG[\"val_window_frac\"]) * n_samples_cv))\n",
    "    train_end = n_samples_cv - val_w\n",
    "\n",
    "    idx_train_final = np.arange(0, train_end, dtype=np.int64)\n",
    "    idx_val_final = np.arange(train_end, n_samples_cv, dtype=np.int64)\n",
    "    idx_holdout = idx_final_test.astype(np.int64)\n",
    "\n",
    "    print(\"Sizes:\")\n",
    "    print(\"  train_final:\", len(idx_train_final))\n",
    "    print(\"  val_final  :\", len(idx_val_final))\n",
    "    print(\"  holdout    :\", len(idx_holdout))\n",
    "    print(f\"True trade ratio (val_final): {split_trade_ratio(idx_val_final, sample_t, y_trade):.3f}\")\n",
    "    print(f\"True trade ratio (holdout):   {split_trade_ratio(idx_holdout, sample_t, y_trade):.3f}\")\n",
    "\n",
    "    X_scaled, node_params = fit_scale_nodes_train_only(X_node_raw, sample_t, idx_train_final, max_abs=float(CFG[\"max_abs_feat\"]))\n",
    "    if bool(CFG.get(\"edge_scale\", True)):\n",
    "        edge_scaled, edge_params = fit_scale_edges_train_only(edge_feat, sample_t, idx_train_final, max_abs=float(CFG[\"max_abs_edge\"]))\n",
    "    else:\n",
    "        edge_scaled = edge_feat.astype(np.float32)\n",
    "        edge_params = None\n",
    "\n",
    "    artifact = train_one_fold_twohead_fixedH(\n",
    "        fold_id=99,\n",
    "        X_scaled=X_scaled,\n",
    "        edge_scaled=edge_scaled,\n",
    "        idx_train=idx_train_final,\n",
    "        idx_val=idx_val_final,\n",
    "        idx_test=idx_holdout,\n",
    "        node_scaler_params=node_params,\n",
    "        edge_scaler_params=edge_params,\n",
    "        cfg=CFG,\n",
    "    )\n",
    "\n",
    "    prod_name = \"production_best\"\n",
    "    extra_meta = {\n",
    "        \"kind\": \"production_best\",\n",
    "        \"fold\": 99,\n",
    "        \"best_epoch\": artifact[\"best_epoch\"],\n",
    "        \"best_sel\": artifact[\"best_sel\"],\n",
    "        \"thr_trade\": artifact[\"thr_trade\"],\n",
    "        \"thr_dir\": artifact[\"thr_dir\"],\n",
    "        \"idx_train\": artifact[\"idx_train\"].tolist(),\n",
    "        \"idx_val\": artifact[\"idx_val\"].tolist(),\n",
    "        \"idx_test\": artifact[\"idx_test\"].tolist(),\n",
    "    }\n",
    "    save_bundle(\n",
    "        bundle_dir=ART_DIR,\n",
    "        name=prod_name,\n",
    "        model_state=artifact[\"model_state\"],\n",
    "        cfg=CFG,\n",
    "        node_scaler_params=artifact[\"node_scaler_params\"],\n",
    "        edge_scaler_params=artifact[\"edge_scaler_params\"],\n",
    "        extra_meta=extra_meta,\n",
    "    )\n",
    "    print(\"\\nSaved production bundle as:\", prod_name)\n",
    "    return prod_name\n",
    "\n",
    "\n",
    "production_name = production_fit_and_save()\n",
    "prod_bundle = load_bundle(ART_DIR, production_name)\n",
    "idx_eval = np.asarray(prod_bundle[\"meta\"][\"idx_test\"], dtype=np.int64)\n",
    "_ = evaluate_bundle_on_indices(ART_DIR, production_name, idx_eval, label=\"TEST-ONLY FROM PRODUCTION BUNDLE (holdout)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recbole_new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
