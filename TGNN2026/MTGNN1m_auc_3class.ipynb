{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nGraph WaveNet / MTGNN-style (Adaptive adjacency + Dilated Gated TCN)\\n3-node graph trading model: ETH (target), BTC, ADA on 1-minute data.\\n\\nNotes on labels:\\n- Your triple-barrier labels already provide a clean 3-class target:\\n    y_tb: 0=down (SHORT), 1=flat (FLAT / no-trade), 2=up (LONG)\\n- This notebook trains a single 3-class model (SHORT/FLAT/LONG) and reports:\\n    trade_auc: AUC(trade vs no-trade) where trade = {SHORT,LONG} vs FLAT\\n    dir_auc:   AUC(direction) on true-trade samples only (LONG vs SHORT)\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Graph WaveNet / MTGNN-style (Adaptive adjacency + Dilated Gated TCN)\n",
    "3-node graph trading model: ETH (target), BTC, ADA on 1-minute data.\n",
    "\n",
    "Notes on labels:\n",
    "- Your triple-barrier labels already provide a clean 3-class target:\n",
    "    y_tb: 0=down (SHORT), 1=flat (FLAT / no-trade), 2=up (LONG)\n",
    "- This notebook trains a single 3-class model (SHORT/FLAT/LONG) and reports:\n",
    "    trade_auc: AUC(trade vs no-trade) where trade = {SHORT,LONG} vs FLAT\n",
    "    dir_auc:   AUC(direction) on true-trade samples only (LONG vs SHORT)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cpu\n",
      "EDGE_LIST: ['ADA->BTC', 'ADA->ETH', 'BTC->ADA', 'BTC->ETH', 'ETH->ADA', 'ETH->BTC', 'ADA->ADA', 'BTC->BTC', 'ETH->ETH']\n",
      "EDGE_INDEX: [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1], [0, 0], [1, 1], [2, 2]]\n"
     ]
    }
   ],
   "source": [
    "# Step 0: imports + config + seed  (COPIED AS-IS FROM YOUR NOTEBOOK)\n",
    "# ======================================================================\n",
    "\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, roc_auc_score\n",
    "\n",
    "\n",
    "def seed_everything(seed: int = 1234) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "seed_everything(100)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "\n",
    "torch.set_num_threads(max(1, os.cpu_count() or 4))\n",
    "\n",
    "CFG: Dict[str, Any] = {\n",
    "    # data\n",
    "    \"freq\": \"1min\",\n",
    "    \"data_dir\": Path(\"../dataset\"),\n",
    "    \"final_test_frac\": 0.10,\n",
    "\n",
    "    # order book\n",
    "    \"book_levels\": 15,\n",
    "    \"top_levels\": 5,\n",
    "    \"near_levels\": 5,\n",
    "\n",
    "    # walk-forward windows (in sample-space)\n",
    "    \"train_min_frac\": 0.50,\n",
    "    \"val_window_frac\": 0.10,\n",
    "    \"test_window_frac\": 0.10,\n",
    "    \"step_window_frac\": 0.10,\n",
    "\n",
    "    # scaling\n",
    "    \"max_abs_feat\": 10.0,\n",
    "    \"max_abs_edge\": 6.0,\n",
    "\n",
    "    # correlations / graph\n",
    "    \"corr_windows\": [6 * 5, 12 * 5, 24 * 5, 48 * 5, 84 * 5],  # 30m,1h,2h,4h,7h\n",
    "    \"corr_lags\": [0, 1, 2, 5],  # lead-lag (no leakage)\n",
    "    \"edges_mode\": \"all_pairs\",  # \"manual\" | \"all_pairs\"\n",
    "    \"edges\": [(\"ADA\", \"BTC\"), (\"ADA\", \"ETH\"), (\"ETH\", \"BTC\")],  # used if edges_mode=\"manual\"\n",
    "    \"add_self_loops\": True,\n",
    "    \"edge_transform\": \"fisher\",  # \"none\" | \"fisher\"\n",
    "    \"edge_scale\": True,\n",
    "    \"edge_dropout\": 0.10,\n",
    "\n",
    "    # triple-barrier\n",
    "    \"tb_horizon\": 1 * 30,\n",
    "    \"lookback\": 4 * 12 * 5,\n",
    "    \"tb_pt_mult\": 1.2,\n",
    "    \"tb_sl_mult\": 1.1,\n",
    "    \"tb_min_barrier\": 0.001,\n",
    "    \"tb_max_barrier\": 0.006,\n",
    "\n",
    "    # training\n",
    "    \"batch_size\": 128,\n",
    "    \"epochs\": 20,\n",
    "    \"lr\": 3e-4,\n",
    "    \"weight_decay\": 5e-4,\n",
    "    \"grad_clip\": 1.0,\n",
    "    \"dropout\": 0.15,\n",
    "\n",
    "    # stability tricks\n",
    "    \"label_smoothing\": 0.02,\n",
    "    \"use_weighted_sampler\": True,\n",
    "    \"use_onecycle\": True,\n",
    "\n",
    "    # model dims\n",
    "    \"hidden\": 128,\n",
    "    \"gnn_layers\": 3,\n",
    "\n",
    "    # --- Temporal (Conv -> AttnPool)\n",
    "    \"tcn_channels\": 128,\n",
    "    \"tcn_layers\": 3,\n",
    "    \"tcn_kernel\": 2,\n",
    "    \"tcn_dropout\": 0.20,\n",
    "    \"tcn_causal\": True,\n",
    "\n",
    "    \"attn_pool_hidden\": 128,\n",
    "    \"attn_pool_dropout\": 0.10,\n",
    "\n",
    "    # --- Learnable adjacency (MTGNN-style)\n",
    "    # A_learned options:\n",
    "    #   \"emb\": A = softmax((E1 @ E2^T)/temp)\n",
    "    #   \"matrix\": A = softmax(A_logits/temp)\n",
    "    \"adj_mode\": \"emb\",\n",
    "    \"adj_emb_dim\": 8,\n",
    "    \"adj_temperature\": 1.0,\n",
    "\n",
    "    # A_prior from edge_attr (last timestep of the sequence)\n",
    "    \"prior_use_abs\": False,       # if True: use abs(mean(edge_attr)) for weights\n",
    "    \"prior_diag_boost\": 1.0,      # ensure diag >= this before row-normalization\n",
    "    \"prior_row_normalize\": True,\n",
    "\n",
    "    # mixing alpha\n",
    "    \"alpha_mode\": \"learned\",      # \"fixed\" | \"learned\"\n",
    "    \"adj_alpha\": 0.50,            # used if alpha_mode=\"fixed\"\n",
    "    \"adj_alpha_min\": 0.05,        # clamp if learned\n",
    "    \"adj_alpha_max\": 0.95,\n",
    "\n",
    "    # adjacency regularization\n",
    "    \"adj_l1_lambda\": 1e-3,\n",
    "    \"adj_prior_lambda\": 1e-2,\n",
    "\n",
    "    # trading eval\n",
    "    \"cost_bps\": 1.0,\n",
    "\n",
    "    # threshold sweep grids (val only)\n",
    "    \"thr_trade_grid\": [0.50, 0.55, 0.60, 0.65, 0.70, 0.75],\n",
    "    \"thr_dir_grid\":   [0.50, 0.55, 0.60, 0.65, 0.70],\n",
    "\n",
    "    # min trades constraints\n",
    "    \"eval_min_trades\": 50,\n",
    "\n",
    "    # anti-overtrading threshold selection\n",
    "    \"max_trade_rate_val\": 0.65,\n",
    "    \"trade_rate_penalty\": 0.10,\n",
    "    \"thr_objective\": \"pnl_sum\",  # \"pnl_sum\" | \"pnl_sharpe\" | \"pnl_per_trade\"\n",
    "\n",
    "    # dynamic quantile thresholds for thr_trade\n",
    "    \"proxy_target_trades\": [50, 100, 200],\n",
    "}\n",
    "\n",
    "ASSETS = [\"ADA\", \"BTC\", \"ETH\"]\n",
    "ASSET2IDX = {a: i for i, a in enumerate(ASSETS)}\n",
    "TARGET_ASSET = \"ETH\"\n",
    "TARGET_NODE = ASSET2IDX[TARGET_ASSET]\n",
    "\n",
    "\n",
    "def build_edge_list(cfg: Dict[str, Any], assets: List[str]) -> List[Tuple[str, str]]:\n",
    "    mode = str(cfg.get(\"edges_mode\", \"manual\"))\n",
    "    if mode == \"manual\":\n",
    "        edges = list(cfg[\"edges\"])\n",
    "    elif mode == \"all_pairs\":\n",
    "        edges = [(s, t) for s in assets for t in assets if s != t]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown edges_mode={mode}\")\n",
    "\n",
    "    if bool(cfg.get(\"add_self_loops\", True)):\n",
    "        edges = edges + [(a, a) for a in assets]\n",
    "    return edges\n",
    "\n",
    "\n",
    "EDGE_LIST = build_edge_list(CFG, ASSETS)\n",
    "EDGE_NAMES = [f\"{s}->{t}\" for s, t in EDGE_LIST]\n",
    "EDGE_INDEX = torch.tensor([[ASSET2IDX[s], ASSET2IDX[t]] for (s, t) in EDGE_LIST], dtype=torch.long)\n",
    "\n",
    "print(\"EDGE_LIST:\", EDGE_NAMES)\n",
    "print(\"EDGE_INDEX:\", EDGE_INDEX.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded df: (12831, 106)\n",
      "Columns example: ['timestamp', 'ADA', 'spread_ADA', 'buys_ADA', 'sells_ADA', 'bids_vol_ADA_0', 'bids_vol_ADA_1', 'bids_vol_ADA_2', 'bids_vol_ADA_3', 'bids_vol_ADA_4', 'bids_vol_ADA_5', 'bids_vol_ADA_6', 'bids_vol_ADA_7', 'bids_vol_ADA_8', 'bids_vol_ADA_9', 'bids_vol_ADA_10', 'bids_vol_ADA_11', 'bids_vol_ADA_12', 'bids_vol_ADA_13', 'bids_vol_ADA_14']\n",
      "Time range: 2021-04-07 11:34:00+00:00 -> 2021-04-16 10:15:00+00:00\n",
      "                  timestamp      ADA  spread_ADA      buys_ADA      sells_ADA  \\\n",
      "0 2021-04-07 11:34:00+00:00  1.16205      0.0001  56936.467913  258248.957367   \n",
      "1 2021-04-07 11:35:00+00:00  1.16800      0.0022  56491.336799   78665.286640   \n",
      "\n",
      "   bids_vol_ADA_0  bids_vol_ADA_1  bids_vol_ADA_2  bids_vol_ADA_3  \\\n",
      "0      876.869995     5984.169922        5.810000       18.240000   \n",
      "1    33769.671875    23137.169922      550.299988      550.299988   \n",
      "\n",
      "   bids_vol_ADA_4  ...  asks_vol_ETH_8  asks_vol_ETH_9  asks_vol_ETH_10  \\\n",
      "0    19844.640625  ...      373.700012      196.699997      2059.709961   \n",
      "1    19012.320312  ...     3873.709961     1954.630005       197.039993   \n",
      "\n",
      "   asks_vol_ETH_11  asks_vol_ETH_12  asks_vol_ETH_13  asks_vol_ETH_14  \\\n",
      "0      3874.989990      5901.209961       178.289993     28512.160156   \n",
      "1     12661.990234     20006.970703     28562.310547      3874.379883   \n",
      "\n",
      "     lr_ADA    lr_BTC    lr_ETH  \n",
      "0  0.000000  0.000000  0.000000  \n",
      "1  0.005107  0.000937  0.001931  \n",
      "\n",
      "[2 rows x 106 columns]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: my original data loading (UNCHANGED / COPIED AS-IS)\n",
    "# ======================================================================\n",
    "\n",
    "def load_asset(asset: str, freq: str, data_dir: Path, book_levels: int, part: Tuple[int, int] = (0, 80)) -> pd.DataFrame:\n",
    "    path = data_dir / f\"{asset}_{freq}.csv\"\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.iloc[int(len(df) * part[0] / 100): int(len(df) * part[1] / 100)]\n",
    "\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"system_time\"]).dt.round(\"min\")\n",
    "    df = df.sort_values(\"timestamp\").set_index(\"timestamp\")\n",
    "\n",
    "    bid_cols = [f\"bids_notional_{i}\" for i in range(book_levels)]\n",
    "    ask_cols = [f\"asks_notional_{i}\" for i in range(book_levels)]\n",
    "\n",
    "    needed = [\"midpoint\", \"spread\", \"buys\", \"sells\"] + bid_cols + ask_cols\n",
    "    missing = [c for c in needed if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"{asset}: missing columns in CSV: {missing[:10]}{'...' if len(missing) > 10 else ''}\")\n",
    "\n",
    "    return df[needed]\n",
    "\n",
    "\n",
    "def load_all_assets() -> pd.DataFrame:\n",
    "    freq = CFG[\"freq\"]\n",
    "    data_dir = CFG[\"data_dir\"]\n",
    "    book_levels = CFG[\"book_levels\"]\n",
    "\n",
    "    def rename_cols(df_one: pd.DataFrame, asset: str) -> pd.DataFrame:\n",
    "        rename_map = {\n",
    "            \"midpoint\": asset,\n",
    "            \"buys\": f\"buys_{asset}\",\n",
    "            \"sells\": f\"sells_{asset}\",\n",
    "            \"spread\": f\"spread_{asset}\",\n",
    "        }\n",
    "        for i in range(book_levels):\n",
    "            rename_map[f\"bids_notional_{i}\"] = f\"bids_vol_{asset}_{i}\"\n",
    "            rename_map[f\"asks_notional_{i}\"] = f\"asks_vol_{asset}_{i}\"\n",
    "        return df_one.rename(columns=rename_map)\n",
    "\n",
    "    df_ada = rename_cols(load_asset(\"ADA\", freq, data_dir, book_levels, part=(0, 75)), \"ADA\")\n",
    "    df_btc = rename_cols(load_asset(\"BTC\", freq, data_dir, book_levels, part=(0, 75)), \"BTC\")\n",
    "    df_eth = rename_cols(load_asset(\"ETH\", freq, data_dir, book_levels, part=(0, 75)), \"ETH\")\n",
    "\n",
    "    df = df_ada.join(df_btc).join(df_eth).reset_index()\n",
    "    return df\n",
    "\n",
    "\n",
    "df = load_all_assets()\n",
    "for a in ASSETS:\n",
    "    df[f\"lr_{a}\"] = np.log(df[a]).diff().fillna(0.0)\n",
    "\n",
    "print(\"Loaded df:\", df.shape)\n",
    "print(\"Columns example:\", df.columns[:20].tolist())\n",
    "print(\"Time range:\", df[\"timestamp\"].min(), \"->\", df[\"timestamp\"].max())\n",
    "print(df.head(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edge_feat shape: (12831, 9, 20) (T,E,edge_dim)\n",
      "edge_dim = 20  = windows * lags = 20\n",
      "Edge names: ['ADA->BTC', 'ADA->ETH', 'BTC->ADA', 'BTC->ETH', 'ETH->ADA', 'ETH->BTC', 'ADA->ADA', 'BTC->BTC', 'ETH->ETH']\n",
      "edge_feat sample [t=100, first 3 edges]:\n",
      " [[ 6.7925054e-01  7.8185719e-01  8.3443433e-01  8.3443433e-01\n",
      "   8.3443433e-01  3.4026209e-01  1.4996846e-01  1.6863135e-01\n",
      "   1.6863135e-01  1.6863135e-01 -2.9266590e-02 -1.5999632e-01\n",
      "  -2.5908518e-01 -2.5908518e-01 -2.5908518e-01  1.4277337e-01\n",
      "  -1.0870378e-02  5.0888385e-04  5.0888385e-04  5.0888385e-04]\n",
      " [ 6.3383293e-01  6.9067067e-01  8.7368768e-01  8.7368768e-01\n",
      "   8.7368768e-01  3.4575835e-01  1.5627505e-01  2.0534241e-01\n",
      "   2.0534241e-01  2.0534241e-01  6.9384493e-02 -1.1163518e-01\n",
      "  -1.7551416e-01 -1.7551416e-01 -1.7551416e-01 -1.6881377e-01\n",
      "  -1.0781832e-01 -6.3380465e-02 -6.3380465e-02 -6.3380465e-02]\n",
      " [ 6.7925054e-01  7.8185719e-01  8.3443433e-01  8.3443433e-01\n",
      "   8.3443433e-01 -4.8168253e-02 -1.6241662e-01 -1.6193953e-01\n",
      "  -1.6193953e-01 -1.6193953e-01 -2.1278685e-01  4.6374347e-02\n",
      "  -1.7361922e-02 -1.7361922e-02 -1.7361922e-02  1.3684873e-01\n",
      "   3.6141057e-02  6.9459870e-02  6.9459870e-02  6.9459870e-02]]\n",
      "edge_feat stats: mean= 0.4511896073818207 std= 0.500860333442688\n"
     ]
    }
   ],
   "source": [
    "# Step 1b: edge features (UNCHANGED / COPIED AS-IS)\n",
    "# ======================================================================\n",
    "\n",
    "def _fisher_z(x: np.ndarray, eps: float = 1e-6) -> np.ndarray:\n",
    "    x = np.clip(x, -0.999, 0.999)\n",
    "    return 0.5 * np.log((1.0 + x + eps) / (1.0 - x + eps))\n",
    "\n",
    "\n",
    "def build_corr_array(\n",
    "    df_: pd.DataFrame,\n",
    "    corr_windows: List[int],\n",
    "    edges: List[Tuple[str, str]],\n",
    "    lags: List[int],\n",
    "    transform: str = \"fisher\",\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Edge features per time:\n",
    "      for edge s->t:\n",
    "        for lag in lags:\n",
    "          corr(lr_s.shift(lag), lr_t) over rolling window\n",
    "    No leakage: shift(lag>0) uses past of source.\n",
    "    Self-loop edges a->a: constant 1.0.\n",
    "    \"\"\"\n",
    "    T_ = len(df_)\n",
    "    E_ = len(edges)\n",
    "    W_ = len(corr_windows)\n",
    "    Lg = len(lags)\n",
    "    out = np.zeros((T_, E_, W_ * Lg), dtype=np.float32)\n",
    "\n",
    "    lr_map = {a: df_[f\"lr_{a}\"].astype(float) for a in ASSETS}\n",
    "\n",
    "    for ei, (s, t) in enumerate(edges):\n",
    "        if s == t:\n",
    "            out[:, ei, :] = 1.0\n",
    "            continue\n",
    "\n",
    "        src0 = lr_map[s]\n",
    "        dst0 = lr_map[t]\n",
    "\n",
    "        feat_idx = 0\n",
    "        for lag in lags:\n",
    "            src = src0.shift(int(lag)) if int(lag) > 0 else src0\n",
    "\n",
    "            for w in corr_windows:\n",
    "                r = src.rolling(int(w), min_periods=1).corr(dst0)\n",
    "                r = np.nan_to_num(r.to_numpy(dtype=np.float32), nan=0.0, posinf=0.0, neginf=0.0)\n",
    "                if transform == \"fisher\":\n",
    "                    r = _fisher_z(r).astype(np.float32)\n",
    "                out[:, ei, feat_idx] = r\n",
    "                feat_idx += 1\n",
    "\n",
    "    return out.astype(np.float32)\n",
    "\n",
    "\n",
    "edge_feat = build_corr_array(\n",
    "    df,\n",
    "    CFG[\"corr_windows\"],\n",
    "    EDGE_LIST,\n",
    "    CFG[\"corr_lags\"],\n",
    "    transform=str(CFG.get(\"edge_transform\", \"fisher\")),\n",
    ")\n",
    "\n",
    "print(\"edge_feat shape:\", edge_feat.shape, \"(T,E,edge_dim)\")\n",
    "print(\"edge_dim =\", edge_feat.shape[-1], \" = windows * lags =\", len(CFG[\"corr_windows\"]) * len(CFG[\"corr_lags\"]))\n",
    "print(\"Edge names:\", EDGE_NAMES)\n",
    "print(\"edge_feat sample [t=100, first 3 edges]:\\n\", edge_feat[100, :3, :])\n",
    "print(\"edge_feat stats: mean=\", float(edge_feat.mean()), \"std=\", float(edge_feat.std()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TB dist [down,flat,up]: [2875 7413 2543]\n",
      "Trade ratio (true): 0.42225859247135844\n"
     ]
    }
   ],
   "source": [
    "# Step 1c: triple-barrier labels (UNCHANGED / COPIED AS-IS)\n",
    "# ======================================================================\n",
    "\n",
    "def triple_barrier_labels_from_lr(\n",
    "    lr: pd.Series,\n",
    "    horizon: int,\n",
    "    vol_window: int,\n",
    "    pt_mult: float,\n",
    "    sl_mult: float,\n",
    "    min_barrier: float,\n",
    "    max_barrier: float,\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      y_tb: {0=down, 1=flat/no-trade, 2=up}\n",
    "      exit_ret: realized log-return to exit (tp/sl/timeout)\n",
    "      exit_t: exit index\n",
    "      thr: barrier per t (float array, len T)\n",
    "    No leakage: vol is shift(1).\n",
    "    \"\"\"\n",
    "    lr = lr.astype(float).copy()\n",
    "    T = len(lr)\n",
    "\n",
    "    vol = lr.rolling(vol_window, min_periods=max(10, vol_window // 10)).std().shift(1)\n",
    "    thr = (vol * np.sqrt(horizon)).clip(lower=min_barrier, upper=max_barrier)\n",
    "\n",
    "    y = np.ones(T, dtype=np.int64)\n",
    "    exit_ret = np.zeros(T, dtype=np.float32)\n",
    "    exit_t = np.arange(T, dtype=np.int64)\n",
    "\n",
    "    lr_np = lr.fillna(0.0).to_numpy(dtype=np.float64)\n",
    "    thr_np = thr.fillna(min_barrier).to_numpy(dtype=np.float64)\n",
    "\n",
    "    for t in range(T - horizon - 1):\n",
    "        up = pt_mult * thr_np[t]\n",
    "        dn = -sl_mult * thr_np[t]\n",
    "\n",
    "        cum = 0.0\n",
    "        hit = 1\n",
    "        et = t + horizon\n",
    "        er = 0.0\n",
    "\n",
    "        for dt in range(1, horizon + 1):\n",
    "            cum += lr_np[t + dt]\n",
    "            if cum >= up:\n",
    "                hit, et, er = 2, t + dt, cum\n",
    "                break\n",
    "            if cum <= dn:\n",
    "                hit, et, er = 0, t + dt, cum\n",
    "                break\n",
    "\n",
    "        if hit == 1:\n",
    "            er = float(np.sum(lr_np[t + 1: t + horizon + 1]))\n",
    "            et = t + horizon\n",
    "\n",
    "        y[t] = hit\n",
    "        exit_ret[t] = er\n",
    "        exit_t[t] = et\n",
    "\n",
    "    return y, exit_ret, exit_t, thr_np\n",
    "\n",
    "\n",
    "y_tb, exit_ret, exit_t, tb_thr = triple_barrier_labels_from_lr(\n",
    "    df[\"lr_ETH\"],\n",
    "    horizon=CFG[\"tb_horizon\"],\n",
    "    vol_window=CFG[\"lookback\"],\n",
    "    pt_mult=CFG[\"tb_pt_mult\"],\n",
    "    sl_mult=CFG[\"tb_sl_mult\"],\n",
    "    min_barrier=CFG[\"tb_min_barrier\"],\n",
    "    max_barrier=CFG[\"tb_max_barrier\"],\n",
    ")\n",
    "\n",
    "# two-stage labels\n",
    "y_trade = (y_tb != 1).astype(np.int64)  # 1=trade, 0=no-trade\n",
    "y_dir = (y_tb == 2).astype(np.int64)    # 1=up, 0=down (meaningful only when y_trade==1)\n",
    "\n",
    "dist = np.bincount(y_tb, minlength=3)\n",
    "print(\"TB dist [down,flat,up]:\", dist)\n",
    "print(\"Trade ratio (true):\", float(y_trade.mean()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_node_raw: (12831, 3, 15) edge_feat: (12831, 9, 20)\n",
      "node_feat_names: ['lr', 'spread', 'log_buys', 'log_sells', 'ofi', 'DI_15', 'DI_L0', 'DI_L1', 'DI_L2', 'DI_L3', 'DI_L4', 'near_ratio_bid', 'near_ratio_ask', 'di_near', 'di_far']\n",
      "n_samples: 12561 | t range: 239 -> 12799\n",
      "Feature stats (TARGET asset, lr): mean= 1.5748046280350536e-05 std= 0.0010532913729548454\n"
     ]
    }
   ],
   "source": [
    "# Step 1d: node tensor (UNCHANGED / COPIED AS-IS)\n",
    "# ======================================================================\n",
    "\n",
    "EPS = 1e-6\n",
    "\n",
    "\n",
    "def safe_log1p(x: np.ndarray) -> np.ndarray:\n",
    "    return np.log1p(np.maximum(x, 0.0))\n",
    "\n",
    "\n",
    "def build_node_tensor(df_: pd.DataFrame) -> Tuple[np.ndarray, List[str]]:\n",
    "    \"\"\"\n",
    "    Features per asset:\n",
    "      lr, spread,\n",
    "      log_buys, log_sells, ofi,\n",
    "      DI_15,\n",
    "      DI_L0..DI_L4,\n",
    "      near_ratio_bid, near_ratio_ask,\n",
    "      di_near, di_far\n",
    "    \"\"\"\n",
    "    book_levels = CFG[\"book_levels\"]\n",
    "    top_k = CFG[\"top_levels\"]\n",
    "    near_k = CFG[\"near_levels\"]\n",
    "\n",
    "    if near_k >= book_levels:\n",
    "        raise ValueError(\"CFG['near_levels'] must be < CFG['book_levels']\")\n",
    "\n",
    "    feat_names = [\n",
    "        \"lr\", \"spread\",\n",
    "        \"log_buys\", \"log_sells\", \"ofi\",\n",
    "        \"DI_15\",\n",
    "        \"DI_L0\", \"DI_L1\", \"DI_L2\", \"DI_L3\", \"DI_L4\",\n",
    "        \"near_ratio_bid\", \"near_ratio_ask\",\n",
    "        \"di_near\", \"di_far\",\n",
    "    ]\n",
    "\n",
    "    feats_all = []\n",
    "    for a in ASSETS:\n",
    "        lr = df_[f\"lr_{a}\"].values.astype(np.float32)\n",
    "        spread = df_[f\"spread_{a}\"].values.astype(np.float32)\n",
    "\n",
    "        buys = df_[f\"buys_{a}\"].values.astype(np.float32)\n",
    "        sells = df_[f\"sells_{a}\"].values.astype(np.float32)\n",
    "\n",
    "        log_buys = safe_log1p(buys).astype(np.float32)\n",
    "        log_sells = safe_log1p(sells).astype(np.float32)\n",
    "\n",
    "        ofi = ((buys - sells) / (buys + sells + EPS)).astype(np.float32)\n",
    "\n",
    "        bids_lvls = np.stack([df_[f\"bids_vol_{a}_{i}\"].values.astype(np.float32) for i in range(book_levels)], axis=1)\n",
    "        asks_lvls = np.stack([df_[f\"asks_vol_{a}_{i}\"].values.astype(np.float32) for i in range(book_levels)], axis=1)\n",
    "\n",
    "        bid_sum = bids_lvls.sum(axis=1)\n",
    "        ask_sum = asks_lvls.sum(axis=1)\n",
    "        di_15 = ((bid_sum - ask_sum) / (bid_sum + ask_sum + EPS)).astype(np.float32)\n",
    "\n",
    "        di_levels = []\n",
    "        for i in range(top_k):\n",
    "            b = bids_lvls[:, i]\n",
    "            s = asks_lvls[:, i]\n",
    "            di_levels.append(((b - s) / (b + s + EPS)).astype(np.float32))\n",
    "        di_l0_4 = np.stack(di_levels, axis=1)  # (T,5)\n",
    "\n",
    "        bid_near = bids_lvls[:, :near_k].sum(axis=1)\n",
    "        ask_near = asks_lvls[:, :near_k].sum(axis=1)\n",
    "        bid_far = bids_lvls[:, near_k:].sum(axis=1)\n",
    "        ask_far = asks_lvls[:, near_k:].sum(axis=1)\n",
    "\n",
    "        near_ratio_bid = (bid_near / (bid_far + EPS)).astype(np.float32)\n",
    "        near_ratio_ask = (ask_near / (ask_far + EPS)).astype(np.float32)\n",
    "\n",
    "        di_near = ((bid_near - ask_near) / (bid_near + ask_near + EPS)).astype(np.float32)\n",
    "        di_far = ((bid_far - ask_far) / (bid_far + ask_far + EPS)).astype(np.float32)\n",
    "\n",
    "        Xa = np.column_stack([\n",
    "            lr, spread,\n",
    "            log_buys, log_sells, ofi,\n",
    "            di_15,\n",
    "            di_l0_4[:, 0], di_l0_4[:, 1], di_l0_4[:, 2], di_l0_4[:, 3], di_l0_4[:, 4],\n",
    "            near_ratio_bid, near_ratio_ask,\n",
    "            di_near, di_far,\n",
    "        ]).astype(np.float32)\n",
    "\n",
    "        feats_all.append(Xa)\n",
    "\n",
    "    X = np.stack(feats_all, axis=1).astype(np.float32)  # (T,N,F)\n",
    "    return X, feat_names\n",
    "\n",
    "\n",
    "X_node_raw, node_feat_names = build_node_tensor(df)\n",
    "T = len(df)\n",
    "L = CFG[\"lookback\"]\n",
    "H = CFG[\"tb_horizon\"]\n",
    "\n",
    "t_min = L - 1\n",
    "t_max = T - H - 2\n",
    "sample_t = np.arange(t_min, t_max + 1)\n",
    "n_samples = len(sample_t)\n",
    "\n",
    "print(\"X_node_raw:\", X_node_raw.shape, \"edge_feat:\", edge_feat.shape)\n",
    "print(\"node_feat_names:\", node_feat_names)\n",
    "print(\"n_samples:\", n_samples, \"| t range:\", int(sample_t[0]), \"->\", int(sample_t[-1]))\n",
    "print(\n",
    "    \"Feature stats (TARGET asset, lr):\",\n",
    "    \"mean=\", float(X_node_raw[:, TARGET_NODE, node_feat_names.index(\"lr\")].mean()),\n",
    "    \"std=\", float(X_node_raw[:, TARGET_NODE, node_feat_names.index(\"lr\")].std()),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holdout split:\n",
      "  n_samples total: 12561\n",
      "  n_samples CV   : 11305 (90.0%)\n",
      "  n_samples FINAL: 1256 (10.0%)\n",
      "  CV range   : 0 11304\n",
      "  FINAL range: 11305 12560\n",
      "\n",
      "Walk-forward folds: 4\n",
      "  fold 1: train=5652 | val=1130 | test=1130\n",
      "  fold 2: train=6782 | val=1130 | test=1130\n",
      "  fold 3: train=7912 | val=1130 | test=1130\n",
      "  fold 4: train=9042 | val=1130 | test=1130\n"
     ]
    }
   ],
   "source": [
    "# Step 1e: splits (UNCHANGED / COPIED AS-IS)\n",
    "# ======================================================================\n",
    "\n",
    "def make_final_holdout_split(n_samples_: int, final_test_frac: float) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    if not (0.0 < final_test_frac < 0.5):\n",
    "        raise ValueError(\"final_test_frac should be in (0, 0.5)\")\n",
    "    n_final = max(1, int(round(final_test_frac * n_samples_)))\n",
    "    n_cv = n_samples_ - n_final\n",
    "    if n_cv <= 50:\n",
    "        raise ValueError(\"Too few samples left for CV after holdout split.\")\n",
    "    idx_cv = np.arange(0, n_cv, dtype=np.int64)\n",
    "    idx_final = np.arange(n_cv, n_samples_, dtype=np.int64)\n",
    "    return idx_cv, idx_final\n",
    "\n",
    "\n",
    "def make_walk_forward_splits(\n",
    "    n_samples_: int,\n",
    "    train_min_frac: float,\n",
    "    val_window_frac: float,\n",
    "    test_window_frac: float,\n",
    "    step_window_frac: float,\n",
    ") -> List[Tuple[np.ndarray, np.ndarray, np.ndarray]]:\n",
    "    train_min = int(train_min_frac * n_samples_)\n",
    "    val_w = max(1, int(val_window_frac * n_samples_))\n",
    "    test_w = max(1, int(test_window_frac * n_samples_))\n",
    "    step_w = max(1, int(step_window_frac * n_samples_))\n",
    "\n",
    "    splits = []\n",
    "    start = train_min\n",
    "    while True:\n",
    "        tr_end = start\n",
    "        va_end = tr_end + val_w\n",
    "        te_end = va_end + test_w\n",
    "        if te_end > n_samples_:\n",
    "            break\n",
    "\n",
    "        idx_train = np.arange(0, tr_end, dtype=np.int64)\n",
    "        idx_val = np.arange(tr_end, va_end, dtype=np.int64)\n",
    "        idx_test = np.arange(va_end, te_end, dtype=np.int64)\n",
    "        splits.append((idx_train, idx_val, idx_test))\n",
    "\n",
    "        start += step_w\n",
    "\n",
    "    return splits\n",
    "\n",
    "\n",
    "idx_cv_all, idx_final_test = make_final_holdout_split(n_samples, CFG[\"final_test_frac\"])\n",
    "n_samples_cv = len(idx_cv_all)\n",
    "n_samples_final = len(idx_final_test)\n",
    "\n",
    "print(\"Holdout split:\")\n",
    "print(f\"  n_samples total: {n_samples}\")\n",
    "print(f\"  n_samples CV   : {n_samples_cv} ({100 * n_samples_cv / n_samples:.1f}%)\")\n",
    "print(f\"  n_samples FINAL: {n_samples_final} ({100 * n_samples_final / n_samples:.1f}%)\")\n",
    "print(\"  CV range   :\", int(idx_cv_all[0]), int(idx_cv_all[-1]))\n",
    "print(\"  FINAL range:\", int(idx_final_test[0]), int(idx_final_test[-1]))\n",
    "\n",
    "walk_splits = make_walk_forward_splits(\n",
    "    n_samples_=n_samples_cv,\n",
    "    train_min_frac=CFG[\"train_min_frac\"],\n",
    "    val_window_frac=CFG[\"val_window_frac\"],\n",
    "    test_window_frac=CFG[\"test_window_frac\"],\n",
    "    step_window_frac=CFG[\"step_window_frac\"],\n",
    ")\n",
    "\n",
    "print(\"\\nWalk-forward folds:\", len(walk_splits))\n",
    "for i, (a, b, c) in enumerate(walk_splits, 1):\n",
    "    print(f\"  fold {i}: train={len(a)} | val={len(b)} | test={len(c)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1f: dataset/scaling helpers (UNCHANGED / COPIED AS-IS)\n",
    "# ======================================================================\n",
    "\n",
    "class LobGraphSequenceDataset2Stage(Dataset):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      x_seq: (L,N,F)\n",
    "      e_seq: (L,E,edge_dim)\n",
    "      y_trade: scalar\n",
    "      y_dir: scalar\n",
    "      exit_ret: scalar\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        X_node: np.ndarray,\n",
    "        E_feat: np.ndarray,\n",
    "        y_trade_arr: np.ndarray,\n",
    "        y_dir_arr: np.ndarray,\n",
    "        exit_ret_arr: np.ndarray,\n",
    "        sample_t_: np.ndarray,\n",
    "        indices: np.ndarray,\n",
    "        lookback: int,\n",
    "    ):\n",
    "        self.X_node = X_node\n",
    "        self.E_feat = E_feat\n",
    "        self.y_trade = y_trade_arr\n",
    "        self.y_dir = y_dir_arr\n",
    "        self.exit_ret = exit_ret_arr\n",
    "        self.sample_t = sample_t_\n",
    "        self.indices = indices.astype(np.int64)\n",
    "        self.L = int(lookback)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return int(len(self.indices))\n",
    "\n",
    "    def __getitem__(self, i: int):\n",
    "        sidx = int(self.indices[i])\n",
    "        t = int(self.sample_t[sidx])\n",
    "        t0 = t - self.L + 1\n",
    "\n",
    "        x_seq = self.X_node[t0:t + 1]  # (L,N,F)\n",
    "        e_seq = self.E_feat[t0:t + 1]  # (L,E,D)\n",
    "\n",
    "        yt = int(self.y_trade[t])\n",
    "        yd = int(self.y_dir[t])\n",
    "        er = float(self.exit_ret[t])\n",
    "\n",
    "        return (\n",
    "            torch.from_numpy(x_seq),\n",
    "            torch.from_numpy(e_seq),\n",
    "            torch.tensor(yt, dtype=torch.long),\n",
    "            torch.tensor(yd, dtype=torch.long),\n",
    "            torch.tensor(er, dtype=torch.float32),\n",
    "        )\n",
    "\n",
    "\n",
    "def collate_fn_2stage(batch):\n",
    "    xs, es, yts, yds, ers = zip(*batch)\n",
    "    return (\n",
    "        torch.stack(xs, 0),   # (B,L,N,F)\n",
    "        torch.stack(es, 0),   # (B,L,E,D)\n",
    "        torch.stack(yts, 0),  # (B,)\n",
    "        torch.stack(yds, 0),  # (B,)\n",
    "        torch.stack(ers, 0),  # (B,)\n",
    "    )\n",
    "\n",
    "\n",
    "def fit_scale_nodes_train_only(\n",
    "    X_node_raw_: np.ndarray,\n",
    "    sample_t_: np.ndarray,\n",
    "    idx_train: np.ndarray,\n",
    "    max_abs: float = 10.0\n",
    ") -> Tuple[np.ndarray, RobustScaler]:\n",
    "    last_train_t = int(sample_t_[int(idx_train[-1])])\n",
    "    train_time_mask = np.arange(0, last_train_t + 1)\n",
    "\n",
    "    X_train_time = X_node_raw_[train_time_mask]  # (Ttr,N,F)\n",
    "    _, _, Fdim = X_train_time.shape\n",
    "\n",
    "    scaler = RobustScaler(with_centering=True, with_scaling=True, quantile_range=(5.0, 95.0))\n",
    "    scaler.fit(X_train_time.reshape(-1, Fdim))\n",
    "\n",
    "    X_scaled = scaler.transform(X_node_raw_.reshape(-1, Fdim)).reshape(X_node_raw_.shape).astype(np.float32)\n",
    "    X_scaled = np.clip(X_scaled, -max_abs, max_abs).astype(np.float32)\n",
    "    X_scaled = np.nan_to_num(X_scaled, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n",
    "    return X_scaled, scaler\n",
    "\n",
    "\n",
    "def fit_scale_edges_train_only(\n",
    "    E_raw_: np.ndarray,\n",
    "    sample_t_: np.ndarray,\n",
    "    idx_train: np.ndarray,\n",
    "    max_abs: float = 6.0\n",
    ") -> Tuple[np.ndarray, RobustScaler]:\n",
    "    \"\"\"\n",
    "    Robust-scale edge features per fold (train timeline only).\n",
    "    Fisher-transformed correlations can be heavy-tailed.\n",
    "    \"\"\"\n",
    "    last_train_t = int(sample_t_[int(idx_train[-1])])\n",
    "    train_time_mask = np.arange(0, last_train_t + 1)\n",
    "\n",
    "    E_train_time = E_raw_[train_time_mask]  # (Ttr,E,D)\n",
    "    _, _, D = E_train_time.shape\n",
    "\n",
    "    scaler = RobustScaler(with_centering=True, with_scaling=True, quantile_range=(5.0, 95.0))\n",
    "    scaler.fit(E_train_time.reshape(-1, D))\n",
    "\n",
    "    E_scaled = scaler.transform(E_raw_.reshape(-1, D)).reshape(E_raw_.shape).astype(np.float32)\n",
    "    E_scaled = np.clip(E_scaled, -max_abs, max_abs).astype(np.float32)\n",
    "    E_scaled = np.nan_to_num(E_scaled, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n",
    "    return E_scaled, scaler\n",
    "\n",
    "\n",
    "def subset_trade_indices(indices: np.ndarray, sample_t_: np.ndarray, y_trade_arr: np.ndarray) -> np.ndarray:\n",
    "    tt = sample_t_[indices]\n",
    "    mask = (y_trade_arr[tt] == 1)\n",
    "    return indices[mask]\n",
    "\n",
    "\n",
    "def split_trade_ratio(indices: np.ndarray, sample_t_: np.ndarray, y_trade_arr: np.ndarray) -> float:\n",
    "    tt = sample_t_[indices]\n",
    "    return float(y_trade_arr[tt].mean()) if len(tt) else float(\"nan\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-class mapping: {0: 'SHORT', 1: 'FLAT', 2: 'LONG'}\n",
      "Checkpoint dir: /Users/vitalii/Desktop/Model_Market_Microstructure/Graph_Neural_Network_for_Market_Microstructure/TGNN2026/checkpoints_gwnet_3class\n"
     ]
    }
   ],
   "source": [
    "# Step 0b: Graph WaveNet config additions (NEW)\n",
    "# ======================================================================\n",
    "\n",
    "CFG.update({\n",
    "    # Graph WaveNet core channels\n",
    "    \"gwn_residual_channels\": 64,\n",
    "    \"gwn_dilation_channels\": 64,\n",
    "    \"gwn_skip_channels\": 128,\n",
    "    \"gwn_end_channels\": 128,\n",
    "\n",
    "    # blocks/layers: dilations reset each block\n",
    "    \"gwn_blocks\": 3,\n",
    "    \"gwn_layers_per_block\": 2,\n",
    "    \"gwn_kernel_size\": 2,\n",
    "\n",
    "    # adaptive adjacency\n",
    "    \"adaptive_topk\": 3,  # for 3 nodes, 3 keeps all; keep as knob for generalization\n",
    "\n",
    "    # optional PnL-ish regularization during training\n",
    "    \"trade_prob_penalty\": 0.01,  # penalize over-trading via mean(p_short+p_long)\n",
    "Ðš\n",
    "    # checkpointing\n",
    "    \"ckpt_dir\": Path(\"./checkpoints_MTGNN1m_auc_3class\"),\n",
    "    \"sel_metric_dir_weight\": 0.50,  # selection metric: trade_auc + w * dir_auc\n",
    "})\n",
    "\n",
    "CFG[\"ckpt_dir\"].mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CLASS_NAMES = [\"SHORT\", \"FLAT\", \"LONG\"]\n",
    "print(\"3-class mapping:\", {0: \"SHORT\", 1: \"FLAT\", 2: \"LONG\"})\n",
    "print(\"Checkpoint dir:\", str(CFG[\"ckpt_dir\"].resolve()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: dataset/dataloader (3-class)  (NEW)\n",
    "# ======================================================================\n",
    "\n",
    "class LobGraphSequenceDataset3Class(Dataset):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      x_seq:    (L,N,F)\n",
    "      e_seq:    (L,E,D)\n",
    "      y_tb:     scalar in {0,1,2} (SHORT, FLAT, LONG)\n",
    "      exit_ret: scalar (log-return to exit)\n",
    "      sidx:     scalar sample index (for time-order reconstruction)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        X_node: np.ndarray,\n",
    "        E_feat: np.ndarray,\n",
    "        y_tb_arr: np.ndarray,\n",
    "        exit_ret_arr: np.ndarray,\n",
    "        sample_t_: np.ndarray,\n",
    "        indices: np.ndarray,\n",
    "        lookback: int,\n",
    "    ):\n",
    "        self.X_node = X_node\n",
    "        self.E_feat = E_feat\n",
    "        self.y_tb = y_tb_arr\n",
    "        self.exit_ret = exit_ret_arr\n",
    "        self.sample_t = sample_t_\n",
    "        self.indices = indices.astype(np.int64)\n",
    "        self.L = int(lookback)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return int(len(self.indices))\n",
    "\n",
    "    def __getitem__(self, i: int):\n",
    "        sidx = int(self.indices[i])\n",
    "        t = int(self.sample_t[sidx])\n",
    "        t0 = t - self.L + 1\n",
    "\n",
    "        x_seq = self.X_node[t0:t + 1]  # (L,N,F)\n",
    "        e_seq = self.E_feat[t0:t + 1]  # (L,E,D)\n",
    "        y = int(self.y_tb[t])\n",
    "        er = float(self.exit_ret[t])\n",
    "\n",
    "        return (\n",
    "            torch.from_numpy(x_seq),\n",
    "            torch.from_numpy(e_seq),\n",
    "            torch.tensor(y, dtype=torch.long),\n",
    "            torch.tensor(er, dtype=torch.float32),\n",
    "            torch.tensor(sidx, dtype=torch.long),\n",
    "        )\n",
    "\n",
    "\n",
    "def collate_fn_3class(batch):\n",
    "    xs, es, ys, ers, sidxs = zip(*batch)\n",
    "    return (\n",
    "        torch.stack(xs, 0),    # (B,L,N,F)\n",
    "        torch.stack(es, 0),    # (B,L,E,D)\n",
    "        torch.stack(ys, 0),    # (B,)\n",
    "        torch.stack(ers, 0),   # (B,)\n",
    "        torch.stack(sidxs, 0), # (B,)\n",
    "    )\n",
    "\n",
    "\n",
    "def make_ce_weights_3class(y_np: np.ndarray) -> torch.Tensor:\n",
    "    y_np = np.asarray(y_np, dtype=np.int64)\n",
    "    counts = np.bincount(y_np, minlength=3).astype(np.float64)\n",
    "    counts = np.maximum(counts, 1.0)\n",
    "    w = counts.sum() / (3.0 * counts)\n",
    "    return torch.tensor(w, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "\n",
    "def make_weighted_sampler_3class(y_np: np.ndarray) -> WeightedRandomSampler:\n",
    "    y_np = np.asarray(y_np, dtype=np.int64)\n",
    "    counts = np.bincount(y_np, minlength=3).astype(np.float64)\n",
    "    counts = np.maximum(counts, 1.0)\n",
    "    class_w = counts.sum() / (3.0 * counts)\n",
    "    sample_w = class_w[y_np].astype(np.float64)\n",
    "    sample_w = torch.tensor(sample_w, dtype=torch.double)\n",
    "    return WeightedRandomSampler(weights=sample_w, num_samples=len(sample_w), replacement=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: model definition (Graph WaveNet / MTGNN-style)  (NEW)\n",
    "# ======================================================================\n",
    "\n",
    "def build_static_adjacency_from_edges(edge_index: torch.Tensor, n_nodes: int, eps: float = 1e-8) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Build A_static (N,N), row-normalized, using the presence of edges in EDGE_LIST.\n",
    "    \"\"\"\n",
    "    A = torch.zeros((n_nodes, n_nodes), dtype=torch.float32)\n",
    "    src = edge_index[:, 0].long()\n",
    "    dst = edge_index[:, 1].long()\n",
    "    A[src, dst] = 1.0\n",
    "    A = A / (A.sum(dim=-1, keepdim=True) + eps)\n",
    "    return A\n",
    "\n",
    "\n",
    "def build_adj_prior_from_edge_attr(\n",
    "    edge_attr_last: torch.Tensor,    # (B,E,D)\n",
    "    edge_index: torch.Tensor,        # (E,2) [src,dst]\n",
    "    n_nodes: int,\n",
    "    use_abs: bool = False,\n",
    "    diag_boost: float = 1.0,\n",
    "    row_normalize: bool = True,\n",
    "    eps: float = 1e-8\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Build A_prior (B,N,N) from edge_attr at the last timestep.\n",
    "      w = sigmoid(mean(edge_attr)) in [0,1]\n",
    "    Fill A[src,dst] = w, enforce diag >= diag_boost, row-normalize.\n",
    "    \"\"\"\n",
    "    edge_attr_last = torch.nan_to_num(edge_attr_last, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    B, E, D = edge_attr_last.shape\n",
    "    r = edge_attr_last.mean(dim=-1)  # (B,E)\n",
    "    if use_abs:\n",
    "        r = r.abs()\n",
    "    w = torch.sigmoid(r)  # (B,E)\n",
    "\n",
    "    A = torch.zeros((B, n_nodes, n_nodes), device=edge_attr_last.device, dtype=edge_attr_last.dtype)\n",
    "    src = edge_index[:, 0].to(edge_attr_last.device)\n",
    "    dst = edge_index[:, 1].to(edge_attr_last.device)\n",
    "    A[:, src, dst] = w\n",
    "\n",
    "    diag = torch.arange(n_nodes, device=edge_attr_last.device)\n",
    "    A[:, diag, diag] = torch.maximum(A[:, diag, diag], torch.full_like(A[:, diag, diag], float(diag_boost)))\n",
    "\n",
    "    if row_normalize:\n",
    "        A = A / (A.sum(dim=-1, keepdim=True) + eps)\n",
    "\n",
    "    return torch.nan_to_num(A, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "\n",
    "class AdaptiveAdjacency(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph WaveNet-style adaptive adjacency from node embeddings:\n",
    "      logits = relu(E1 @ E2^T) / temp\n",
    "      (optional) top-k per row\n",
    "      A_adapt = softmax(logits_row)\n",
    "    Also returns sparsity_proxy = sigmoid(logits) for L1(offdiag).\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes: int, cfg: Dict[str, Any]):\n",
    "        super().__init__()\n",
    "        self.n = int(n_nodes)\n",
    "        k = int(cfg.get(\"adj_emb_dim\", 8))\n",
    "        self.E1 = nn.Parameter(0.01 * torch.randn(self.n, k))\n",
    "        self.E2 = nn.Parameter(0.01 * torch.randn(self.n, k))\n",
    "        self.temp = float(cfg.get(\"adj_temperature\", 1.0))\n",
    "        self.temp = max(self.temp, 1e-3)\n",
    "        self.topk = int(cfg.get(\"adaptive_topk\", self.n))\n",
    "\n",
    "    def forward(self) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        logits = (self.E1 @ self.E2.t())\n",
    "        logits = F.relu(logits) / self.temp  # (N,N)\n",
    "        sparsity_proxy = torch.sigmoid(logits)\n",
    "\n",
    "        if self.topk is not None and 0 < self.topk < self.n:\n",
    "            vals, idx = torch.topk(logits, k=self.topk, dim=-1)\n",
    "            mask = torch.full_like(logits, fill_value=float(\"-inf\"))\n",
    "            mask.scatter_(-1, idx, vals)\n",
    "            logits = mask\n",
    "\n",
    "        A = torch.softmax(logits, dim=-1)  # row-stochastic\n",
    "        return A, sparsity_proxy, logits\n",
    "\n",
    "\n",
    "class LearnableSupportMix(nn.Module):\n",
    "    \"\"\"\n",
    "    Blend supports (static, prior, adapt) using softmax weights.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_supports: int = 3):\n",
    "        super().__init__()\n",
    "        self.w_logits = nn.Parameter(torch.zeros(n_supports, dtype=torch.float32))\n",
    "\n",
    "    def forward(self) -> torch.Tensor:\n",
    "        return torch.softmax(self.w_logits, dim=0)\n",
    "\n",
    "\n",
    "class CausalConv2dTime(nn.Module):\n",
    "    \"\"\"\n",
    "    2D convolution causal along time dimension only.\n",
    "    Input:  (B,C,N,T)\n",
    "    Conv kernel: (1,k), dilation: (1,d)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch: int, out_ch: int, kernel_size: int, dilation: int):\n",
    "        super().__init__()\n",
    "        self.k = int(kernel_size)\n",
    "        self.d = int(dilation)\n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, kernel_size=(1, self.k), dilation=(1, self.d))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        pad_left = (self.k - 1) * self.d\n",
    "        x = F.pad(x, (pad_left, 0, 0, 0))  # pad time (W) on the left\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "def graph_message_passing(x: torch.Tensor, A: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    x: (B,C,N,T)\n",
    "    A: (B,N,N) with A[src,dst]\n",
    "    returns: (B,C,N,T) where dst aggregates from src\n",
    "    \"\"\"\n",
    "    return torch.einsum(\"bcnt,bnm->bcmt\", x, A)\n",
    "\n",
    "\n",
    "class GraphWaveNetBlock(nn.Module):\n",
    "    def __init__(self, residual_ch: int, dilation_ch: int, skip_ch: int, kernel_size: int, dilation: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.filter_conv = CausalConv2dTime(residual_ch, dilation_ch, kernel_size=kernel_size, dilation=dilation)\n",
    "        self.gate_conv = CausalConv2dTime(residual_ch, dilation_ch, kernel_size=kernel_size, dilation=dilation)\n",
    "\n",
    "        self.residual_conv = nn.Conv2d(dilation_ch, residual_ch, kernel_size=(1, 1))\n",
    "        self.skip_conv = nn.Conv2d(dilation_ch, skip_ch, kernel_size=(1, 1))\n",
    "\n",
    "        self.dropout = nn.Dropout(float(dropout))\n",
    "        self.bn = nn.BatchNorm2d(residual_ch)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, A: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        x: (B,residual_ch,N,T)\n",
    "        A: (B,N,N)\n",
    "        \"\"\"\n",
    "        x = torch.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        A = torch.nan_to_num(A, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "        residual = x\n",
    "\n",
    "        f = torch.tanh(self.filter_conv(x))\n",
    "        g = torch.sigmoid(self.gate_conv(x))\n",
    "        z = f * g  # (B,dilation_ch,N,T)\n",
    "\n",
    "        z = self.dropout(z)\n",
    "\n",
    "        skip = self.skip_conv(z)  # (B,skip_ch,N,T)\n",
    "\n",
    "        out = self.residual_conv(z)  # (B,residual_ch,N,T)\n",
    "        out = graph_message_passing(out, A)  # spatial mixing\n",
    "        out = out + residual  # residual\n",
    "        out = self.bn(out)\n",
    "\n",
    "        out = torch.nan_to_num(out, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        skip = torch.nan_to_num(skip, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        return out, skip\n",
    "\n",
    "\n",
    "class GraphWaveNet3Class(nn.Module):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "      x_seq: (B,L,N,F)\n",
    "      e_seq: (B,L,E,D)   (only used to build A_prior from last step)\n",
    "\n",
    "    Output:\n",
    "      logits_eth: (B,3) for ETH node only: [SHORT, FLAT, LONG]\n",
    "    \"\"\"\n",
    "    def __init__(self, node_in: int, edge_dim: int, cfg: Dict[str, Any], n_nodes: int, target_node: int):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.n_nodes = int(n_nodes)\n",
    "        self.target_node = int(target_node)\n",
    "\n",
    "        residual_ch = int(cfg[\"gwn_residual_channels\"])\n",
    "        dilation_ch = int(cfg[\"gwn_dilation_channels\"])\n",
    "        skip_ch = int(cfg[\"gwn_skip_channels\"])\n",
    "        end_ch = int(cfg[\"gwn_end_channels\"])\n",
    "        k = int(cfg[\"gwn_kernel_size\"])\n",
    "        blocks = int(cfg[\"gwn_blocks\"])\n",
    "        layers_per_block = int(cfg[\"gwn_layers_per_block\"])\n",
    "        drop = float(cfg.get(\"dropout\", 0.0))\n",
    "\n",
    "        self.in_proj = nn.Linear(int(node_in), residual_ch)\n",
    "\n",
    "        # supports\n",
    "        A_static = build_static_adjacency_from_edges(EDGE_INDEX, n_nodes=self.n_nodes)\n",
    "        self.register_buffer(\"A_static\", A_static)\n",
    "\n",
    "        self.adapt = AdaptiveAdjacency(n_nodes=self.n_nodes, cfg=cfg)\n",
    "        self.support_mix = LearnableSupportMix(n_supports=3)\n",
    "\n",
    "        # blocks with dilation schedule resetting each block\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for b in range(blocks):\n",
    "            for l in range(layers_per_block):\n",
    "                dilation = 2 ** l\n",
    "                self.blocks.append(GraphWaveNetBlock(\n",
    "                    residual_ch=residual_ch,\n",
    "                    dilation_ch=dilation_ch,\n",
    "                    skip_ch=skip_ch,\n",
    "                    kernel_size=k,\n",
    "                    dilation=dilation,\n",
    "                    dropout=drop,\n",
    "                ))\n",
    "\n",
    "        self.end1 = nn.Conv2d(skip_ch, end_ch, kernel_size=(1, 1))\n",
    "        self.end2 = nn.Conv2d(end_ch, 3, kernel_size=(1, 1))  # 3-class\n",
    "\n",
    "        # init\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def _compute_supports(self, e_seq: torch.Tensor) -> Tuple[torch.Tensor, Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Build A_prior (batch), A_adapt (global), then mix:\n",
    "          A_mix = w0*A_static + w1*A_prior + w2*A_adapt\n",
    "        \"\"\"\n",
    "        B, L_, E, D = e_seq.shape\n",
    "        e_last = e_seq[:, -1, :, :]  # (B,E,D)\n",
    "\n",
    "        A_prior = build_adj_prior_from_edge_attr(\n",
    "            edge_attr_last=e_last,\n",
    "            edge_index=EDGE_INDEX.to(e_seq.device),\n",
    "            n_nodes=self.n_nodes,\n",
    "            use_abs=bool(self.cfg.get(\"prior_use_abs\", False)),\n",
    "            diag_boost=float(self.cfg.get(\"prior_diag_boost\", 1.0)),\n",
    "            row_normalize=bool(self.cfg.get(\"prior_row_normalize\", True)),\n",
    "        )  # (B,N,N)\n",
    "\n",
    "        A_adapt_base, sparsity_proxy, adapt_logits = self.adapt()  # (N,N)\n",
    "        A_adapt = A_adapt_base.unsqueeze(0).expand(B, -1, -1)      # (B,N,N)\n",
    "\n",
    "        w = self.support_mix()  # (3,)\n",
    "        A_static = self.A_static.to(e_seq.device).to(e_seq.dtype).unsqueeze(0).expand(B, -1, -1)\n",
    "\n",
    "        A_mix = w[0] * A_static + w[1] * A_prior + w[2] * A_adapt\n",
    "        A_mix = A_mix / (A_mix.sum(dim=-1, keepdim=True) + 1e-8)\n",
    "\n",
    "        # regularization terms (adapt only)\n",
    "        N = self.n_nodes\n",
    "        offdiag = (1.0 - torch.eye(N, device=e_seq.device, dtype=e_seq.dtype))\n",
    "        l1_off = (sparsity_proxy.to(e_seq.dtype) * offdiag).abs().mean()\n",
    "        mse_prior = ((A_adapt - A_prior) ** 2 * offdiag).mean()\n",
    "\n",
    "        aux = {\n",
    "            \"support_w\": w.detach().cpu().numpy().tolist(),\n",
    "            \"l1_off\": float(l1_off.detach().cpu().item()),\n",
    "            \"mse_prior\": float(mse_prior.detach().cpu().item()),\n",
    "            \"_l1_off_t\": l1_off,\n",
    "            \"_mse_prior_t\": mse_prior,\n",
    "        }\n",
    "        return A_mix, aux\n",
    "\n",
    "    def forward(self, x_seq: torch.Tensor, e_seq: torch.Tensor, return_aux: bool = False):\n",
    "        x_seq = torch.nan_to_num(x_seq, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        e_seq = torch.nan_to_num(e_seq, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "        B, L_, N, Fdim = x_seq.shape\n",
    "        assert N == self.n_nodes\n",
    "\n",
    "        # (B,L,N,F) -> (B,N,L,residual_ch) -> (B,residual_ch,N,L)\n",
    "        x = self.in_proj(x_seq)              # (B,L,N,C)\n",
    "        x = x.permute(0, 3, 2, 1).contiguous()  # (B,C,N,T)\n",
    "\n",
    "        A_mix, aux = self._compute_supports(e_seq)\n",
    "\n",
    "        skip_sum = None\n",
    "        for blk in self.blocks:\n",
    "            x, skip = blk(x, A_mix)\n",
    "            skip_sum = skip if skip_sum is None else (skip_sum + skip)\n",
    "\n",
    "        y = F.relu(skip_sum)\n",
    "        y = F.relu(self.end1(y))\n",
    "        y = self.end2(y)  # (B,3,N,T)\n",
    "\n",
    "        logits_eth = y[:, :, self.target_node, -1]  # (B,3)\n",
    "        logits_eth = torch.nan_to_num(logits_eth, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "        if return_aux:\n",
    "            return logits_eth, aux\n",
    "        return logits_eth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FOLD 1/4 sizes: train=5652 val=1130 test=1130\n",
      "True trade ratio (val):  0.365\n",
      "True trade ratio (test): 0.304\n",
      "[fold 01] ep 01 lr=9.84e-05 tr_loss=1.1131 val_trade_auc=0.476 val_dir_auc=0.509 sel=0.730 best=0.730@ep01 supports=[0.333, 0.333, 0.333]\n",
      "[fold 01] ep 02 lr=2.34e-04 tr_loss=0.9962 val_trade_auc=0.473 val_dir_auc=0.529 sel=0.737 best=0.737@ep02 supports=[0.334, 0.333, 0.334]\n",
      "[fold 01] ep 03 lr=3.00e-04 tr_loss=0.9589 val_trade_auc=0.476 val_dir_auc=0.519 sel=0.735 best=0.737@ep02 supports=[0.334, 0.333, 0.334]\n",
      "[fold 01] ep 04 lr=2.97e-04 tr_loss=0.9220 val_trade_auc=0.485 val_dir_auc=0.534 sel=0.751 best=0.751@ep04 supports=[0.335, 0.33, 0.335]\n",
      "[fold 01] ep 05 lr=2.90e-04 tr_loss=0.8931 val_trade_auc=0.493 val_dir_auc=0.580 sel=0.783 best=0.783@ep05 supports=[0.337, 0.326, 0.337]\n",
      "[fold 01] ep 06 lr=2.77e-04 tr_loss=0.8848 val_trade_auc=0.489 val_dir_auc=0.591 sel=0.785 best=0.785@ep06 supports=[0.339, 0.321, 0.339]\n",
      "[fold 01] ep 07 lr=2.61e-04 tr_loss=0.8391 val_trade_auc=0.500 val_dir_auc=0.577 sel=0.788 best=0.788@ep07 supports=[0.34, 0.319, 0.34]\n",
      "[fold 01] ep 08 lr=2.40e-04 tr_loss=0.7968 val_trade_auc=0.515 val_dir_auc=0.576 sel=0.803 best=0.803@ep08 supports=[0.343, 0.315, 0.343]\n",
      "[fold 01] ep 09 lr=2.16e-04 tr_loss=0.7661 val_trade_auc=0.501 val_dir_auc=0.574 sel=0.788 best=0.803@ep08 supports=[0.345, 0.311, 0.345]\n",
      "[fold 01] ep 10 lr=1.91e-04 tr_loss=0.7173 val_trade_auc=0.507 val_dir_auc=0.593 sel=0.804 best=0.804@ep10 supports=[0.346, 0.307, 0.347]\n",
      "[fold 01] ep 11 lr=1.64e-04 tr_loss=0.6982 val_trade_auc=0.512 val_dir_auc=0.592 sel=0.808 best=0.808@ep11 supports=[0.347, 0.306, 0.348]\n",
      "[fold 01] ep 12 lr=1.36e-04 tr_loss=0.6670 val_trade_auc=0.509 val_dir_auc=0.570 sel=0.794 best=0.808@ep11 supports=[0.348, 0.303, 0.349]\n",
      "[fold 01] ep 13 lr=1.09e-04 tr_loss=0.6313 val_trade_auc=0.512 val_dir_auc=0.559 sel=0.791 best=0.808@ep11 supports=[0.348, 0.302, 0.35]\n",
      "[fold 01] ep 14 lr=8.30e-05 tr_loss=0.6117 val_trade_auc=0.516 val_dir_auc=0.562 sel=0.797 best=0.808@ep11 supports=[0.348, 0.3, 0.351]\n",
      "[fold 01] ep 15 lr=5.96e-05 tr_loss=0.6141 val_trade_auc=0.508 val_dir_auc=0.558 sel=0.787 best=0.808@ep11 supports=[0.348, 0.299, 0.352]\n",
      "[fold 01] ep 16 lr=3.93e-05 tr_loss=0.5898 val_trade_auc=0.511 val_dir_auc=0.555 sel=0.788 best=0.808@ep11 supports=[0.349, 0.299, 0.353]\n",
      "[fold 01] ep 17 lr=2.27e-05 tr_loss=0.5791 val_trade_auc=0.513 val_dir_auc=0.553 sel=0.790 best=0.808@ep11 supports=[0.349, 0.299, 0.353]\n",
      "[fold 01] ep 18 lr=1.05e-05 tr_loss=0.5668 val_trade_auc=0.514 val_dir_auc=0.556 sel=0.792 best=0.808@ep11 supports=[0.349, 0.299, 0.353]\n",
      "[fold 01] chosen thresholds on VAL: thr_trade=0.932 thr_dir=0.500 | val pnl_sum=0.0307 val trade_rate=0.044\n",
      "[fold 01] TEST (fixed thresholds from VAL): trade_auc=0.618 dir_auc=0.580 pnl_sum=0.0147 trade_rate=0.077 trades=87\n",
      "Saved fold checkpoint: checkpoints_gwnet_3class/fold_01_best.pt\n",
      "\n",
      "================================================================================\n",
      "FOLD 2/4 sizes: train=6782 val=1130 test=1130\n",
      "True trade ratio (val):  0.304\n",
      "True trade ratio (test): 0.463\n",
      "[fold 02] ep 01 lr=9.83e-05 tr_loss=1.0521 val_trade_auc=0.473 val_dir_auc=0.466 sel=0.706 best=0.706@ep01 supports=[0.333, 0.334, 0.333]\n",
      "[fold 02] ep 02 lr=2.34e-04 tr_loss=0.9848 val_trade_auc=0.550 val_dir_auc=0.549 sel=0.824 best=0.824@ep02 supports=[0.332, 0.335, 0.332]\n",
      "[fold 02] ep 03 lr=3.00e-04 tr_loss=0.9619 val_trade_auc=0.576 val_dir_auc=0.576 sel=0.864 best=0.864@ep03 supports=[0.331, 0.337, 0.331]\n",
      "[fold 02] ep 04 lr=2.97e-04 tr_loss=0.9303 val_trade_auc=0.604 val_dir_auc=0.582 sel=0.895 best=0.895@ep04 supports=[0.332, 0.336, 0.332]\n",
      "[fold 02] ep 05 lr=2.90e-04 tr_loss=0.9093 val_trade_auc=0.613 val_dir_auc=0.566 sel=0.896 best=0.896@ep05 supports=[0.334, 0.331, 0.335]\n",
      "[fold 02] ep 06 lr=2.77e-04 tr_loss=0.8606 val_trade_auc=0.608 val_dir_auc=0.560 sel=0.888 best=0.896@ep05 supports=[0.336, 0.327, 0.337]\n",
      "[fold 02] ep 07 lr=2.61e-04 tr_loss=0.8159 val_trade_auc=0.591 val_dir_auc=0.513 sel=0.848 best=0.896@ep05 supports=[0.337, 0.322, 0.34]\n",
      "[fold 02] ep 08 lr=2.40e-04 tr_loss=0.7715 val_trade_auc=0.572 val_dir_auc=0.547 sel=0.846 best=0.896@ep05 supports=[0.339, 0.317, 0.344]\n",
      "[fold 02] ep 09 lr=2.17e-04 tr_loss=0.7244 val_trade_auc=0.561 val_dir_auc=0.528 sel=0.825 best=0.896@ep05 supports=[0.339, 0.313, 0.348]\n",
      "[fold 02] ep 10 lr=1.91e-04 tr_loss=0.6892 val_trade_auc=0.569 val_dir_auc=0.514 sel=0.826 best=0.896@ep05 supports=[0.339, 0.31, 0.352]\n",
      "[fold 02] ep 11 lr=1.64e-04 tr_loss=0.6598 val_trade_auc=0.553 val_dir_auc=0.523 sel=0.815 best=0.896@ep05 supports=[0.338, 0.307, 0.355]\n",
      "[fold 02] ep 12 lr=1.36e-04 tr_loss=0.6290 val_trade_auc=0.558 val_dir_auc=0.522 sel=0.819 best=0.896@ep05 supports=[0.337, 0.304, 0.359]\n",
      "[fold 02] chosen thresholds on VAL: thr_trade=0.907 thr_dir=0.500 | val pnl_sum=0.1385 val trade_rate=0.088\n",
      "[fold 02] TEST (fixed thresholds from VAL): trade_auc=0.525 dir_auc=0.506 pnl_sum=0.2543 trade_rate=0.216 trades=244\n",
      "Saved fold checkpoint: checkpoints_gwnet_3class/fold_02_best.pt\n",
      "\n",
      "================================================================================\n",
      "FOLD 3/4 sizes: train=7912 val=1130 test=1130\n",
      "True trade ratio (val):  0.463\n",
      "True trade ratio (test): 0.558\n",
      "[fold 03] ep 01 lr=9.82e-05 tr_loss=1.0721 val_trade_auc=0.523 val_dir_auc=0.519 sel=0.782 best=0.782@ep01 supports=[0.333, 0.334, 0.333]\n",
      "[fold 03] ep 02 lr=2.34e-04 tr_loss=0.9853 val_trade_auc=0.523 val_dir_auc=0.492 sel=0.769 best=0.782@ep01 supports=[0.332, 0.336, 0.332]\n",
      "[fold 03] ep 03 lr=3.00e-04 tr_loss=0.9511 val_trade_auc=0.521 val_dir_auc=0.500 sel=0.771 best=0.782@ep01 supports=[0.332, 0.336, 0.332]\n",
      "[fold 03] ep 04 lr=2.97e-04 tr_loss=0.9315 val_trade_auc=0.516 val_dir_auc=0.503 sel=0.767 best=0.782@ep01 supports=[0.333, 0.333, 0.333]\n",
      "[fold 03] ep 05 lr=2.90e-04 tr_loss=0.9056 val_trade_auc=0.511 val_dir_auc=0.502 sel=0.763 best=0.782@ep01 supports=[0.336, 0.328, 0.336]\n",
      "[fold 03] ep 06 lr=2.77e-04 tr_loss=0.8615 val_trade_auc=0.504 val_dir_auc=0.483 sel=0.745 best=0.782@ep01 supports=[0.337, 0.325, 0.338]\n",
      "[fold 03] ep 07 lr=2.61e-04 tr_loss=0.8406 val_trade_auc=0.498 val_dir_auc=0.485 sel=0.741 best=0.782@ep01 supports=[0.339, 0.319, 0.341]\n",
      "[fold 03] ep 08 lr=2.40e-04 tr_loss=0.8071 val_trade_auc=0.492 val_dir_auc=0.456 sel=0.720 best=0.782@ep01 supports=[0.339, 0.316, 0.345]\n",
      "[fold 03] chosen thresholds on VAL: thr_trade=0.700 thr_dir=0.550 | val pnl_sum=0.4258 val trade_rate=0.604\n",
      "[fold 03] TEST (fixed thresholds from VAL): trade_auc=0.556 dir_auc=0.466 pnl_sum=0.0488 trade_rate=0.633 trades=715\n",
      "Saved fold checkpoint: checkpoints_gwnet_3class/fold_03_best.pt\n",
      "\n",
      "================================================================================\n",
      "FOLD 4/4 sizes: train=9042 val=1130 test=1130\n",
      "True trade ratio (val):  0.558\n",
      "True trade ratio (test): 0.506\n",
      "[fold 04] ep 01 lr=9.81e-05 tr_loss=1.0553 val_trade_auc=0.535 val_dir_auc=0.523 sel=0.797 best=0.797@ep01 supports=[0.333, 0.333, 0.333]\n",
      "[fold 04] ep 02 lr=2.34e-04 tr_loss=0.9904 val_trade_auc=0.612 val_dir_auc=0.489 sel=0.856 best=0.856@ep02 supports=[0.333, 0.334, 0.333]\n",
      "[fold 04] ep 03 lr=3.00e-04 tr_loss=0.9695 val_trade_auc=0.655 val_dir_auc=0.483 sel=0.897 best=0.897@ep03 supports=[0.334, 0.332, 0.334]\n",
      "[fold 04] ep 04 lr=2.97e-04 tr_loss=0.9368 val_trade_auc=0.672 val_dir_auc=0.472 sel=0.908 best=0.908@ep04 supports=[0.335, 0.33, 0.335]\n",
      "[fold 04] ep 05 lr=2.90e-04 tr_loss=0.9153 val_trade_auc=0.670 val_dir_auc=0.455 sel=0.897 best=0.908@ep04 supports=[0.336, 0.327, 0.337]\n",
      "[fold 04] ep 06 lr=2.77e-04 tr_loss=0.8842 val_trade_auc=0.670 val_dir_auc=0.457 sel=0.899 best=0.908@ep04 supports=[0.336, 0.325, 0.338]\n",
      "[fold 04] ep 07 lr=2.61e-04 tr_loss=0.8405 val_trade_auc=0.670 val_dir_auc=0.440 sel=0.889 best=0.908@ep04 supports=[0.336, 0.322, 0.342]\n",
      "[fold 04] ep 08 lr=2.40e-04 tr_loss=0.8062 val_trade_auc=0.661 val_dir_auc=0.447 sel=0.884 best=0.908@ep04 supports=[0.335, 0.318, 0.346]\n",
      "[fold 04] ep 09 lr=2.17e-04 tr_loss=0.7626 val_trade_auc=0.667 val_dir_auc=0.455 sel=0.895 best=0.908@ep04 supports=[0.334, 0.315, 0.352]\n",
      "[fold 04] ep 10 lr=1.91e-04 tr_loss=0.7126 val_trade_auc=0.668 val_dir_auc=0.489 sel=0.912 best=0.912@ep10 supports=[0.332, 0.311, 0.357]\n",
      "[fold 04] ep 11 lr=1.64e-04 tr_loss=0.6779 val_trade_auc=0.660 val_dir_auc=0.471 sel=0.896 best=0.912@ep10 supports=[0.329, 0.31, 0.361]\n",
      "[fold 04] ep 12 lr=1.36e-04 tr_loss=0.6500 val_trade_auc=0.662 val_dir_auc=0.474 sel=0.899 best=0.912@ep10 supports=[0.327, 0.309, 0.364]\n",
      "[fold 04] ep 13 lr=1.09e-04 tr_loss=0.6257 val_trade_auc=0.663 val_dir_auc=0.476 sel=0.901 best=0.912@ep10 supports=[0.326, 0.308, 0.367]\n",
      "[fold 04] ep 14 lr=8.32e-05 tr_loss=0.5963 val_trade_auc=0.664 val_dir_auc=0.480 sel=0.904 best=0.912@ep10 supports=[0.324, 0.307, 0.369]\n",
      "[fold 04] ep 15 lr=5.98e-05 tr_loss=0.5757 val_trade_auc=0.664 val_dir_auc=0.490 sel=0.909 best=0.912@ep10 supports=[0.323, 0.306, 0.371]\n",
      "[fold 04] ep 16 lr=3.94e-05 tr_loss=0.5775 val_trade_auc=0.661 val_dir_auc=0.489 sel=0.905 best=0.912@ep10 supports=[0.323, 0.305, 0.372]\n",
      "[fold 04] ep 17 lr=2.28e-05 tr_loss=0.5682 val_trade_auc=0.660 val_dir_auc=0.484 sel=0.902 best=0.912@ep10 supports=[0.323, 0.305, 0.373]\n",
      "[fold 04] chosen thresholds on VAL: thr_trade=0.700 thr_dir=0.700 | val pnl_sum=0.0554 val trade_rate=0.609\n",
      "[fold 04] TEST (fixed thresholds from VAL): trade_auc=0.610 dir_auc=0.493 pnl_sum=0.0518 trade_rate=0.573 trades=647\n",
      "Saved fold checkpoint: checkpoints_gwnet_3class/fold_04_best.pt\n"
     ]
    },
    {
     "ename": "UnpicklingError",
     "evalue": "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL pathlib.PosixPath was not an allowed global by default. Please use `torch.serialization.add_safe_globals([pathlib.PosixPath])` or the `torch.serialization.safe_globals([pathlib.PosixPath])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnpicklingError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 544\u001b[39m\n\u001b[32m    540\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mSaved overall best checkpoint:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mstr\u001b[39m(overall_copy))\n\u001b[32m    541\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cv_summary, fold_artifacts, overall_copy\n\u001b[32m--> \u001b[39m\u001b[32m544\u001b[39m cv_summary_3c, fold_artifacts_3c, overall_best_ckpt = \u001b[43mrun_walk_forward_cv_3class\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    546\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m80\u001b[39m)\n\u001b[32m    547\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCV summary (3-class model; TEST uses thresholds selected on VAL):\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 539\u001b[39m, in \u001b[36mrun_walk_forward_cv_3class\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m best_overall_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    538\u001b[39m overall_copy = CFG[\u001b[33m\"\u001b[39m\u001b[33mckpt_dir\u001b[39m\u001b[33m\"\u001b[39m] / \u001b[33m\"\u001b[39m\u001b[33moverall_best.pt\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m539\u001b[39m save_checkpoint(overall_copy, {**\u001b[43mload_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_overall_path\u001b[49m\u001b[43m)\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33mkind\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33moverall_best\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33msource_ckpt\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(best_overall_path)})\n\u001b[32m    540\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mSaved overall best checkpoint:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mstr\u001b[39m(overall_copy))\n\u001b[32m    541\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cv_summary, fold_artifacts, overall_copy\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 461\u001b[39m, in \u001b[36mload_checkpoint\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m    460\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_checkpoint\u001b[39m(path: Path) -> Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[32m--> \u001b[39m\u001b[32m461\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/recbole_new_env/lib/python3.11/site-packages/torch/serialization.py:1524\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1516\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[32m   1517\u001b[39m                     opened_zipfile,\n\u001b[32m   1518\u001b[39m                     map_location,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1521\u001b[39m                     **pickle_load_args,\n\u001b[32m   1522\u001b[39m                 )\n\u001b[32m   1523\u001b[39m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle.UnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m1524\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle.UnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1525\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[32m   1526\u001b[39m             opened_zipfile,\n\u001b[32m   1527\u001b[39m             map_location,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1530\u001b[39m             **pickle_load_args,\n\u001b[32m   1531\u001b[39m         )\n\u001b[32m   1532\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n",
      "\u001b[31mUnpicklingError\u001b[39m: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL pathlib.PosixPath was not an allowed global by default. Please use `torch.serialization.add_safe_globals([pathlib.PosixPath])` or the `torch.serialization.safe_globals([pathlib.PosixPath])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."
     ]
    }
   ],
   "source": [
    "# Step 4: training loop with folds (same split logic)  (NEW)\n",
    "# ======================================================================\n",
    "\n",
    "def total_loss_with_adj_reg(loss: torch.Tensor, aux: Dict[str, Any], cfg: Dict[str, Any]) -> torch.Tensor:\n",
    "    lam_l1 = float(cfg.get(\"adj_l1_lambda\", 0.0))\n",
    "    lam_pr = float(cfg.get(\"adj_prior_lambda\", 0.0))\n",
    "    reg = 0.0\n",
    "    if lam_l1 > 0:\n",
    "        reg = reg + lam_l1 * aux[\"_l1_off_t\"]\n",
    "    if lam_pr > 0:\n",
    "        reg = reg + lam_pr * aux[\"_mse_prior_t\"]\n",
    "    return loss + reg\n",
    "\n",
    "\n",
    "def _safe_auc_binary(y_true: np.ndarray, score: np.ndarray) -> float:\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    score = np.asarray(score, dtype=np.float64)\n",
    "    if y_true.size == 0 or len(np.unique(y_true)) < 2:\n",
    "        return float(\"nan\")\n",
    "    return float(roc_auc_score(y_true, score))\n",
    "\n",
    "\n",
    "def compute_trade_dir_auc_from_probs(y_tb_true: np.ndarray, prob3: np.ndarray) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    y_tb_true: (n,) in {0,1,2}\n",
    "    prob3: (n,3) in order [SHORT,FLAT,LONG]\n",
    "    trade_auc: trade vs no-trade where trade={0,2} vs flat=1, score = 1 - p_flat\n",
    "    dir_auc: LONG vs SHORT on true-trade samples only, score = p_long/(p_long+p_short)\n",
    "    \"\"\"\n",
    "    y_tb_true = np.asarray(y_tb_true, dtype=np.int64)\n",
    "    prob3 = np.asarray(prob3, dtype=np.float64)\n",
    "\n",
    "    y_trade_bin = (y_tb_true != 1).astype(np.int64)\n",
    "    p_trade = 1.0 - prob3[:, 1]\n",
    "    trade_auc = _safe_auc_binary(y_trade_bin, p_trade)\n",
    "\n",
    "    mask_trade = (y_tb_true != 1)\n",
    "    y_dir_bin = (y_tb_true[mask_trade] == 2).astype(np.int64)  # 1=LONG, 0=SHORT\n",
    "    p_short = prob3[mask_trade, 0]\n",
    "    p_long = prob3[mask_trade, 2]\n",
    "    p_dir = p_long / (p_long + p_short + 1e-12)\n",
    "    dir_auc = _safe_auc_binary(y_dir_bin, p_dir)\n",
    "\n",
    "    return trade_auc, dir_auc\n",
    "\n",
    "\n",
    "def pnl_from_probs_3class(prob3: np.ndarray, exit_ret_arr: np.ndarray, thr_trade: float, thr_dir: float, cost_bps: float) -> Dict[str, Any]:\n",
    "    prob3 = np.asarray(prob3, dtype=np.float64)\n",
    "    exit_ret_arr = np.asarray(exit_ret_arr, dtype=np.float64)\n",
    "\n",
    "    p_short = prob3[:, 0]\n",
    "    p_flat = prob3[:, 1]\n",
    "    p_long = prob3[:, 2]\n",
    "\n",
    "    trade_conf = 1.0 - p_flat\n",
    "    dir_prob = p_long / (p_long + p_short + 1e-12)\n",
    "    dir_conf = np.maximum(dir_prob, 1.0 - dir_prob)\n",
    "\n",
    "    mask = (trade_conf >= float(thr_trade)) & (dir_conf >= float(thr_dir))\n",
    "\n",
    "    action = np.zeros_like(exit_ret_arr, dtype=np.float64)\n",
    "    action[mask] = np.where(dir_prob[mask] >= 0.5, 1.0, -1.0)\n",
    "\n",
    "    cost = (float(cost_bps) * 1e-4) * mask.astype(np.float64)\n",
    "    pnl = action * exit_ret_arr - cost\n",
    "\n",
    "    n = int(len(exit_ret_arr))\n",
    "    n_tr = int(mask.sum())\n",
    "\n",
    "    return {\n",
    "        \"n\": n,\n",
    "        \"n_trades\": n_tr,\n",
    "        \"trade_rate\": float(n_tr / max(1, n)),\n",
    "        \"pnl_sum\": float(pnl.sum()),\n",
    "        \"pnl_mean\": float(pnl.mean()) if n else float(\"nan\"),\n",
    "        \"pnl_per_trade\": float(pnl.sum() / max(1, n_tr)),\n",
    "        \"pnl_sharpe\": float((pnl.mean() / (pnl.std() + 1e-12)) * np.sqrt(288)) if n else float(\"nan\"),\n",
    "    }\n",
    "\n",
    "\n",
    "def build_trade_threshold_grid(p_trade: np.ndarray, base_grid: Optional[List[float]], target_trades_list: Optional[List[int]]) -> List[float]:\n",
    "    p_trade = np.asarray(p_trade, dtype=np.float64)\n",
    "    p_trade = p_trade[np.isfinite(p_trade)]\n",
    "    if p_trade.size == 0:\n",
    "        return base_grid or [0.5]\n",
    "\n",
    "    thrs = set(float(t) for t in (base_grid or []))\n",
    "\n",
    "    if target_trades_list:\n",
    "        N = int(p_trade.size)\n",
    "        for k in target_trades_list:\n",
    "            k = int(k)\n",
    "            if k <= 0:\n",
    "                continue\n",
    "            if k >= N:\n",
    "                thr = float(np.min(p_trade))\n",
    "            else:\n",
    "                q = 1.0 - (k / N)\n",
    "                thr = float(np.quantile(p_trade, q))\n",
    "            thrs.add(float(np.clip(thr, 0.01, 0.99)))\n",
    "\n",
    "    out = sorted(thrs)\n",
    "    cleaned = []\n",
    "    for t in out:\n",
    "        if not cleaned or abs(t - cleaned[-1]) > 1e-6:\n",
    "            cleaned.append(float(t))\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def sweep_thresholds_3class(prob3: np.ndarray, exit_ret_arr: np.ndarray, cfg: Dict[str, Any], min_trades: int, target_trade_rate: Optional[float]) -> pd.DataFrame:\n",
    "    prob3 = np.asarray(prob3, dtype=np.float64)\n",
    "    p_flat = prob3[:, 1]\n",
    "    p_trade = 1.0 - p_flat\n",
    "\n",
    "    thr_trade_grid = build_trade_threshold_grid(\n",
    "        p_trade=p_trade,\n",
    "        base_grid=cfg.get(\"thr_trade_grid\", [0.5]),\n",
    "        target_trades_list=cfg.get(\"proxy_target_trades\", None),\n",
    "    )\n",
    "    thr_dir_grid = cfg.get(\"thr_dir_grid\", [0.5])\n",
    "\n",
    "    obj = str(cfg.get(\"thr_objective\", \"pnl_sum\"))\n",
    "    max_rate = cfg.get(\"max_trade_rate_val\", None)\n",
    "    penalty = float(cfg.get(\"trade_rate_penalty\", 0.0))\n",
    "\n",
    "    rows = []\n",
    "    for thr_t in thr_trade_grid:\n",
    "        for thr_d in thr_dir_grid:\n",
    "            m = pnl_from_probs_3class(prob3, exit_ret_arr, thr_t, thr_d, cfg[\"cost_bps\"])\n",
    "            if int(m[\"n_trades\"]) < int(min_trades):\n",
    "                continue\n",
    "            if max_rate is not None and float(m[\"trade_rate\"]) > float(max_rate):\n",
    "                continue\n",
    "\n",
    "            base = float(m.get(obj, np.nan))\n",
    "            if not np.isfinite(base):\n",
    "                continue\n",
    "\n",
    "            if target_trade_rate is not None:\n",
    "                score = base - penalty * abs(float(m[\"trade_rate\"]) - float(target_trade_rate))\n",
    "            else:\n",
    "                score = base - penalty * float(m[\"trade_rate\"])\n",
    "\n",
    "            rows.append({\"thr_trade\": float(thr_t), \"thr_dir\": float(thr_d), \"score\": float(score), **m})\n",
    "\n",
    "    if not rows:\n",
    "        # relax\n",
    "        return sweep_thresholds_3class(prob3, exit_ret_arr, cfg, min_trades=1, target_trade_rate=target_trade_rate)\n",
    "\n",
    "    return pd.DataFrame(rows).sort_values([\"score\", \"pnl_sum\"], ascending=False)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_probs_on_indices_3class(\n",
    "    model: nn.Module,\n",
    "    X_scaled: np.ndarray,\n",
    "    edge_scaled: np.ndarray,\n",
    "    indices: np.ndarray,\n",
    "    cfg: Dict[str, Any],\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    ds = LobGraphSequenceDataset3Class(\n",
    "        X_node=X_scaled,\n",
    "        E_feat=edge_scaled,\n",
    "        y_tb_arr=y_tb,\n",
    "        exit_ret_arr=exit_ret,\n",
    "        sample_t_=sample_t,\n",
    "        indices=indices,\n",
    "        lookback=cfg[\"lookback\"],\n",
    "    )\n",
    "    loader = DataLoader(ds, batch_size=int(cfg[\"batch_size\"]), shuffle=False, collate_fn=collate_fn_3class, num_workers=0)\n",
    "\n",
    "    probs = []\n",
    "    ers = []\n",
    "    ys = []\n",
    "    for x, e, y, er, _sidx in loader:\n",
    "        x = x.to(DEVICE).float()\n",
    "        e = e.to(DEVICE).float()\n",
    "        logits = model(x, e, return_aux=False)\n",
    "        p = torch.softmax(logits, dim=-1).detach().cpu().numpy()\n",
    "        probs.append(p)\n",
    "        ers.append(er.detach().cpu().numpy())\n",
    "        ys.append(y.detach().cpu().numpy())\n",
    "\n",
    "    return np.concatenate(probs, axis=0), np.concatenate(ers, axis=0), np.concatenate(ys, axis=0)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_3class_on_indices(\n",
    "    model: nn.Module,\n",
    "    X_scaled: np.ndarray,\n",
    "    edge_scaled: np.ndarray,\n",
    "    indices: np.ndarray,\n",
    "    loss_fn: nn.Module,\n",
    "    cfg: Dict[str, Any],\n",
    ") -> Dict[str, Any]:\n",
    "    ds = LobGraphSequenceDataset3Class(\n",
    "        X_node=X_scaled,\n",
    "        E_feat=edge_scaled,\n",
    "        y_tb_arr=y_tb,\n",
    "        exit_ret_arr=exit_ret,\n",
    "        sample_t_=sample_t,\n",
    "        indices=indices,\n",
    "        lookback=cfg[\"lookback\"],\n",
    "    )\n",
    "    loader = DataLoader(ds, batch_size=int(cfg[\"batch_size\"]), shuffle=False, collate_fn=collate_fn_3class, num_workers=0)\n",
    "\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    n = 0\n",
    "\n",
    "    probs = []\n",
    "    ers = []\n",
    "    ys = []\n",
    "\n",
    "    for x, e, y, er, _sidx in loader:\n",
    "        x = x.to(DEVICE).float()\n",
    "        e = e.to(DEVICE).float()\n",
    "        y = y.to(DEVICE).long()\n",
    "\n",
    "        logits, aux = model(x, e, return_aux=True)\n",
    "        ce = loss_fn(logits, y)\n",
    "        loss = total_loss_with_adj_reg(ce, aux, cfg)\n",
    "\n",
    "        total_loss += float(loss.item()) * int(y.size(0))\n",
    "        n += int(y.size(0))\n",
    "\n",
    "        p = torch.softmax(logits, dim=-1).detach().cpu().numpy()\n",
    "        probs.append(p)\n",
    "        ers.append(er.detach().cpu().numpy())\n",
    "        ys.append(y.detach().cpu().numpy())\n",
    "\n",
    "    prob3 = np.concatenate(probs, axis=0) if probs else np.zeros((0, 3), dtype=np.float64)\n",
    "    er_arr = np.concatenate(ers, axis=0) if ers else np.zeros((0,), dtype=np.float64)\n",
    "    y_arr = np.concatenate(ys, axis=0) if ys else np.zeros((0,), dtype=np.int64)\n",
    "\n",
    "    trade_auc, dir_auc = compute_trade_dir_auc_from_probs(y_arr, prob3)\n",
    "\n",
    "    y_pred = prob3.argmax(axis=1) if len(y_arr) else np.array([], dtype=np.int64)\n",
    "    acc = float(accuracy_score(y_arr, y_pred)) if len(y_arr) else float(\"nan\")\n",
    "    f1m = float(f1_score(y_arr, y_pred, average=\"macro\")) if len(y_arr) else float(\"nan\")\n",
    "    cm = confusion_matrix(y_arr, y_pred, labels=[0, 1, 2]) if len(y_arr) else None\n",
    "\n",
    "    return {\n",
    "        \"loss\": float(total_loss / max(1, n)),\n",
    "        \"acc\": acc,\n",
    "        \"f1m\": f1m,\n",
    "        \"cm\": cm,\n",
    "        \"trade_auc\": float(trade_auc) if np.isfinite(trade_auc) else float(\"nan\"),\n",
    "        \"dir_auc\": float(dir_auc) if np.isfinite(dir_auc) else float(\"nan\"),\n",
    "        \"prob3\": prob3,\n",
    "        \"er\": er_arr,\n",
    "        \"y\": y_arr,\n",
    "    }\n",
    "\n",
    "\n",
    "def train_one_fold_3class(\n",
    "    fold_id: int,\n",
    "    X_scaled: np.ndarray,\n",
    "    edge_scaled: np.ndarray,\n",
    "    idx_train: np.ndarray,\n",
    "    idx_val: np.ndarray,\n",
    "    idx_test: np.ndarray,\n",
    "    node_scaler: RobustScaler,\n",
    "    edge_scaler: RobustScaler,\n",
    "    cfg: Dict[str, Any],\n",
    ") -> Dict[str, Any]:\n",
    "    # train labels for weights/sampler\n",
    "    t_train = sample_t[idx_train]\n",
    "    y_train = y_tb[t_train].astype(np.int64)\n",
    "\n",
    "    tr_ds = LobGraphSequenceDataset3Class(X_scaled, edge_scaled, y_tb, exit_ret, sample_t, idx_train, cfg[\"lookback\"])\n",
    "    va_ds = LobGraphSequenceDataset3Class(X_scaled, edge_scaled, y_tb, exit_ret, sample_t, idx_val,   cfg[\"lookback\"])\n",
    "    te_ds = LobGraphSequenceDataset3Class(X_scaled, edge_scaled, y_tb, exit_ret, sample_t, idx_test,  cfg[\"lookback\"])\n",
    "\n",
    "    sampler = None\n",
    "    shuffle = True\n",
    "    if bool(cfg.get(\"use_weighted_sampler\", True)):\n",
    "        sampler = make_weighted_sampler_3class(y_train)\n",
    "        shuffle = False\n",
    "\n",
    "    tr_loader = DataLoader(tr_ds, batch_size=int(cfg[\"batch_size\"]), shuffle=shuffle, sampler=sampler, collate_fn=collate_fn_3class, num_workers=0)\n",
    "    va_loader = DataLoader(va_ds, batch_size=int(cfg[\"batch_size\"]), shuffle=False, collate_fn=collate_fn_3class, num_workers=0)\n",
    "    te_loader = DataLoader(te_ds, batch_size=int(cfg[\"batch_size\"]), shuffle=False, collate_fn=collate_fn_3class, num_workers=0)\n",
    "\n",
    "    model = GraphWaveNet3Class(\n",
    "        node_in=int(X_scaled.shape[-1]),\n",
    "        edge_dim=int(edge_scaled.shape[-1]),\n",
    "        cfg=cfg,\n",
    "        n_nodes=len(ASSETS),\n",
    "        target_node=TARGET_NODE,\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    ce_w = make_ce_weights_3class(y_train)\n",
    "    loss_fn = nn.CrossEntropyLoss(weight=ce_w, label_smoothing=float(cfg.get(\"label_smoothing\", 0.0)))\n",
    "\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=float(cfg[\"lr\"]), weight_decay=float(cfg[\"weight_decay\"]))\n",
    "\n",
    "    use_onecycle = bool(cfg.get(\"use_onecycle\", True))\n",
    "    if use_onecycle:\n",
    "        sch = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            opt,\n",
    "            max_lr=float(cfg[\"lr\"]),\n",
    "            epochs=int(cfg[\"epochs\"]),\n",
    "            steps_per_epoch=max(1, len(tr_loader)),\n",
    "            pct_start=0.15,\n",
    "            div_factor=10.0,\n",
    "            final_div_factor=50.0,\n",
    "        )\n",
    "    else:\n",
    "        sch = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"max\", factor=0.5, patience=3)\n",
    "\n",
    "    best_sel = -1e18\n",
    "    best_state = None\n",
    "    best_epoch = -1\n",
    "    patience = 7\n",
    "    bad = 0\n",
    "\n",
    "    sel_w_dir = float(cfg.get(\"sel_metric_dir_weight\", 0.5))\n",
    "    trade_pen = float(cfg.get(\"trade_prob_penalty\", 0.0))\n",
    "\n",
    "    def _sel_metric(trade_auc: float, dir_auc: float) -> float:\n",
    "        ta = float(trade_auc) if np.isfinite(trade_auc) else -1e18\n",
    "        da = float(dir_auc) if np.isfinite(dir_auc) else 0.0\n",
    "        return ta + sel_w_dir * da\n",
    "\n",
    "    for ep in range(1, int(cfg[\"epochs\"]) + 1):\n",
    "        model.train()\n",
    "        tot_loss = 0.0\n",
    "        n = 0\n",
    "\n",
    "        for x, e, y, _er, _sidx in tr_loader:\n",
    "            x = x.to(DEVICE).float()\n",
    "            e = e.to(DEVICE).float()\n",
    "            y = y.to(DEVICE).long()\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "\n",
    "            logits, aux = model(x, e, return_aux=True)\n",
    "            ce = loss_fn(logits, y)\n",
    "\n",
    "            # optional: discourage overtrading probability mass\n",
    "            if trade_pen > 0:\n",
    "                p = torch.softmax(logits, dim=-1)\n",
    "                p_trade = (p[:, 0] + p[:, 2]).mean()\n",
    "                ce = ce + trade_pen * p_trade\n",
    "\n",
    "            loss = total_loss_with_adj_reg(ce, aux, cfg)\n",
    "            if not torch.isfinite(loss):\n",
    "                continue\n",
    "\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), float(cfg[\"grad_clip\"]))\n",
    "            opt.step()\n",
    "            if use_onecycle:\n",
    "                sch.step()\n",
    "\n",
    "            tot_loss += float(loss.item()) * int(y.size(0))\n",
    "            n += int(y.size(0))\n",
    "\n",
    "        tr_loss = tot_loss / max(1, n)\n",
    "\n",
    "        # quick val metrics (AUCs) each epoch\n",
    "        model.eval()\n",
    "        va_probs = []\n",
    "        va_ys = []\n",
    "        for x, e, y, _er, _sidx in va_loader:\n",
    "            x = x.to(DEVICE).float()\n",
    "            e = e.to(DEVICE).float()\n",
    "            logits = model(x, e, return_aux=False)\n",
    "            p = torch.softmax(logits, dim=-1).detach().cpu().numpy()\n",
    "            va_probs.append(p)\n",
    "            va_ys.append(y.numpy())\n",
    "\n",
    "        va_prob3 = np.concatenate(va_probs, axis=0) if va_probs else np.zeros((0, 3), dtype=np.float64)\n",
    "        va_y = np.concatenate(va_ys, axis=0) if va_ys else np.zeros((0,), dtype=np.int64)\n",
    "        trade_auc, dir_auc = compute_trade_dir_auc_from_probs(va_y, va_prob3)\n",
    "        sel = _sel_metric(trade_auc, dir_auc)\n",
    "\n",
    "        if sel > best_sel:\n",
    "            best_sel = sel\n",
    "            best_epoch = ep\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            bad = 0\n",
    "        else:\n",
    "            bad += 1\n",
    "\n",
    "        if not use_onecycle:\n",
    "            sch.step(sel)\n",
    "\n",
    "        lr_now = opt.param_groups[0][\"lr\"]\n",
    "        w_support = model.support_mix().detach().cpu().numpy().tolist()\n",
    "        print(\n",
    "            f\"[fold {fold_id:02d}] ep {ep:02d} lr={lr_now:.2e} \"\n",
    "            f\"tr_loss={tr_loss:.4f} val_trade_auc={trade_auc:.3f} val_dir_auc={dir_auc:.3f} \"\n",
    "            f\"sel={sel:.3f} best={best_sel:.3f}@ep{best_epoch:02d} supports={np.round(w_support, 3).tolist()}\"\n",
    "        )\n",
    "\n",
    "        if bad >= patience:\n",
    "            break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    # final eval on val + test (and threshold selection on val only)\n",
    "    val_eval = eval_3class_on_indices(model, X_scaled, edge_scaled, idx_val, loss_fn, cfg)\n",
    "    test_eval = eval_3class_on_indices(model, X_scaled, edge_scaled, idx_test, loss_fn, cfg)\n",
    "\n",
    "    # thresholds chosen on VAL for PnL, then applied to TEST\n",
    "    true_val_trade_rate = split_trade_ratio(idx_val, sample_t, y_trade)\n",
    "    sweep_val = sweep_thresholds_3class(\n",
    "        prob3=val_eval[\"prob3\"],\n",
    "        exit_ret_arr=val_eval[\"er\"],\n",
    "        cfg=cfg,\n",
    "        min_trades=int(cfg[\"eval_min_trades\"]),\n",
    "        target_trade_rate=float(true_val_trade_rate),\n",
    "    )\n",
    "    best_thr = sweep_val.iloc[0].to_dict()\n",
    "    thr_trade = float(best_thr[\"thr_trade\"])\n",
    "    thr_dir = float(best_thr[\"thr_dir\"])\n",
    "\n",
    "    pnl_val = pnl_from_probs_3class(val_eval[\"prob3\"], val_eval[\"er\"], thr_trade, thr_dir, cfg[\"cost_bps\"])\n",
    "    pnl_test = pnl_from_probs_3class(test_eval[\"prob3\"], test_eval[\"er\"], thr_trade, thr_dir, cfg[\"cost_bps\"])\n",
    "\n",
    "    print(\n",
    "        f\"[fold {fold_id:02d}] chosen thresholds on VAL: thr_trade={thr_trade:.3f} thr_dir={thr_dir:.3f} \"\n",
    "        f\"| val pnl_sum={pnl_val['pnl_sum']:.4f} val trade_rate={pnl_val['trade_rate']:.3f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"[fold {fold_id:02d}] TEST (fixed thresholds from VAL): \"\n",
    "        f\"trade_auc={test_eval['trade_auc']:.3f} dir_auc={test_eval['dir_auc']:.3f} \"\n",
    "        f\"pnl_sum={pnl_test['pnl_sum']:.4f} trade_rate={pnl_test['trade_rate']:.3f} trades={pnl_test['n_trades']}\"\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"fold\": int(fold_id),\n",
    "        \"model_state\": {k: v.detach().cpu().clone() for k, v in model.state_dict().items()},\n",
    "        \"cfg\": cfg,\n",
    "        \"node_scaler\": node_scaler,\n",
    "        \"edge_scaler\": edge_scaler,\n",
    "        \"idx_train\": idx_train,\n",
    "        \"idx_val\": idx_val,\n",
    "        \"idx_test\": idx_test,\n",
    "        \"best_epoch\": int(best_epoch),\n",
    "        \"best_sel\": float(best_sel),\n",
    "        \"val_eval\": val_eval,\n",
    "        \"test_eval\": test_eval,\n",
    "        \"thr_trade\": thr_trade,\n",
    "        \"thr_dir\": thr_dir,\n",
    "        \"pnl_val\": pnl_val,\n",
    "        \"pnl_test\": pnl_test,\n",
    "        \"sweep_val_head\": sweep_val.head(5),\n",
    "    }\n",
    "\n",
    "\n",
    "def save_checkpoint(path: Path, payload: Dict[str, Any]) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    torch.save(payload, str(path))\n",
    "\n",
    "\n",
    "def load_checkpoint(path: Path) -> Dict[str, Any]:\n",
    "    return torch.load(str(path), map_location=\"cpu\", weights_only=False)\n",
    "\n",
    "\n",
    "def run_walk_forward_cv_3class() -> Tuple[pd.DataFrame, List[Dict[str, Any]], Path]:\n",
    "    fold_artifacts = []\n",
    "    rows = []\n",
    "\n",
    "    best_overall_sel = -1e18\n",
    "    best_overall_path = None\n",
    "\n",
    "    for fi, (idx_tr, idx_va, idx_te) in enumerate(walk_splits, 1):\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(f\"FOLD {fi}/{len(walk_splits)} sizes: train={len(idx_tr)} val={len(idx_va)} test={len(idx_te)}\")\n",
    "        print(f\"True trade ratio (val):  {split_trade_ratio(idx_va, sample_t, y_trade):.3f}\")\n",
    "        print(f\"True trade ratio (test): {split_trade_ratio(idx_te, sample_t, y_trade):.3f}\")\n",
    "\n",
    "        # fold scaling (fit only on fold train timeline)\n",
    "        X_scaled, node_scaler = fit_scale_nodes_train_only(X_node_raw, sample_t, idx_tr, max_abs=CFG[\"max_abs_feat\"])\n",
    "        if bool(CFG.get(\"edge_scale\", True)):\n",
    "            edge_scaled, edge_scaler = fit_scale_edges_train_only(edge_feat, sample_t, idx_tr, max_abs=CFG[\"max_abs_edge\"])\n",
    "        else:\n",
    "            edge_scaled = edge_feat.astype(np.float32)\n",
    "            edge_scaler = None\n",
    "\n",
    "        artifact = train_one_fold_3class(\n",
    "            fold_id=fi,\n",
    "            X_scaled=X_scaled,\n",
    "            edge_scaled=edge_scaled,\n",
    "            idx_train=idx_tr,\n",
    "            idx_val=idx_va,\n",
    "            idx_test=idx_te,\n",
    "            node_scaler=node_scaler,\n",
    "            edge_scaler=edge_scaler,\n",
    "            cfg=CFG,\n",
    "        )\n",
    "\n",
    "        # save fold checkpoint\n",
    "        ckpt_path = CFG[\"ckpt_dir\"] / f\"fold_{fi:02d}_best.pt\"\n",
    "        save_checkpoint(ckpt_path, {\n",
    "            \"kind\": \"fold_best\",\n",
    "            \"fold\": fi,\n",
    "            \"model_state\": artifact[\"model_state\"],\n",
    "            \"cfg\": dict(CFG),\n",
    "            \"node_scaler\": artifact[\"node_scaler\"],\n",
    "            \"edge_scaler\": artifact[\"edge_scaler\"],\n",
    "            \"thr_trade\": artifact[\"thr_trade\"],\n",
    "            \"thr_dir\": artifact[\"thr_dir\"],\n",
    "            \"idx_train\": artifact[\"idx_train\"],\n",
    "            \"idx_val\": artifact[\"idx_val\"],\n",
    "            \"idx_test\": artifact[\"idx_test\"],\n",
    "        })\n",
    "        print(\"Saved fold checkpoint:\", str(ckpt_path))\n",
    "\n",
    "        # track overall best by selection metric\n",
    "        if float(artifact[\"best_sel\"]) > best_overall_sel:\n",
    "            best_overall_sel = float(artifact[\"best_sel\"])\n",
    "            best_overall_path = ckpt_path\n",
    "\n",
    "        fold_artifacts.append(artifact)\n",
    "\n",
    "        rows.append({\n",
    "            \"fold\": fi,\n",
    "            \"val_trade_auc\": artifact[\"val_eval\"][\"trade_auc\"],\n",
    "            \"val_dir_auc\": artifact[\"val_eval\"][\"dir_auc\"],\n",
    "            \"test_trade_auc\": artifact[\"test_eval\"][\"trade_auc\"],\n",
    "            \"test_dir_auc\": artifact[\"test_eval\"][\"dir_auc\"],\n",
    "            \"thr_trade\": artifact[\"thr_trade\"],\n",
    "            \"thr_dir\": artifact[\"thr_dir\"],\n",
    "            \"test_trade_rate_pred\": artifact[\"pnl_test\"][\"trade_rate\"],\n",
    "            \"test_pnl_sum\": artifact[\"pnl_test\"][\"pnl_sum\"],\n",
    "            \"test_pnl_mean\": artifact[\"pnl_test\"][\"pnl_mean\"],\n",
    "            \"test_n_trades\": artifact[\"pnl_test\"][\"n_trades\"],\n",
    "            \"best_sel\": artifact[\"best_sel\"],\n",
    "        })\n",
    "\n",
    "    cv_summary = pd.DataFrame(rows)\n",
    "    assert best_overall_path is not None\n",
    "    overall_copy = CFG[\"ckpt_dir\"] / \"overall_best.pt\"\n",
    "    save_checkpoint(overall_copy, {**load_checkpoint(best_overall_path), \"kind\": \"overall_best\", \"source_ckpt\": str(best_overall_path)})\n",
    "    print(\"\\nSaved overall best checkpoint:\", str(overall_copy))\n",
    "    return cv_summary, fold_artifacts, overall_copy\n",
    "\n",
    "\n",
    "cv_summary_3c, fold_artifacts_3c, overall_best_ckpt = run_walk_forward_cv_3class()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CV summary (3-class model; TEST uses thresholds selected on VAL):\")\n",
    "print(cv_summary_3c)\n",
    "print(\"\\nMeans (debug only):\")\n",
    "print(cv_summary_3c.mean(numeric_only=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_checkpoint(path: Path) -> Dict[str, Any]:\n",
    "    return torch.load(str(path), map_location=\"cpu\", weights_only=False)\n",
    "\n",
    "overall_best_ckpt = CFG[\"ckpt_dir\"] / \"overall_best.pt\"\n",
    "best_overall_path = CFG[\"ckpt_dir\"] / \"fold_04_best.pt\"\n",
    "\n",
    "save_checkpoint(overall_best_ckpt, {**load_checkpoint(best_overall_path), \"kind\": \"overall_best\", \"source_ckpt\": str(best_overall_path)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FINAL HOLDOUT (10%) using overall_best.pt (no refit)\n",
      "ckpt: checkpoints_gwnet_3class/overall_best.pt\n",
      "trade_auc=0.534 | dir_auc=0.521\n",
      "pnl_sum=-0.1026 | trade_rate=0.613 | trades=770\n",
      "\n",
      "================================================================================\n",
      "PRODUCTION-FIT (train on CV(90%) -> select thresholds on val_final -> eval on FINAL holdout(10%))\n",
      "Sizes:\n",
      "  train_final: 10175\n",
      "  val_final  : 1130\n",
      "  holdout    : 1256\n",
      "True trade ratio (val_final): 0.504\n",
      "True trade ratio (holdout):   0.576\n",
      "[fold 99] ep 01 lr=9.80e-05 tr_loss=1.1010 val_trade_auc=0.545 val_dir_auc=0.511 sel=0.800 best=0.800@ep01 supports=[0.333, 0.334, 0.333]\n",
      "[fold 99] ep 02 lr=2.34e-04 tr_loss=1.0059 val_trade_auc=0.616 val_dir_auc=0.556 sel=0.894 best=0.894@ep02 supports=[0.333, 0.334, 0.333]\n",
      "[fold 99] ep 03 lr=3.00e-04 tr_loss=0.9740 val_trade_auc=0.627 val_dir_auc=0.559 sel=0.906 best=0.906@ep03 supports=[0.333, 0.334, 0.333]\n",
      "[fold 99] ep 04 lr=2.97e-04 tr_loss=0.9573 val_trade_auc=0.626 val_dir_auc=0.565 sel=0.908 best=0.908@ep04 supports=[0.335, 0.329, 0.335]\n",
      "[fold 99] ep 05 lr=2.90e-04 tr_loss=0.9251 val_trade_auc=0.619 val_dir_auc=0.572 sel=0.905 best=0.908@ep04 supports=[0.338, 0.325, 0.338]\n",
      "[fold 99] ep 06 lr=2.77e-04 tr_loss=0.9002 val_trade_auc=0.625 val_dir_auc=0.564 sel=0.907 best=0.908@ep04 supports=[0.34, 0.32, 0.34]\n",
      "[fold 99] ep 07 lr=2.61e-04 tr_loss=0.8644 val_trade_auc=0.615 val_dir_auc=0.562 sel=0.896 best=0.908@ep04 supports=[0.342, 0.316, 0.342]\n",
      "[fold 99] ep 08 lr=2.40e-04 tr_loss=0.8299 val_trade_auc=0.622 val_dir_auc=0.546 sel=0.895 best=0.908@ep04 supports=[0.344, 0.31, 0.346]\n",
      "[fold 99] ep 09 lr=2.17e-04 tr_loss=0.8049 val_trade_auc=0.612 val_dir_auc=0.572 sel=0.898 best=0.908@ep04 supports=[0.346, 0.306, 0.348]\n",
      "[fold 99] ep 10 lr=1.91e-04 tr_loss=0.7679 val_trade_auc=0.608 val_dir_auc=0.545 sel=0.880 best=0.908@ep04 supports=[0.347, 0.301, 0.352]\n",
      "[fold 99] ep 11 lr=1.64e-04 tr_loss=0.7330 val_trade_auc=0.590 val_dir_auc=0.554 sel=0.867 best=0.908@ep04 supports=[0.347, 0.298, 0.355]\n",
      "[fold 99] chosen thresholds on VAL: thr_trade=0.750 thr_dir=0.650 | val pnl_sum=0.2796 val trade_rate=0.238\n",
      "[fold 99] TEST (fixed thresholds from VAL): trade_auc=0.503 dir_auc=0.570 pnl_sum=0.0915 trade_rate=0.205 trades=258\n",
      "Saved production checkpoint: checkpoints_gwnet_3class/production_best.pt\n",
      "\n",
      "PRODUCTION FINAL HOLDOUT RESULT:\n",
      "trade_auc=0.503 | dir_auc=0.570\n",
      "trade_rate=0.205 | pnl_sum=0.0915 | trades=258\n"
     ]
    }
   ],
   "source": [
    "# Step 5: evaluation (final holdout) + production-fit  (NEW)\n",
    "# ======================================================================\n",
    "\n",
    "def apply_node_scaler(X_raw: np.ndarray, scaler: RobustScaler, max_abs: float) -> np.ndarray:\n",
    "    Fdim = X_raw.shape[-1]\n",
    "    X_scaled = scaler.transform(X_raw.reshape(-1, Fdim)).reshape(X_raw.shape).astype(np.float32)\n",
    "    X_scaled = np.clip(X_scaled, -max_abs, max_abs).astype(np.float32)\n",
    "    return np.nan_to_num(X_scaled, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n",
    "\n",
    "\n",
    "def apply_edge_scaler(E_raw: np.ndarray, scaler: RobustScaler, max_abs: float) -> np.ndarray:\n",
    "    D = E_raw.shape[-1]\n",
    "    E_scaled = scaler.transform(E_raw.reshape(-1, D)).reshape(E_raw.shape).astype(np.float32)\n",
    "    E_scaled = np.clip(E_scaled, -max_abs, max_abs).astype(np.float32)\n",
    "    return np.nan_to_num(E_scaled, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n",
    "\n",
    "\n",
    "def build_model_from_ckpt(ckpt: Dict[str, Any]) -> nn.Module:\n",
    "    cfg = ckpt[\"cfg\"]\n",
    "    model = GraphWaveNet3Class(\n",
    "        node_in=int(X_node_raw.shape[-1]),\n",
    "        edge_dim=int(edge_feat.shape[-1]),\n",
    "        cfg=cfg,\n",
    "        n_nodes=len(ASSETS),\n",
    "        target_node=TARGET_NODE,\n",
    "    ).to(DEVICE)\n",
    "    model.load_state_dict(ckpt[\"model_state\"])\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_checkpoint_on_indices(ckpt_path: Path, indices: np.ndarray, label: str) -> Dict[str, Any]:\n",
    "    ckpt = load_checkpoint(ckpt_path)\n",
    "    model = build_model_from_ckpt(ckpt)\n",
    "\n",
    "    node_scaler = ckpt[\"node_scaler\"]\n",
    "    edge_scaler = ckpt[\"edge_scaler\"]\n",
    "\n",
    "    X_scaled = apply_node_scaler(X_node_raw, node_scaler, max_abs=float(CFG[\"max_abs_feat\"]))\n",
    "    if edge_scaler is not None:\n",
    "        E_scaled = apply_edge_scaler(edge_feat, edge_scaler, max_abs=float(CFG[\"max_abs_edge\"]))\n",
    "    else:\n",
    "        E_scaled = edge_feat.astype(np.float32)\n",
    "\n",
    "    # loss fn for reporting\n",
    "    t_train = sample_t[np.asarray(ckpt[\"idx_train\"], dtype=np.int64)]\n",
    "    y_train = y_tb[t_train].astype(np.int64)\n",
    "    ce_w = make_ce_weights_3class(y_train)\n",
    "    loss_fn = nn.CrossEntropyLoss(weight=ce_w, label_smoothing=float(CFG.get(\"label_smoothing\", 0.0)))\n",
    "\n",
    "    ev = eval_3class_on_indices(model, X_scaled, E_scaled, indices.astype(np.int64), loss_fn, CFG)\n",
    "\n",
    "    pnl = pnl_from_probs_3class(ev[\"prob3\"], ev[\"er\"], float(ckpt[\"thr_trade\"]), float(ckpt[\"thr_dir\"]), float(CFG[\"cost_bps\"]))\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"{label}\")\n",
    "    print(f\"ckpt: {str(ckpt_path)}\")\n",
    "    print(f\"trade_auc={ev['trade_auc']:.3f} | dir_auc={ev['dir_auc']:.3f}\")\n",
    "    print(f\"pnl_sum={pnl['pnl_sum']:.4f} | trade_rate={pnl['trade_rate']:.3f} | trades={pnl['n_trades']}\")\n",
    "    return {\"eval\": ev, \"pnl\": pnl}\n",
    "\n",
    "\n",
    "# Evaluate overall best (from CV) on FINAL holdout (10%) without refit\n",
    "holdout_indices = idx_final_test.astype(np.int64)\n",
    "_ = evaluate_checkpoint_on_indices(overall_best_ckpt, holdout_indices, label=\"FINAL HOLDOUT (10%) using overall_best.pt (no refit)\")\n",
    "\n",
    "# Production-fit: train on CV(90%) with final val window; select thresholds on val_final; evaluate on holdout (10%)\n",
    "def run_production_fit_3class() -> Dict[str, Any]:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"PRODUCTION-FIT (train on CV(90%) -> select thresholds on val_final -> eval on FINAL holdout(10%))\")\n",
    "\n",
    "    val_w = max(1, int(CFG[\"val_window_frac\"] * n_samples_cv))\n",
    "    train_end = n_samples_cv - val_w\n",
    "\n",
    "    idx_train_final = np.arange(0, train_end, dtype=np.int64)\n",
    "    idx_val_final = np.arange(train_end, n_samples_cv, dtype=np.int64)\n",
    "    idx_holdout = idx_final_test.astype(np.int64)\n",
    "\n",
    "    print(\"Sizes:\")\n",
    "    print(\"  train_final:\", len(idx_train_final))\n",
    "    print(\"  val_final  :\", len(idx_val_final))\n",
    "    print(\"  holdout    :\", len(idx_holdout))\n",
    "    print(f\"True trade ratio (val_final): {split_trade_ratio(idx_val_final, sample_t, y_trade):.3f}\")\n",
    "    print(f\"True trade ratio (holdout):   {split_trade_ratio(idx_holdout, sample_t, y_trade):.3f}\")\n",
    "\n",
    "    X_scaled, node_scaler = fit_scale_nodes_train_only(X_node_raw, sample_t, idx_train_final, max_abs=CFG[\"max_abs_feat\"])\n",
    "    if bool(CFG.get(\"edge_scale\", True)):\n",
    "        edge_scaled, edge_scaler = fit_scale_edges_train_only(edge_feat, sample_t, idx_train_final, max_abs=CFG[\"max_abs_edge\"])\n",
    "    else:\n",
    "        edge_scaled = edge_feat.astype(np.float32)\n",
    "        edge_scaler = None\n",
    "\n",
    "    artifact = train_one_fold_3class(\n",
    "        fold_id=99,  # production id\n",
    "        X_scaled=X_scaled,\n",
    "        edge_scaled=edge_scaled,\n",
    "        idx_train=idx_train_final,\n",
    "        idx_val=idx_val_final,\n",
    "        idx_test=idx_holdout,\n",
    "        node_scaler=node_scaler,\n",
    "        edge_scaler=edge_scaler,\n",
    "        cfg=CFG,\n",
    "    )\n",
    "\n",
    "    ckpt_path = CFG[\"ckpt_dir\"] / \"production_best.pt\"\n",
    "    save_checkpoint(ckpt_path, {\n",
    "        \"kind\": \"production_best\",\n",
    "        \"model_state\": artifact[\"model_state\"],\n",
    "        \"cfg\": dict(CFG),\n",
    "        \"node_scaler\": artifact[\"node_scaler\"],\n",
    "        \"edge_scaler\": artifact[\"edge_scaler\"],\n",
    "        \"thr_trade\": artifact[\"thr_trade\"],\n",
    "        \"thr_dir\": artifact[\"thr_dir\"],\n",
    "        \"idx_train\": artifact[\"idx_train\"],\n",
    "        \"idx_val\": artifact[\"idx_val\"],\n",
    "        \"idx_test\": artifact[\"idx_test\"],  # holdout\n",
    "    })\n",
    "    print(\"Saved production checkpoint:\", str(ckpt_path))\n",
    "\n",
    "    print(\"\\nPRODUCTION FINAL HOLDOUT RESULT:\")\n",
    "    print(f\"trade_auc={artifact['test_eval']['trade_auc']:.3f} | dir_auc={artifact['test_eval']['dir_auc']:.3f}\")\n",
    "    print(f\"trade_rate={artifact['pnl_test']['trade_rate']:.3f} | pnl_sum={artifact['pnl_test']['pnl_sum']:.4f} | trades={artifact['pnl_test']['n_trades']}\")\n",
    "\n",
    "    return {\"artifact\": artifact, \"ckpt_path\": ckpt_path}\n",
    "\n",
    "\n",
    "prod_out = run_production_fit_3class()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TEST-ONLY FROM CHECKPOINT\n",
      "ckpt: checkpoints_gwnet_3class/production_best.pt\n",
      "trade_auc=0.503 | dir_auc=0.570\n",
      "pnl_sum=0.0915 | trade_rate=0.205 | trades=258\n"
     ]
    }
   ],
   "source": [
    "# Step 6: checkpoint save/load utilities + \"test-only from checkpoint\"  (NEW)\n",
    "# ======================================================================\n",
    "\n",
    "# Edit this path to any saved checkpoint:\n",
    "TEST_ONLY_CKPT_PATH = prod_out[\"ckpt_path\"]  # e.g. CFG[\"ckpt_dir\"] / \"production_best.pt\"\n",
    "\n",
    "# Choose what to evaluate:\n",
    "# - For fold checkpoints: use the saved idx_test (fold test)\n",
    "# - For production checkpoint: idx_test is the FINAL holdout\n",
    "ckpt_loaded = load_checkpoint(TEST_ONLY_CKPT_PATH)\n",
    "idx_eval = np.asarray(ckpt_loaded[\"idx_test\"], dtype=np.int64)\n",
    "\n",
    "_ = evaluate_checkpoint_on_indices(TEST_ONLY_CKPT_PATH, idx_eval, label=\"TEST-ONLY FROM CHECKPOINT\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single sample shapes:\n",
      "  x_seq: (240, 3, 15) (L,N,F)\n",
      "  e_seq: (240, 9, 20) (L,E,D)\n",
      "  y_tb: 0 -> SHORT\n",
      "  exit_ret: -0.008445847779512405 sidx: 0\n",
      "\n",
      "Model forward sanity:\n",
      "  logits: torch.Size([1, 3]) expected (B,3)\n",
      "  logits finite: True\n",
      "  support weights: [0.3333333432674408, 0.3333333432674408, 0.3333333432674408]\n",
      "  reg l1_off: 0.33333662152290344 mse_prior: 0.005591095890849829\n"
     ]
    }
   ],
   "source": [
    "# Step 7: debug / sanity checks  (NEW)\n",
    "# ======================================================================\n",
    "\n",
    "# One forward pass shape sanity check\n",
    "with torch.no_grad():\n",
    "    # use first 2 samples from CV space\n",
    "    idx_dbg = np.arange(0, min(2, len(idx_cv_all)), dtype=np.int64)\n",
    "\n",
    "    # scale using a simple train-only scaler on a small prefix (debug only)\n",
    "    idx_tr_dbg = np.arange(0, max(200, int(0.2 * len(idx_cv_all))), dtype=np.int64)\n",
    "    X_dbg, _scn = fit_scale_nodes_train_only(X_node_raw, sample_t, idx_tr_dbg, max_abs=CFG[\"max_abs_feat\"])\n",
    "    E_dbg, _sce = fit_scale_edges_train_only(edge_feat, sample_t, idx_tr_dbg, max_abs=CFG[\"max_abs_edge\"])\n",
    "\n",
    "    ds_dbg = LobGraphSequenceDataset3Class(X_dbg, E_dbg, y_tb, exit_ret, sample_t, idx_dbg, CFG[\"lookback\"])\n",
    "    x_seq, e_seq, y0, er0, sidx0 = ds_dbg[0]\n",
    "\n",
    "    print(\"Single sample shapes:\")\n",
    "    print(\"  x_seq:\", tuple(x_seq.shape), \"(L,N,F)\")\n",
    "    print(\"  e_seq:\", tuple(e_seq.shape), \"(L,E,D)\")\n",
    "    print(\"  y_tb:\", int(y0.item()), \"->\", CLASS_NAMES[int(y0.item())])\n",
    "    print(\"  exit_ret:\", float(er0.item()), \"sidx:\", int(sidx0.item()))\n",
    "\n",
    "    m_dbg = GraphWaveNet3Class(\n",
    "        node_in=int(X_node_raw.shape[-1]),\n",
    "        edge_dim=int(edge_feat.shape[-1]),\n",
    "        cfg=CFG,\n",
    "        n_nodes=len(ASSETS),\n",
    "        target_node=TARGET_NODE,\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    xb = x_seq.unsqueeze(0).to(DEVICE).float()  # (1,L,N,F)\n",
    "    eb = e_seq.unsqueeze(0).to(DEVICE).float()  # (1,L,E,D)\n",
    "    logits, aux = m_dbg(xb, eb, return_aux=True)\n",
    "\n",
    "    print(\"\\nModel forward sanity:\")\n",
    "    print(\"  logits:\", logits.shape, \"expected (B,3)\")\n",
    "    print(\"  logits finite:\", bool(torch.isfinite(logits).all().item()))\n",
    "    print(\"  support weights:\", aux[\"support_w\"])\n",
    "    print(\"  reg l1_off:\", aux[\"l1_off\"], \"mse_prior:\", aux[\"mse_prior\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recbole_new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
