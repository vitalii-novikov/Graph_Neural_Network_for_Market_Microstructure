{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cpu\n",
      "EDGE_LIST: ['ADA->BTC', 'ADA->ETH', 'BTC->ADA', 'BTC->ETH', 'ETH->ADA', 'ETH->BTC', 'ADA->ADA', 'BTC->BTC', 'ETH->ETH']\n",
      "EDGE_INDEX: [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1], [0, 0], [1, 1], [2, 2]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Two-stage LOB GNN (SGA-TCN) — VS Code / Jupyter compatible (.py)\n",
    "\n",
    "Key principles:\n",
    "- Train-only scaling (time-ordered, no leakage)\n",
    "- Stage A: trade/no-trade (AUC)\n",
    "- Stage B: direction on trade-only (AUC on trade-only)\n",
    "- Thresholds (thr_trade, thr_dir) are selected ONLY on validation, never on test/holdout\n",
    "\n",
    "Step 10:\n",
    "- Walk-forward CV on CV-part (90%) with fold test\n",
    "- We also store per-fold artifacts (models + thresholds + val predictions)\n",
    "\n",
    "Step 11:\n",
    "- Post-CV \"final checks\" on FINAL holdout (10%) WITHOUT any additional refit:\n",
    "  3 evaluation methods (no mean/median thresholds by default):\n",
    "    1) LAST fold model + LAST fold thresholds\n",
    "    2) BEST-VAL fold model + BEST-VAL fold thresholds\n",
    "    3) LAST fold model + GLOBAL thresholds selected on concatenated fold-VAL predictions\n",
    "\n",
    "Step 12:\n",
    "- Production-fit: train on full CV-part (90%) with last val window, select thresholds on val_final,\n",
    "  then evaluate on FINAL holdout (10%).\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, roc_auc_score\n",
    "\n",
    "\n",
    "def seed_everything(seed: int = 1234) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "seed_everything(100)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "\n",
    "torch.set_num_threads(max(1, os.cpu_count() or 4))\n",
    "\n",
    "CFG: Dict[str, Any] = {\n",
    "    # data\n",
    "    \"freq\": \"1min\",\n",
    "    \"data_dir\": Path(\"../dataset\"),\n",
    "    \"final_test_frac\": 0.10,\n",
    "\n",
    "    # order book\n",
    "    \"book_levels\": 15,\n",
    "    \"top_levels\": 5,\n",
    "    \"near_levels\": 5,\n",
    "\n",
    "    # walk-forward windows (in sample-space)\n",
    "    \"train_min_frac\": 0.50,\n",
    "    \"val_window_frac\": 0.10,\n",
    "    \"test_window_frac\": 0.10,\n",
    "    \"step_window_frac\": 0.10,\n",
    "\n",
    "    # scaling\n",
    "    \"max_abs_feat\": 10.0,\n",
    "    \"max_abs_edge\": 6.0,\n",
    "\n",
    "    # correlations / graph\n",
    "    \"corr_windows\": [6 * 5, 12 * 5, 24 * 5, 48 * 5, 84 * 5],  # 30m,1h,2h,4h,7h\n",
    "    \"corr_lags\": [0, 1, 2, 5],  # lead-lag (no leakage: shift source)\n",
    "    \"edges_mode\": \"all_pairs\",  # \"manual\" | \"all_pairs\"\n",
    "    \"edges\": [(\"ADA\", \"BTC\"), (\"ADA\", \"ETH\"), (\"ETH\", \"BTC\")],  # used if edges_mode=\"manual\"\n",
    "    \"add_self_loops\": True,\n",
    "    \"edge_transform\": \"fisher\",  # \"none\" | \"fisher\"\n",
    "    \"edge_scale\": True,\n",
    "    \"edge_dropout\": 0.10,\n",
    "\n",
    "    # triple-barrier\n",
    "    \"tb_horizon\": 1 * 30,\n",
    "    \"lookback\": 4 * 12 * 5,\n",
    "    \"tb_pt_mult\": 1.2,\n",
    "    \"tb_sl_mult\": 1.1,\n",
    "    \"tb_min_barrier\": 0.001,\n",
    "    \"tb_max_barrier\": 0.006,\n",
    "\n",
    "    # training\n",
    "    \"batch_size\": 128,\n",
    "    \"epochs\": 20,\n",
    "    \"lr\": 3e-4,\n",
    "    \"weight_decay\": 5e-4,\n",
    "    \"grad_clip\": 1.0,\n",
    "    \"dropout\": 0.15,\n",
    "\n",
    "    # stability tricks\n",
    "    \"label_smoothing\": 0.02,\n",
    "    \"use_weighted_sampler\": True,\n",
    "    \"use_onecycle\": True,\n",
    "\n",
    "    # model\n",
    "    \"hidden\": 128,\n",
    "    \"gnn_layers\": 3,\n",
    "\n",
    "    # spatial (GAT)\n",
    "    \"gat_heads\": 2,\n",
    "\n",
    "    # temporal (TCN)\n",
    "    \"tcn_channels\": 128,\n",
    "    \"tcn_layers\": 3,\n",
    "    \"tcn_kernel\": 2,\n",
    "    \"tcn_dropout\": 0.20,\n",
    "    \"tcn_causal\": True,\n",
    "    \"tcn_pool\": \"last\",\n",
    "\n",
    "    # trading eval\n",
    "    \"cost_bps\": 1.0,\n",
    "\n",
    "    # threshold sweep grids (val only)\n",
    "    \"thr_trade_grid\": [0.50, 0.55, 0.60, 0.65, 0.70, 0.75],\n",
    "    \"thr_dir_grid\":   [0.50, 0.55, 0.60, 0.65, 0.70],\n",
    "\n",
    "    # min trades constraints\n",
    "    \"eval_min_trades\": 50,\n",
    "\n",
    "    # anti-overtrading threshold selection\n",
    "    \"max_trade_rate_val\": 0.65,\n",
    "    \"trade_rate_penalty\": 0.10,\n",
    "    \"thr_objective\": \"pnl_sum\",  # \"pnl_sum\" | \"pnl_sharpe\" | \"pnl_per_trade\"\n",
    "\n",
    "    # dynamic quantile candidates for thr_trade\n",
    "    \"proxy_target_trades\": [50, 100, 200],\n",
    "}\n",
    "\n",
    "ASSETS = [\"ADA\", \"BTC\", \"ETH\"]\n",
    "ASSET2IDX = {a: i for i, a in enumerate(ASSETS)}\n",
    "TARGET_ASSET = \"ETH\"\n",
    "TARGET_NODE = ASSET2IDX[TARGET_ASSET]\n",
    "\n",
    "\n",
    "def build_edge_list(cfg: Dict[str, Any], assets: List[str]) -> List[Tuple[str, str]]:\n",
    "    mode = str(cfg.get(\"edges_mode\", \"manual\"))\n",
    "    if mode == \"manual\":\n",
    "        edges = list(cfg[\"edges\"])\n",
    "    elif mode == \"all_pairs\":\n",
    "        edges = [(s, t) for s in assets for t in assets if s != t]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown edges_mode={mode}\")\n",
    "\n",
    "    if bool(cfg.get(\"add_self_loops\", True)):\n",
    "        edges = edges + [(a, a) for a in assets]\n",
    "    return edges\n",
    "\n",
    "\n",
    "EDGE_LIST = build_edge_list(CFG, ASSETS)\n",
    "EDGE_NAMES = [f\"{s}->{t}\" for s, t in EDGE_LIST]\n",
    "EDGE_INDEX = torch.tensor([[ASSET2IDX[s], ASSET2IDX[t]] for (s, t) in EDGE_LIST], dtype=torch.long)\n",
    "\n",
    "print(\"EDGE_LIST:\", EDGE_NAMES)\n",
    "print(\"EDGE_INDEX:\", EDGE_INDEX.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded df: (12831, 106)\n",
      "Columns example: ['timestamp', 'ADA', 'spread_ADA', 'buys_ADA', 'sells_ADA', 'bids_vol_ADA_0', 'bids_vol_ADA_1', 'bids_vol_ADA_2', 'bids_vol_ADA_3', 'bids_vol_ADA_4', 'bids_vol_ADA_5', 'bids_vol_ADA_6', 'bids_vol_ADA_7', 'bids_vol_ADA_8', 'bids_vol_ADA_9', 'bids_vol_ADA_10', 'bids_vol_ADA_11', 'bids_vol_ADA_12', 'bids_vol_ADA_13', 'bids_vol_ADA_14']\n",
      "Time range: 2021-04-07 11:34:00+00:00 -> 2021-04-16 10:15:00+00:00\n",
      "                  timestamp      ADA  spread_ADA      buys_ADA      sells_ADA  \\\n",
      "0 2021-04-07 11:34:00+00:00  1.16205      0.0001  56936.467913  258248.957367   \n",
      "1 2021-04-07 11:35:00+00:00  1.16800      0.0022  56491.336799   78665.286640   \n",
      "\n",
      "   bids_vol_ADA_0  bids_vol_ADA_1  bids_vol_ADA_2  bids_vol_ADA_3  \\\n",
      "0      876.869995     5984.169922        5.810000       18.240000   \n",
      "1    33769.671875    23137.169922      550.299988      550.299988   \n",
      "\n",
      "   bids_vol_ADA_4  ...  asks_vol_ETH_8  asks_vol_ETH_9  asks_vol_ETH_10  \\\n",
      "0    19844.640625  ...      373.700012      196.699997      2059.709961   \n",
      "1    19012.320312  ...     3873.709961     1954.630005       197.039993   \n",
      "\n",
      "   asks_vol_ETH_11  asks_vol_ETH_12  asks_vol_ETH_13  asks_vol_ETH_14  \\\n",
      "0      3874.989990      5901.209961       178.289993     28512.160156   \n",
      "1     12661.990234     20006.970703     28562.310547      3874.379883   \n",
      "\n",
      "     lr_ADA    lr_BTC    lr_ETH  \n",
      "0  0.000000  0.000000  0.000000  \n",
      "1  0.005107  0.000937  0.001931  \n",
      "\n",
      "[2 rows x 106 columns]\n"
     ]
    }
   ],
   "source": [
    "# Step 1 — Load data + log returns\n",
    "\n",
    "def load_asset(asset: str, freq: str, data_dir: Path, book_levels: int, part: Tuple[int, int] = (0, 80)) -> pd.DataFrame:\n",
    "    path = data_dir / f\"{asset}_{freq}.csv\"\n",
    "    df_ = pd.read_csv(path)\n",
    "    df_ = df_.iloc[int(len(df_) * part[0] / 100): int(len(df_) * part[1] / 100)]\n",
    "\n",
    "    df_[\"timestamp\"] = pd.to_datetime(df_[\"system_time\"]).dt.round(\"min\")\n",
    "    df_ = df_.sort_values(\"timestamp\").set_index(\"timestamp\")\n",
    "\n",
    "    bid_cols = [f\"bids_notional_{i}\" for i in range(book_levels)]\n",
    "    ask_cols = [f\"asks_notional_{i}\" for i in range(book_levels)]\n",
    "\n",
    "    needed = [\"midpoint\", \"spread\", \"buys\", \"sells\"] + bid_cols + ask_cols\n",
    "    missing = [c for c in needed if c not in df_.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"{asset}: missing columns in CSV: {missing[:10]}{'...' if len(missing) > 10 else ''}\")\n",
    "\n",
    "    return df_[needed]\n",
    "\n",
    "\n",
    "def load_all_assets() -> pd.DataFrame:\n",
    "    freq = CFG[\"freq\"]\n",
    "    data_dir = CFG[\"data_dir\"]\n",
    "    book_levels = CFG[\"book_levels\"]\n",
    "\n",
    "    def rename_cols(df_one: pd.DataFrame, asset: str) -> pd.DataFrame:\n",
    "        rename_map = {\n",
    "            \"midpoint\": asset,\n",
    "            \"buys\": f\"buys_{asset}\",\n",
    "            \"sells\": f\"sells_{asset}\",\n",
    "            \"spread\": f\"spread_{asset}\",\n",
    "        }\n",
    "        for i in range(book_levels):\n",
    "            rename_map[f\"bids_notional_{i}\"] = f\"bids_vol_{asset}_{i}\"\n",
    "            rename_map[f\"asks_notional_{i}\"] = f\"asks_vol_{asset}_{i}\"\n",
    "        return df_one.rename(columns=rename_map)\n",
    "\n",
    "    df_ada = rename_cols(load_asset(\"ADA\", freq, data_dir, book_levels, part=(0, 75)), \"ADA\")\n",
    "    df_btc = rename_cols(load_asset(\"BTC\", freq, data_dir, book_levels, part=(0, 75)), \"BTC\")\n",
    "    df_eth = rename_cols(load_asset(\"ETH\", freq, data_dir, book_levels, part=(0, 75)), \"ETH\")\n",
    "\n",
    "    df_ = df_ada.join(df_btc).join(df_eth).reset_index()\n",
    "    return df_\n",
    "\n",
    "\n",
    "df = load_all_assets()\n",
    "for a in ASSETS:\n",
    "    df[f\"lr_{a}\"] = np.log(df[a]).diff().fillna(0.0)\n",
    "\n",
    "print(\"Loaded df:\", df.shape)\n",
    "print(\"Columns example:\", df.columns[:20].tolist())\n",
    "print(\"Time range:\", df[\"timestamp\"].min(), \"->\", df[\"timestamp\"].max())\n",
    "print(df.head(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edge_feat shape: (12831, 9, 20) (T,E,edge_dim)\n",
      "edge_dim = 20  = windows * lags = 20\n",
      "Edge names: ['ADA->BTC', 'ADA->ETH', 'BTC->ADA', 'BTC->ETH', 'ETH->ADA', 'ETH->BTC', 'ADA->ADA', 'BTC->BTC', 'ETH->ETH']\n",
      "edge_feat sample [t=100, first 3 edges]:\n",
      " [[ 6.7925054e-01  7.8185719e-01  8.3443433e-01  8.3443433e-01\n",
      "   8.3443433e-01  3.4026209e-01  1.4996846e-01  1.6863135e-01\n",
      "   1.6863135e-01  1.6863135e-01 -2.9266590e-02 -1.5999632e-01\n",
      "  -2.5908518e-01 -2.5908518e-01 -2.5908518e-01  1.4277337e-01\n",
      "  -1.0870378e-02  5.0888385e-04  5.0888385e-04  5.0888385e-04]\n",
      " [ 6.3383293e-01  6.9067067e-01  8.7368768e-01  8.7368768e-01\n",
      "   8.7368768e-01  3.4575835e-01  1.5627505e-01  2.0534241e-01\n",
      "   2.0534241e-01  2.0534241e-01  6.9384493e-02 -1.1163518e-01\n",
      "  -1.7551416e-01 -1.7551416e-01 -1.7551416e-01 -1.6881377e-01\n",
      "  -1.0781832e-01 -6.3380465e-02 -6.3380465e-02 -6.3380465e-02]\n",
      " [ 6.7925054e-01  7.8185719e-01  8.3443433e-01  8.3443433e-01\n",
      "   8.3443433e-01 -4.8168253e-02 -1.6241662e-01 -1.6193953e-01\n",
      "  -1.6193953e-01 -1.6193953e-01 -2.1278685e-01  4.6374347e-02\n",
      "  -1.7361922e-02 -1.7361922e-02 -1.7361922e-02  1.3684873e-01\n",
      "   3.6141057e-02  6.9459870e-02  6.9459870e-02  6.9459870e-02]]\n",
      "edge_feat stats: mean= 0.4511896073818207 std= 0.500860333442688\n"
     ]
    }
   ],
   "source": [
    "# Step 2 — Multi-window correlations → edge features (T,E,D)\n",
    "\n",
    "def _fisher_z(x: np.ndarray, eps: float = 1e-6) -> np.ndarray:\n",
    "    x = np.clip(x, -0.999, 0.999)\n",
    "    return 0.5 * np.log((1.0 + x + eps) / (1.0 - x + eps))\n",
    "\n",
    "\n",
    "def build_corr_array(\n",
    "    df_: pd.DataFrame,\n",
    "    corr_windows: List[int],\n",
    "    edges: List[Tuple[str, str]],\n",
    "    lags: List[int],\n",
    "    transform: str = \"fisher\",\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Edge features per time:\n",
    "      for edge s->t:\n",
    "        for lag in lags:\n",
    "          corr(lr_s.shift(lag), lr_t) over rolling window\n",
    "\n",
    "    No leakage: shift(lag>0) uses past of source.\n",
    "    Self-loop edges a->a: set constant 1.0.\n",
    "    \"\"\"\n",
    "    T_ = len(df_)\n",
    "    E_ = len(edges)\n",
    "    W_ = len(corr_windows)\n",
    "    Lg = len(lags)\n",
    "    out = np.zeros((T_, E_, W_ * Lg), dtype=np.float32)\n",
    "\n",
    "    lr_map = {a: df_[f\"lr_{a}\"].astype(float) for a in ASSETS}\n",
    "\n",
    "    for ei, (s, t) in enumerate(edges):\n",
    "        if s == t:\n",
    "            out[:, ei, :] = 1.0\n",
    "            continue\n",
    "\n",
    "        src0 = lr_map[s]\n",
    "        dst0 = lr_map[t]\n",
    "\n",
    "        feat_idx = 0\n",
    "        for lag in lags:\n",
    "            src = src0.shift(int(lag)) if int(lag) > 0 else src0\n",
    "\n",
    "            for w in corr_windows:\n",
    "                r = src.rolling(int(w), min_periods=1).corr(dst0)\n",
    "                r = np.nan_to_num(r.to_numpy(dtype=np.float32), nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "                if transform == \"fisher\":\n",
    "                    r = _fisher_z(r).astype(np.float32)\n",
    "\n",
    "                out[:, ei, feat_idx] = r\n",
    "                feat_idx += 1\n",
    "\n",
    "    return out.astype(np.float32)\n",
    "\n",
    "\n",
    "edge_feat = build_corr_array(\n",
    "    df,\n",
    "    CFG[\"corr_windows\"],\n",
    "    EDGE_LIST,\n",
    "    CFG[\"corr_lags\"],\n",
    "    transform=str(CFG.get(\"edge_transform\", \"fisher\")),\n",
    ")\n",
    "\n",
    "print(\"edge_feat shape:\", edge_feat.shape, \"(T,E,edge_dim)\")\n",
    "print(\"edge_dim =\", edge_feat.shape[-1], \" = windows * lags =\", len(CFG[\"corr_windows\"]) * len(CFG[\"corr_lags\"]))\n",
    "print(\"Edge names:\", EDGE_NAMES)\n",
    "print(\"edge_feat sample [t=100, first 3 edges]:\\n\", edge_feat[100, :3, :])\n",
    "print(\"edge_feat stats: mean=\", float(edge_feat.mean()), \"std=\", float(edge_feat.std()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TB dist [down,flat,up]: [2875 7413 2543]\n",
      "Trade ratio (true): 0.42225859247135844\n"
     ]
    }
   ],
   "source": [
    "# Step 3 — Triple-barrier labels → two-stage labels + exit_ret\n",
    "\n",
    "def triple_barrier_labels_from_lr(\n",
    "    lr: pd.Series,\n",
    "    horizon: int,\n",
    "    vol_window: int,\n",
    "    pt_mult: float,\n",
    "    sl_mult: float,\n",
    "    min_barrier: float,\n",
    "    max_barrier: float,\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      y_tb: {0=down, 1=flat/no-trade, 2=up}\n",
    "      exit_ret: realized log-return to exit (tp/sl/timeout)\n",
    "      exit_t: exit index\n",
    "      thr: barrier per t (float array, len T)\n",
    "    No leakage: vol is shift(1).\n",
    "    \"\"\"\n",
    "    lr = lr.astype(float).copy()\n",
    "    T_ = len(lr)\n",
    "\n",
    "    vol = lr.rolling(vol_window, min_periods=max(10, vol_window // 10)).std().shift(1)\n",
    "    thr = (vol * np.sqrt(horizon)).clip(lower=min_barrier, upper=max_barrier)\n",
    "\n",
    "    y = np.ones(T_, dtype=np.int64)\n",
    "    exit_ret = np.zeros(T_, dtype=np.float32)\n",
    "    exit_t = np.arange(T_, dtype=np.int64)\n",
    "\n",
    "    lr_np = lr.fillna(0.0).to_numpy(dtype=np.float64)\n",
    "    thr_np = thr.fillna(min_barrier).to_numpy(dtype=np.float64)\n",
    "\n",
    "    for t in range(T_ - horizon - 1):\n",
    "        up = pt_mult * thr_np[t]\n",
    "        dn = -sl_mult * thr_np[t]\n",
    "\n",
    "        cum = 0.0\n",
    "        hit = 1\n",
    "        et = t + horizon\n",
    "        er = 0.0\n",
    "\n",
    "        for dt in range(1, horizon + 1):\n",
    "            cum += lr_np[t + dt]\n",
    "            if cum >= up:\n",
    "                hit, et, er = 2, t + dt, cum\n",
    "                break\n",
    "            if cum <= dn:\n",
    "                hit, et, er = 0, t + dt, cum\n",
    "                break\n",
    "\n",
    "        if hit == 1:\n",
    "            er = float(np.sum(lr_np[t + 1: t + horizon + 1]))\n",
    "            et = t + horizon\n",
    "\n",
    "        y[t] = hit\n",
    "        exit_ret[t] = er\n",
    "        exit_t[t] = et\n",
    "\n",
    "    return y, exit_ret, exit_t, thr_np\n",
    "\n",
    "\n",
    "y_tb, exit_ret, exit_t, tb_thr = triple_barrier_labels_from_lr(\n",
    "    df[\"lr_ETH\"],\n",
    "    horizon=CFG[\"tb_horizon\"],\n",
    "    vol_window=CFG[\"lookback\"],\n",
    "    pt_mult=CFG[\"tb_pt_mult\"],\n",
    "    sl_mult=CFG[\"tb_sl_mult\"],\n",
    "    min_barrier=CFG[\"tb_min_barrier\"],\n",
    "    max_barrier=CFG[\"tb_max_barrier\"],\n",
    ")\n",
    "\n",
    "# two-stage labels\n",
    "y_trade = (y_tb != 1).astype(np.int64)   # 1=trade, 0=no-trade\n",
    "y_dir = (y_tb == 2).astype(np.int64)     # 1=up, 0=down (meaningful only when y_trade==1)\n",
    "\n",
    "dist = np.bincount(y_tb, minlength=3)\n",
    "print(\"TB dist [down,flat,up]:\", dist)\n",
    "print(\"Trade ratio (true):\", float(y_trade.mean()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_node_raw: (12831, 3, 15) edge_feat: (12831, 9, 20)\n",
      "node_feat_names: ['lr', 'spread', 'log_buys', 'log_sells', 'ofi', 'DI_15', 'DI_L0', 'DI_L1', 'DI_L2', 'DI_L3', 'DI_L4', 'near_ratio_bid', 'near_ratio_ask', 'di_near', 'di_far']\n",
      "n_samples: 12561 | t range: 239 -> 12799\n",
      "Feature stats (TARGET asset, lr): mean= 1.5748046280350536e-05 std= 0.0010532913729548454\n"
     ]
    }
   ],
   "source": [
    "# Step 4 — Build node tensor (T,N,F) + sample_t (valid indices in sample-space)\n",
    "\n",
    "EPS = 1e-6\n",
    "\n",
    "\n",
    "def safe_log1p(x: np.ndarray) -> np.ndarray:\n",
    "    return np.log1p(np.maximum(x, 0.0))\n",
    "\n",
    "\n",
    "def build_node_tensor(df_: pd.DataFrame) -> Tuple[np.ndarray, List[str]]:\n",
    "    \"\"\"\n",
    "    Features per asset:\n",
    "      lr, spread,\n",
    "      log_buys, log_sells, ofi,\n",
    "      DI_15,\n",
    "      DI_L0..DI_L4,\n",
    "      near_ratio_bid, near_ratio_ask,\n",
    "      di_near, di_far\n",
    "    \"\"\"\n",
    "    book_levels = CFG[\"book_levels\"]\n",
    "    top_k = CFG[\"top_levels\"]\n",
    "    near_k = CFG[\"near_levels\"]\n",
    "\n",
    "    if near_k >= book_levels:\n",
    "        raise ValueError(\"CFG['near_levels'] must be < CFG['book_levels']\")\n",
    "\n",
    "    feat_names = [\n",
    "        \"lr\", \"spread\",\n",
    "        \"log_buys\", \"log_sells\", \"ofi\",\n",
    "        \"DI_15\",\n",
    "        \"DI_L0\", \"DI_L1\", \"DI_L2\", \"DI_L3\", \"DI_L4\",\n",
    "        \"near_ratio_bid\", \"near_ratio_ask\",\n",
    "        \"di_near\", \"di_far\",\n",
    "    ]\n",
    "\n",
    "    feats_all = []\n",
    "    for a in ASSETS:\n",
    "        lr = df_[f\"lr_{a}\"].values.astype(np.float32)\n",
    "        spread = df_[f\"spread_{a}\"].values.astype(np.float32)\n",
    "\n",
    "        buys = df_[f\"buys_{a}\"].values.astype(np.float32)\n",
    "        sells = df_[f\"sells_{a}\"].values.astype(np.float32)\n",
    "\n",
    "        log_buys = safe_log1p(buys).astype(np.float32)\n",
    "        log_sells = safe_log1p(sells).astype(np.float32)\n",
    "\n",
    "        ofi = ((buys - sells) / (buys + sells + EPS)).astype(np.float32)\n",
    "\n",
    "        bids_lvls = np.stack([df_[f\"bids_vol_{a}_{i}\"].values.astype(np.float32) for i in range(book_levels)], axis=1)\n",
    "        asks_lvls = np.stack([df_[f\"asks_vol_{a}_{i}\"].values.astype(np.float32) for i in range(book_levels)], axis=1)\n",
    "\n",
    "        bid_sum = bids_lvls.sum(axis=1)\n",
    "        ask_sum = asks_lvls.sum(axis=1)\n",
    "        di_15 = ((bid_sum - ask_sum) / (bid_sum + ask_sum + EPS)).astype(np.float32)\n",
    "\n",
    "        di_levels = []\n",
    "        for i in range(top_k):\n",
    "            b = bids_lvls[:, i]\n",
    "            s = asks_lvls[:, i]\n",
    "            di_levels.append(((b - s) / (b + s + EPS)).astype(np.float32))\n",
    "        di_l0_4 = np.stack(di_levels, axis=1)  # (T,5)\n",
    "\n",
    "        bid_near = bids_lvls[:, :near_k].sum(axis=1)\n",
    "        ask_near = asks_lvls[:, :near_k].sum(axis=1)\n",
    "        bid_far = bids_lvls[:, near_k:].sum(axis=1)\n",
    "        ask_far = asks_lvls[:, near_k:].sum(axis=1)\n",
    "\n",
    "        near_ratio_bid = (bid_near / (bid_far + EPS)).astype(np.float32)\n",
    "        near_ratio_ask = (ask_near / (ask_far + EPS)).astype(np.float32)\n",
    "\n",
    "        di_near = ((bid_near - ask_near) / (bid_near + ask_near + EPS)).astype(np.float32)\n",
    "        di_far = ((bid_far - ask_far) / (bid_far + ask_far + EPS)).astype(np.float32)\n",
    "\n",
    "        Xa = np.column_stack([\n",
    "            lr, spread,\n",
    "            log_buys, log_sells, ofi,\n",
    "            di_15,\n",
    "            di_l0_4[:, 0], di_l0_4[:, 1], di_l0_4[:, 2], di_l0_4[:, 3], di_l0_4[:, 4],\n",
    "            near_ratio_bid, near_ratio_ask,\n",
    "            di_near, di_far,\n",
    "        ]).astype(np.float32)\n",
    "\n",
    "        feats_all.append(Xa)\n",
    "\n",
    "    X = np.stack(feats_all, axis=1).astype(np.float32)  # (T,N,F)\n",
    "    return X, feat_names\n",
    "\n",
    "\n",
    "X_node_raw, node_feat_names = build_node_tensor(df)\n",
    "T = len(df)\n",
    "L = CFG[\"lookback\"]\n",
    "H = CFG[\"tb_horizon\"]\n",
    "\n",
    "t_min = L - 1\n",
    "t_max = T - H - 2\n",
    "sample_t = np.arange(t_min, t_max + 1)\n",
    "n_samples = len(sample_t)\n",
    "\n",
    "print(\"X_node_raw:\", X_node_raw.shape, \"edge_feat:\", edge_feat.shape)\n",
    "print(\"node_feat_names:\", node_feat_names)\n",
    "print(\"n_samples:\", n_samples, \"| t range:\", int(sample_t[0]), \"->\", int(sample_t[-1]))\n",
    "print(\"Feature stats (TARGET asset, lr):\",\n",
    "      \"mean=\", float(X_node_raw[:, TARGET_NODE, node_feat_names.index(\"lr\")].mean()),\n",
    "      \"std=\", float(X_node_raw[:, TARGET_NODE, node_feat_names.index(\"lr\")].std()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holdout split:\n",
      "  n_samples total: 12561\n",
      "  n_samples CV   : 11305 (90.0%)\n",
      "  n_samples FINAL: 1256 (10.0%)\n",
      "  CV range   : 0 11304\n",
      "  FINAL range: 11305 12560\n",
      "\n",
      "Walk-forward folds: 4\n",
      "  fold 1: train=5652 | val=1130 | test=1130\n",
      "  fold 2: train=6782 | val=1130 | test=1130\n",
      "  fold 3: train=7912 | val=1130 | test=1130\n",
      "  fold 4: train=9042 | val=1130 | test=1130\n"
     ]
    }
   ],
   "source": [
    "# Step 5 — Final holdout split (time-ordered) + walk-forward splits (CV-part only)\n",
    "\n",
    "def make_final_holdout_split(n_samples_: int, final_test_frac: float) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    if not (0.0 < final_test_frac < 0.5):\n",
    "        raise ValueError(\"final_test_frac should be in (0, 0.5)\")\n",
    "    n_final = max(1, int(round(final_test_frac * n_samples_)))\n",
    "    n_cv = n_samples_ - n_final\n",
    "    if n_cv <= 50:\n",
    "        raise ValueError(\"Too few samples left for CV after holdout split.\")\n",
    "    idx_cv = np.arange(0, n_cv, dtype=np.int64)\n",
    "    idx_final = np.arange(n_cv, n_samples_, dtype=np.int64)\n",
    "    return idx_cv, idx_final\n",
    "\n",
    "\n",
    "def make_walk_forward_splits(\n",
    "    n_samples_: int,\n",
    "    train_min_frac: float,\n",
    "    val_window_frac: float,\n",
    "    test_window_frac: float,\n",
    "    step_window_frac: float,\n",
    ") -> List[Tuple[np.ndarray, np.ndarray, np.ndarray]]:\n",
    "    train_min = int(train_min_frac * n_samples_)\n",
    "    val_w = max(1, int(val_window_frac * n_samples_))\n",
    "    test_w = max(1, int(test_window_frac * n_samples_))\n",
    "    step_w = max(1, int(step_window_frac * n_samples_))\n",
    "\n",
    "    splits = []\n",
    "    start = train_min\n",
    "    while True:\n",
    "        tr_end = start\n",
    "        va_end = tr_end + val_w\n",
    "        te_end = va_end + test_w\n",
    "        if te_end > n_samples_:\n",
    "            break\n",
    "\n",
    "        idx_train = np.arange(0, tr_end, dtype=np.int64)\n",
    "        idx_val = np.arange(tr_end, va_end, dtype=np.int64)\n",
    "        idx_test = np.arange(va_end, te_end, dtype=np.int64)\n",
    "        splits.append((idx_train, idx_val, idx_test))\n",
    "\n",
    "        start += step_w\n",
    "\n",
    "    return splits\n",
    "\n",
    "\n",
    "idx_cv_all, idx_final_test = make_final_holdout_split(n_samples, CFG[\"final_test_frac\"])\n",
    "n_samples_cv = len(idx_cv_all)\n",
    "n_samples_final = len(idx_final_test)\n",
    "\n",
    "print(\"Holdout split:\")\n",
    "print(f\"  n_samples total: {n_samples}\")\n",
    "print(f\"  n_samples CV   : {n_samples_cv} ({100 * n_samples_cv / n_samples:.1f}%)\")\n",
    "print(f\"  n_samples FINAL: {n_samples_final} ({100 * n_samples_final / n_samples:.1f}%)\")\n",
    "print(\"  CV range   :\", int(idx_cv_all[0]), int(idx_cv_all[-1]))\n",
    "print(\"  FINAL range:\", int(idx_final_test[0]), int(idx_final_test[-1]))\n",
    "\n",
    "walk_splits = make_walk_forward_splits(\n",
    "    n_samples_=n_samples_cv,\n",
    "    train_min_frac=CFG[\"train_min_frac\"],\n",
    "    val_window_frac=CFG[\"val_window_frac\"],\n",
    "    test_window_frac=CFG[\"test_window_frac\"],\n",
    "    step_window_frac=CFG[\"step_window_frac\"],\n",
    ")\n",
    "\n",
    "print(\"\\nWalk-forward folds:\", len(walk_splits))\n",
    "for i, (a, b, c) in enumerate(walk_splits, 1):\n",
    "    print(f\"  fold {i}: train={len(a)} | val={len(b)} | test={len(c)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6 — Dataset + scaling (train-only) + helpers\n",
    "\n",
    "class LobGraphSequenceDataset2Stage(Dataset):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      x_seq: (L,N,F)\n",
    "      e_seq: (L,E,edge_dim)\n",
    "      y_trade: scalar\n",
    "      y_dir: scalar\n",
    "      exit_ret: scalar\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        X_node: np.ndarray,\n",
    "        E_feat: np.ndarray,\n",
    "        y_trade_arr: np.ndarray,\n",
    "        y_dir_arr: np.ndarray,\n",
    "        exit_ret_arr: np.ndarray,\n",
    "        sample_t_: np.ndarray,\n",
    "        indices: np.ndarray,\n",
    "        lookback: int,\n",
    "    ):\n",
    "        self.X_node = X_node\n",
    "        self.E_feat = E_feat\n",
    "        self.y_trade = y_trade_arr\n",
    "        self.y_dir = y_dir_arr\n",
    "        self.exit_ret = exit_ret_arr\n",
    "        self.sample_t = sample_t_\n",
    "        self.indices = indices.astype(np.int64)\n",
    "        self.L = int(lookback)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return int(len(self.indices))\n",
    "\n",
    "    def __getitem__(self, i: int):\n",
    "        sidx = int(self.indices[i])\n",
    "        t = int(self.sample_t[sidx])\n",
    "        t0 = t - self.L + 1\n",
    "\n",
    "        x_seq = self.X_node[t0:t + 1]  # (L,N,F)\n",
    "        e_seq = self.E_feat[t0:t + 1]  # (L,E,edge_dim)\n",
    "\n",
    "        yt = int(self.y_trade[t])\n",
    "        yd = int(self.y_dir[t])\n",
    "        er = float(self.exit_ret[t])\n",
    "\n",
    "        return (\n",
    "            torch.from_numpy(x_seq),\n",
    "            torch.from_numpy(e_seq),\n",
    "            torch.tensor(yt, dtype=torch.long),\n",
    "            torch.tensor(yd, dtype=torch.long),\n",
    "            torch.tensor(er, dtype=torch.float32),\n",
    "        )\n",
    "\n",
    "\n",
    "def collate_fn_2stage(batch):\n",
    "    xs, es, yts, yds, ers = zip(*batch)\n",
    "    return (\n",
    "        torch.stack(xs, 0),   # (B,L,N,F)\n",
    "        torch.stack(es, 0),   # (B,L,E,edge_dim)\n",
    "        torch.stack(yts, 0),  # (B,)\n",
    "        torch.stack(yds, 0),  # (B,)\n",
    "        torch.stack(ers, 0),  # (B,)\n",
    "    )\n",
    "\n",
    "\n",
    "def fit_scale_nodes_train_only(\n",
    "    X_node_raw_: np.ndarray,\n",
    "    sample_t_: np.ndarray,\n",
    "    idx_train: np.ndarray,\n",
    "    max_abs: float = 10.0\n",
    ") -> Tuple[np.ndarray, RobustScaler]:\n",
    "    last_train_t = int(sample_t_[int(idx_train[-1])])\n",
    "    train_time_mask = np.arange(0, last_train_t + 1)\n",
    "\n",
    "    X_train_time = X_node_raw_[train_time_mask]  # (Ttr,N,F)\n",
    "    _, _, Fdim = X_train_time.shape\n",
    "\n",
    "    scaler = RobustScaler(with_centering=True, with_scaling=True, quantile_range=(5.0, 95.0))\n",
    "    scaler.fit(X_train_time.reshape(-1, Fdim))\n",
    "\n",
    "    X_scaled = scaler.transform(X_node_raw_.reshape(-1, Fdim)).reshape(X_node_raw_.shape).astype(np.float32)\n",
    "    X_scaled = np.clip(X_scaled, -max_abs, max_abs).astype(np.float32)\n",
    "    X_scaled = np.nan_to_num(X_scaled, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n",
    "    return X_scaled, scaler\n",
    "\n",
    "\n",
    "def fit_scale_edges_train_only(\n",
    "    E_raw_: np.ndarray,\n",
    "    sample_t_: np.ndarray,\n",
    "    idx_train: np.ndarray,\n",
    "    max_abs: float = 6.0\n",
    ") -> Tuple[np.ndarray, RobustScaler]:\n",
    "    \"\"\"\n",
    "    Robust-scale edge features per fold (train timeline only).\n",
    "    Important because fisher-transformed correlations can have long tails.\n",
    "    \"\"\"\n",
    "    last_train_t = int(sample_t_[int(idx_train[-1])])\n",
    "    train_time_mask = np.arange(0, last_train_t + 1)\n",
    "\n",
    "    E_train_time = E_raw_[train_time_mask]  # (Ttr,E,D)\n",
    "    _, _, D = E_train_time.shape\n",
    "\n",
    "    scaler = RobustScaler(with_centering=True, with_scaling=True, quantile_range=(5.0, 95.0))\n",
    "    scaler.fit(E_train_time.reshape(-1, D))\n",
    "\n",
    "    E_scaled = scaler.transform(E_raw_.reshape(-1, D)).reshape(E_raw_.shape).astype(np.float32)\n",
    "    E_scaled = np.clip(E_scaled, -max_abs, max_abs).astype(np.float32)\n",
    "    E_scaled = np.nan_to_num(E_scaled, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n",
    "    return E_scaled, scaler\n",
    "\n",
    "\n",
    "def subset_trade_indices(indices: np.ndarray, sample_t_: np.ndarray, y_trade_arr: np.ndarray) -> np.ndarray:\n",
    "    tt = sample_t_[indices]\n",
    "    mask = (y_trade_arr[tt] == 1)\n",
    "    return indices[mask]\n",
    "\n",
    "\n",
    "def split_trade_ratio(indices: np.ndarray, sample_t_: np.ndarray, y_trade_arr: np.ndarray) -> float:\n",
    "    tt = sample_t_[indices]\n",
    "    return float(y_trade_arr[tt].mean()) if len(tt) else float(\"nan\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model sanity logits: torch.Size([2, 2]) | finite: True\n"
     ]
    }
   ],
   "source": [
    "# Step 7 — Model (SGA-TCN) — drop-in logits (B,2)\n",
    "\n",
    "class SpatialGraphAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Attention with edge_attr in attention scorer:\n",
    "      score_e = a^T [h_src || h_dst || edge_emb]\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim: int, out_dim: int, edge_dim: int, heads: int = 1, dropout: float = 0.1, edge_dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.in_dim = int(in_dim)\n",
    "        self.out_dim = int(out_dim)\n",
    "        self.heads = max(1, int(heads))\n",
    "        self.dropout = float(dropout)\n",
    "        self.edge_dropout = float(edge_dropout)\n",
    "\n",
    "        self.head_dim = max(1, int(math.ceil(out_dim / self.heads)))\n",
    "        self.inner_dim = self.heads * self.head_dim\n",
    "\n",
    "        self.lin_node = nn.Linear(self.in_dim, self.inner_dim, bias=False)\n",
    "        self.lin_edge = nn.Linear(edge_dim, self.inner_dim, bias=False)\n",
    "        self.lin_msg = nn.Linear(self.inner_dim, self.inner_dim, bias=False)\n",
    "\n",
    "        self.attn_vec = nn.Parameter(torch.empty(self.heads, 3 * self.head_dim))\n",
    "\n",
    "        self.out_proj = nn.Linear(self.inner_dim, self.out_dim, bias=False)\n",
    "        self.res_proj = nn.Identity() if self.in_dim == self.out_dim else nn.Linear(self.in_dim, self.out_dim, bias=False)\n",
    "\n",
    "        self.ln = nn.LayerNorm(self.out_dim)\n",
    "        self.attn_drop = nn.Dropout(self.dropout)\n",
    "        self.out_drop = nn.Dropout(self.dropout)\n",
    "        self.act = nn.LeakyReLU(0.2)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        for m in [self.lin_node, self.lin_edge, self.lin_msg, self.out_proj]:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "        if isinstance(self.res_proj, nn.Linear):\n",
    "            nn.init.xavier_uniform_(self.res_proj.weight)\n",
    "        nn.init.xavier_uniform_(self.attn_vec)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, edge_attr: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:\n",
    "        x = torch.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        edge_attr = torch.nan_to_num(edge_attr, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "        if self.training and self.edge_dropout > 0:\n",
    "            edge_attr = F.dropout(edge_attr, p=self.edge_dropout, training=True)\n",
    "\n",
    "        B, N, _ = x.shape\n",
    "        E = edge_index.shape[0]\n",
    "\n",
    "        src_idx = edge_index[:, 0]\n",
    "        dst_idx = edge_index[:, 1]\n",
    "\n",
    "        h = self.lin_node(x).view(B, N, self.heads, self.head_dim)                 # (B,N,Hh,dh)\n",
    "        eemb = self.lin_edge(edge_attr).view(B, E, self.heads, self.head_dim)      # (B,E,Hh,dh)\n",
    "\n",
    "        h_src = h[:, src_idx, :, :]  # (B,E,Hh,dh)\n",
    "        h_dst = h[:, dst_idx, :, :]  # (B,E,Hh,dh)\n",
    "\n",
    "        cat = torch.cat([h_src, h_dst, eemb], dim=-1)                               # (B,E,Hh,3*dh)\n",
    "        scores = (cat * self.attn_vec[None, None, :, :]).sum(dim=-1)               # (B,E,Hh)\n",
    "        scores = self.act(scores)\n",
    "\n",
    "        # softmax per dst-node\n",
    "        alphas = torch.zeros_like(scores)\n",
    "        for n in range(N):\n",
    "            mask = (dst_idx == n)\n",
    "            if int(mask.sum()) == 0:\n",
    "                continue\n",
    "            s = scores[:, mask, :]\n",
    "            a = torch.softmax(s, dim=1)\n",
    "            a = self.attn_drop(a)\n",
    "            alphas[:, mask, :] = a\n",
    "\n",
    "        msg = self.lin_msg(h_src.reshape(B, E, self.inner_dim)).view(B, E, self.heads, self.head_dim)\n",
    "\n",
    "        agg = torch.zeros((B, N, self.heads, self.head_dim), device=x.device, dtype=x.dtype)\n",
    "        for e_i in range(E):\n",
    "            dst = int(dst_idx[e_i].item())\n",
    "            agg[:, dst, :, :] += alphas[:, e_i, :].unsqueeze(-1) * msg[:, e_i, :, :]\n",
    "\n",
    "        out = agg.reshape(B, N, self.inner_dim)\n",
    "        out = self.out_proj(out)\n",
    "        out = self.out_drop(out)\n",
    "\n",
    "        res = self.res_proj(x)\n",
    "        y = self.ln(res + out)\n",
    "        return torch.nan_to_num(y, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "\n",
    "class SpatialGraphAttentionMP(nn.Module):\n",
    "    \"\"\"Applies SpatialGraphAttentionLayer independently at each timestep.\"\"\"\n",
    "    def __init__(self, in_dim: int, hidden: int, edge_dim: int, heads: int, dropout: float, edge_dropout: float):\n",
    "        super().__init__()\n",
    "        self.gat = SpatialGraphAttentionLayer(\n",
    "            in_dim=in_dim, out_dim=hidden, edge_dim=edge_dim,\n",
    "            heads=heads, dropout=dropout, edge_dropout=edge_dropout\n",
    "        )\n",
    "\n",
    "    def forward(self, x_seq: torch.Tensor, e_seq: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:\n",
    "        B, L_, N, F_ = x_seq.shape\n",
    "        x_flat = x_seq.reshape(B * L_, N, F_)\n",
    "        e_flat = e_seq.reshape(B * L_, e_seq.size(2), e_seq.size(3))\n",
    "        h_flat = self.gat(x_flat, e_flat, edge_index)  # (B*L,N,H)\n",
    "        return h_flat.reshape(B, L_, N, -1)\n",
    "\n",
    "\n",
    "class CausalConv1d(nn.Module):\n",
    "    def __init__(self, in_ch: int, out_ch: int, kernel_size: int, dilation: int = 1):\n",
    "        super().__init__()\n",
    "        self.kernel_size = int(kernel_size)\n",
    "        self.dilation = int(dilation)\n",
    "        self.conv = nn.Conv1d(in_ch, out_ch, kernel_size=self.kernel_size, dilation=self.dilation)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        pad_left = (self.kernel_size - 1) * self.dilation\n",
    "        x = F.pad(x, (pad_left, 0))\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, in_ch: int, out_ch: int, kernel_size: int, dilation: int, dropout: float, causal: bool = True):\n",
    "        super().__init__()\n",
    "        self.causal = bool(causal)\n",
    "\n",
    "        if self.causal:\n",
    "            self.conv1 = CausalConv1d(in_ch, out_ch, kernel_size, dilation=dilation)\n",
    "            self.conv2 = CausalConv1d(out_ch, out_ch, kernel_size, dilation=dilation)\n",
    "        else:\n",
    "            pad = ((kernel_size - 1) * dilation) // 2\n",
    "            self.conv1 = nn.Conv1d(in_ch, out_ch, kernel_size, dilation=dilation, padding=pad)\n",
    "            self.conv2 = nn.Conv1d(out_ch, out_ch, kernel_size, dilation=dilation, padding=pad)\n",
    "\n",
    "        self.act = nn.GELU()\n",
    "        self.drop = nn.Dropout(float(dropout))\n",
    "        self.downsample = nn.Identity() if in_ch == out_ch else nn.Conv1d(in_ch, out_ch, kernel_size=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = torch.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        y = self.drop(self.act(self.conv1(x)))\n",
    "        y = self.drop(self.act(self.conv2(y)))\n",
    "        res = self.downsample(x)\n",
    "        return torch.nan_to_num(self.act(y + res), nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "\n",
    "class TemporalConvNet(nn.Module):\n",
    "    def __init__(self, in_ch: int, channels: List[int], kernel_size: int, dropout: float, causal: bool = True):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        cur = int(in_ch)\n",
    "        for i, out_ch in enumerate(channels):\n",
    "            dilation = 2 ** i\n",
    "            layers.append(TemporalBlock(cur, int(out_ch), int(kernel_size), int(dilation), float(dropout), causal=causal))\n",
    "            cur = int(out_ch)\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class GNN_TCN_Classifier(nn.Module):\n",
    "    def __init__(self, node_in: int, edge_dim: int, cfg: Dict[str, Any], target_node: int, n_classes: int = 2):\n",
    "        super().__init__()\n",
    "        self.target_node = int(target_node)\n",
    "\n",
    "        hidden = int(cfg[\"hidden\"])\n",
    "        dropout = float(cfg[\"dropout\"])\n",
    "        edge_dropout = float(cfg.get(\"edge_dropout\", 0.0))\n",
    "\n",
    "        gat_heads = int(cfg[\"gat_heads\"])\n",
    "        tcn_channels = int(cfg[\"tcn_channels\"])\n",
    "        tcn_layers_n = int(cfg[\"tcn_layers\"])\n",
    "        tcn_kernel = int(cfg[\"tcn_kernel\"])\n",
    "        tcn_dropout = float(cfg[\"tcn_dropout\"])\n",
    "        tcn_causal = bool(cfg[\"tcn_causal\"])\n",
    "        self.tcn_pool = str(cfg[\"tcn_pool\"])\n",
    "\n",
    "        self.gnns = nn.ModuleList()\n",
    "        for i in range(int(cfg[\"gnn_layers\"])):\n",
    "            in_dim = int(node_in) if i == 0 else hidden\n",
    "            self.gnns.append(\n",
    "                SpatialGraphAttentionMP(\n",
    "                    in_dim=in_dim, hidden=hidden, edge_dim=int(edge_dim),\n",
    "                    heads=gat_heads, dropout=dropout, edge_dropout=edge_dropout\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.tcn_in = nn.Linear(hidden, tcn_channels)\n",
    "        self.tcn = TemporalConvNet(\n",
    "            in_ch=tcn_channels,\n",
    "            channels=[tcn_channels] * tcn_layers_n,\n",
    "            kernel_size=tcn_kernel,\n",
    "            dropout=tcn_dropout,\n",
    "            causal=tcn_causal,\n",
    "        )\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(tcn_channels),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(tcn_channels, tcn_channels),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(tcn_channels, n_classes),\n",
    "        )\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, e: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:\n",
    "        x = torch.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        e = torch.nan_to_num(e, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "        h = x\n",
    "        for li, gnn in enumerate(self.gnns):\n",
    "            h = gnn(h, e, edge_index)  # (B,L,N,H)\n",
    "            if li < len(self.gnns) - 1:\n",
    "                h = F.gelu(h)\n",
    "\n",
    "        h_tgt = h[:, :, self.target_node, :]  # (B,L,H)\n",
    "        z = self.tcn_in(h_tgt)                # (B,L,C)\n",
    "        z = z.transpose(1, 2)                 # (B,C,L)\n",
    "\n",
    "        y = self.tcn(z)                       # (B,C,L)\n",
    "        emb = y[:, :, -1] if self.tcn_pool == \"last\" else y.mean(dim=-1)\n",
    "        logits = self.head(emb)               # (B,2)\n",
    "        return torch.nan_to_num(logits, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "\n",
    "# sanity\n",
    "B_ = 2\n",
    "Fdim = X_node_raw.shape[-1]\n",
    "E_ = EDGE_INDEX.shape[0]\n",
    "Dedge = edge_feat.shape[-1]\n",
    "x_dummy = torch.randn(B_, L, len(ASSETS), Fdim)\n",
    "e_dummy = torch.randn(B_, L, E_, Dedge)\n",
    "m_dummy = GNN_TCN_Classifier(node_in=Fdim, edge_dim=Dedge, cfg=CFG, target_node=TARGET_NODE).to(DEVICE)\n",
    "with torch.no_grad():\n",
    "    out = m_dummy(x_dummy.to(DEVICE), e_dummy.to(DEVICE), EDGE_INDEX.to(DEVICE))\n",
    "print(\"Model sanity logits:\", out.shape, \"| finite:\", bool(torch.isfinite(out).all().item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8 — Train/Eval helpers (AUC-oriented)\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_binary(model: nn.Module, loader: DataLoader, loss_fn: nn.Module, y_key: str) -> Dict[str, Any]:\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    n = 0\n",
    "\n",
    "    ys = []\n",
    "    probs = []\n",
    "    ers = []\n",
    "\n",
    "    for x, e, y_trade_b, y_dir_b, er in loader:\n",
    "        x = x.to(DEVICE).float()\n",
    "        e = e.to(DEVICE).float()\n",
    "        y = (y_trade_b if y_key == \"trade\" else y_dir_b).to(DEVICE).long()\n",
    "\n",
    "        logits = model(x, e, EDGE_INDEX.to(DEVICE))\n",
    "        loss = loss_fn(logits, y)\n",
    "\n",
    "        total_loss += float(loss.item()) * int(y.size(0))\n",
    "        n += int(y.size(0))\n",
    "\n",
    "        p = torch.softmax(logits, dim=-1).detach().cpu().numpy()\n",
    "        ys.append(y.detach().cpu().numpy())\n",
    "        probs.append(p)\n",
    "        ers.append(er.detach().cpu().numpy())\n",
    "\n",
    "    ys = np.concatenate(ys) if ys else np.array([], dtype=np.int64)\n",
    "    probs = np.concatenate(probs) if probs else np.zeros((0, 2), dtype=np.float32)\n",
    "    ers = np.concatenate(ers) if ers else np.array([], dtype=np.float32)\n",
    "\n",
    "    if len(ys) == 0:\n",
    "        return {\"loss\": np.nan, \"acc\": np.nan, \"f1m\": np.nan, \"auc\": np.nan, \"cm\": None, \"y\": ys, \"prob\": probs, \"er\": ers}\n",
    "\n",
    "    y_pred = probs.argmax(axis=1)\n",
    "    acc = accuracy_score(ys, y_pred)\n",
    "    f1m = f1_score(ys, y_pred, average=\"macro\")\n",
    "    auc = roc_auc_score(ys, probs[:, 1]) if len(np.unique(ys)) == 2 else np.nan\n",
    "    cm = confusion_matrix(ys, y_pred)\n",
    "\n",
    "    return {\n",
    "        \"loss\": total_loss / max(1, n),\n",
    "        \"acc\": float(acc),\n",
    "        \"f1m\": float(f1m),\n",
    "        \"auc\": float(auc) if np.isfinite(auc) else np.nan,\n",
    "        \"cm\": cm,\n",
    "        \"y\": ys,\n",
    "        \"prob\": probs,\n",
    "        \"er\": ers,\n",
    "    }\n",
    "\n",
    "\n",
    "def make_ce_weights_binary(y_np: np.ndarray) -> torch.Tensor:\n",
    "    y_np = np.asarray(y_np, dtype=np.int64)\n",
    "    counts = np.bincount(y_np, minlength=2).astype(np.float64)\n",
    "    counts = np.maximum(counts, 1.0)\n",
    "    w = counts.sum() / (2.0 * counts)\n",
    "    return torch.tensor(w, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "\n",
    "def make_weighted_sampler_from_labels(y_np: np.ndarray) -> WeightedRandomSampler:\n",
    "    y_np = np.asarray(y_np, dtype=np.int64)\n",
    "    counts = np.bincount(y_np, minlength=2).astype(np.float64)\n",
    "    counts = np.maximum(counts, 1.0)\n",
    "    class_w = counts.sum() / (2.0 * counts)\n",
    "    sample_w = class_w[y_np].astype(np.float64)\n",
    "    sample_w = torch.tensor(sample_w, dtype=torch.double)\n",
    "    return WeightedRandomSampler(weights=sample_w, num_samples=len(sample_w), replacement=True)\n",
    "\n",
    "\n",
    "def train_binary_classifier(\n",
    "    X_scaled: np.ndarray,\n",
    "    edge_scaled: np.ndarray,\n",
    "    y_trade_arr: np.ndarray,\n",
    "    y_dir_arr: np.ndarray,\n",
    "    exit_ret_arr: np.ndarray,\n",
    "    sample_t_: np.ndarray,\n",
    "    idx_train: np.ndarray,\n",
    "    idx_val: np.ndarray,\n",
    "    idx_test: np.ndarray,\n",
    "    cfg: Dict[str, Any],\n",
    "    stage_name: str,  # \"trade\" or \"dir\"\n",
    ") -> Tuple[nn.Module, Dict[str, Any]]:\n",
    "    assert stage_name in (\"trade\", \"dir\")\n",
    "\n",
    "    L_ = int(cfg[\"lookback\"])\n",
    "    bs = int(cfg[\"batch_size\"])\n",
    "\n",
    "    tr_ds = LobGraphSequenceDataset2Stage(X_scaled, edge_scaled, y_trade_arr, y_dir_arr, exit_ret_arr, sample_t_, idx_train, L_)\n",
    "    va_ds = LobGraphSequenceDataset2Stage(X_scaled, edge_scaled, y_trade_arr, y_dir_arr, exit_ret_arr, sample_t_, idx_val,   L_)\n",
    "    te_ds = LobGraphSequenceDataset2Stage(X_scaled, edge_scaled, y_trade_arr, y_dir_arr, exit_ret_arr, sample_t_, idx_test,  L_)\n",
    "\n",
    "    # labels for sampler/weights (TRAIN only)\n",
    "    t_train = sample_t_[idx_train]\n",
    "    y_train_np = (y_trade_arr[t_train] if stage_name == \"trade\" else y_dir_arr[t_train]).astype(np.int64)\n",
    "\n",
    "    sampler = None\n",
    "    shuffle = True\n",
    "    if bool(cfg.get(\"use_weighted_sampler\", True)):\n",
    "        sampler = make_weighted_sampler_from_labels(y_train_np)\n",
    "        shuffle = False\n",
    "\n",
    "    tr_loader = DataLoader(tr_ds, batch_size=bs, shuffle=shuffle, sampler=sampler, drop_last=False, collate_fn=collate_fn_2stage, num_workers=0)\n",
    "    va_loader = DataLoader(va_ds, batch_size=bs, shuffle=False, drop_last=False, collate_fn=collate_fn_2stage, num_workers=0)\n",
    "    te_loader = DataLoader(te_ds, batch_size=bs, shuffle=False, drop_last=False, collate_fn=collate_fn_2stage, num_workers=0)\n",
    "\n",
    "    node_in = int(X_scaled.shape[-1])\n",
    "    edge_dim = int(edge_scaled.shape[-1])\n",
    "    model = GNN_TCN_Classifier(node_in=node_in, edge_dim=edge_dim, cfg=cfg, target_node=TARGET_NODE).to(DEVICE)\n",
    "\n",
    "    ce_w = make_ce_weights_binary(y_train_np)\n",
    "    loss_fn = nn.CrossEntropyLoss(weight=ce_w, label_smoothing=float(cfg.get(\"label_smoothing\", 0.0)))\n",
    "\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=float(cfg[\"lr\"]), weight_decay=float(cfg[\"weight_decay\"]))\n",
    "\n",
    "    use_onecycle = bool(cfg.get(\"use_onecycle\", True))\n",
    "    if use_onecycle:\n",
    "        sch = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            opt,\n",
    "            max_lr=float(cfg[\"lr\"]),\n",
    "            epochs=int(cfg[\"epochs\"]),\n",
    "            steps_per_epoch=max(1, len(tr_loader)),\n",
    "            pct_start=0.15,\n",
    "            div_factor=10.0,\n",
    "            final_div_factor=50.0,\n",
    "        )\n",
    "    else:\n",
    "        sch = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"max\", factor=0.5, patience=3)\n",
    "\n",
    "    best_auc = -1e18\n",
    "    best_state = None\n",
    "    best_epoch = -1\n",
    "\n",
    "    patience = 7\n",
    "    bad = 0\n",
    "\n",
    "    for ep in range(1, int(cfg[\"epochs\"]) + 1):\n",
    "        model.train()\n",
    "        tot_loss = 0.0\n",
    "        n = 0\n",
    "\n",
    "        for x, e, y_trade_b, y_dir_b, _er in tr_loader:\n",
    "            x = x.to(DEVICE).float()\n",
    "            e = e.to(DEVICE).float()\n",
    "            y = (y_trade_b if stage_name == \"trade\" else y_dir_b).to(DEVICE).long()\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            logits = model(x, e, EDGE_INDEX.to(DEVICE))\n",
    "            loss = loss_fn(logits, y)\n",
    "            if not torch.isfinite(loss):\n",
    "                continue\n",
    "\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), float(cfg[\"grad_clip\"]))\n",
    "            opt.step()\n",
    "\n",
    "            if use_onecycle:\n",
    "                sch.step()\n",
    "\n",
    "            tot_loss += float(loss.item()) * int(y.size(0))\n",
    "            n += int(y.size(0))\n",
    "\n",
    "        tr_loss = tot_loss / max(1, n)\n",
    "\n",
    "        va = eval_binary(model, va_loader, loss_fn, y_key=stage_name)\n",
    "        va_auc = va[\"auc\"]\n",
    "        sel_auc = float(va_auc) if np.isfinite(va_auc) else -1e18\n",
    "\n",
    "        if sel_auc > best_auc:\n",
    "            best_auc = sel_auc\n",
    "            best_epoch = ep\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            bad = 0\n",
    "        else:\n",
    "            bad += 1\n",
    "\n",
    "        if not use_onecycle:\n",
    "            sch.step(sel_auc)\n",
    "\n",
    "        lr_now = opt.param_groups[0][\"lr\"]\n",
    "        print(\n",
    "            f\"[{stage_name}] ep {ep:02d} lr={lr_now:.2e} \"\n",
    "            f\"tr_loss={tr_loss:.4f} va_loss={va['loss']:.4f} va_auc={va_auc:.3f} \"\n",
    "            f\"best={best_auc:.3f}@ep{best_epoch:02d}\"\n",
    "        )\n",
    "\n",
    "        if bad >= patience:\n",
    "            break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    va = eval_binary(model, va_loader, loss_fn, y_key=stage_name)\n",
    "    te = eval_binary(model, te_loader, loss_fn, y_key=stage_name)\n",
    "\n",
    "    res = {\n",
    "        \"best_epoch\": int(best_epoch),\n",
    "        \"best_val_auc\": float(best_auc) if np.isfinite(best_auc) else np.nan,\n",
    "        \"val\": va,\n",
    "        \"test\": te,\n",
    "    }\n",
    "    return model, res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9 — Two-stage PnL + threshold sweep (val only)\n",
    "\n",
    "def build_trade_threshold_grid(\n",
    "    p_trade: np.ndarray,\n",
    "    base_grid: Optional[List[float]] = None,\n",
    "    target_trades_list: Optional[List[int]] = None,\n",
    "    min_thr: float = 0.01,\n",
    "    max_thr: float = 0.99,\n",
    ") -> List[float]:\n",
    "    p_trade = np.asarray(p_trade, dtype=np.float64)\n",
    "    p_trade = p_trade[np.isfinite(p_trade)]\n",
    "    if p_trade.size == 0:\n",
    "        return base_grid or [0.5]\n",
    "\n",
    "    thrs = set(float(t) for t in (base_grid or []))\n",
    "\n",
    "    if target_trades_list:\n",
    "        N = int(p_trade.size)\n",
    "        for k in target_trades_list:\n",
    "            k = int(k)\n",
    "            if k <= 0:\n",
    "                continue\n",
    "            if k >= N:\n",
    "                thr = float(np.min(p_trade))\n",
    "            else:\n",
    "                q = 1.0 - (k / N)\n",
    "                thr = float(np.quantile(p_trade, q))\n",
    "            thrs.add(float(np.clip(thr, min_thr, max_thr)))\n",
    "\n",
    "    out = sorted(thrs)\n",
    "    cleaned = []\n",
    "    for t in out:\n",
    "        if not cleaned or abs(t - cleaned[-1]) > 1e-6:\n",
    "            cleaned.append(float(t))\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def two_stage_trade_mask(prob_trade: np.ndarray, prob_dir: np.ndarray, thr_trade: float, thr_dir: float) -> np.ndarray:\n",
    "    p_trade = prob_trade[:, 1]\n",
    "    p_up = prob_dir[:, 1]\n",
    "    conf_dir = np.maximum(p_up, 1.0 - p_up)\n",
    "    return (p_trade >= float(thr_trade)) & (conf_dir >= float(thr_dir))\n",
    "\n",
    "\n",
    "def two_stage_pnl_by_threshold(\n",
    "    prob_trade: np.ndarray,\n",
    "    prob_dir: np.ndarray,\n",
    "    exit_ret_arr: np.ndarray,\n",
    "    thr_trade: float,\n",
    "    thr_dir: float,\n",
    "    cost_bps: float,\n",
    ") -> Dict[str, Any]:\n",
    "    p_up = prob_dir[:, 1]\n",
    "    mask = two_stage_trade_mask(prob_trade, prob_dir, thr_trade, thr_dir)\n",
    "\n",
    "    action = np.zeros_like(exit_ret_arr, dtype=np.float32)\n",
    "    action[mask] = np.where(p_up[mask] >= 0.5, 1.0, -1.0).astype(np.float32)\n",
    "\n",
    "    cost = (float(cost_bps) * 1e-4) * mask.astype(np.float32)\n",
    "    pnl = action * exit_ret_arr - cost\n",
    "\n",
    "    n = int(len(exit_ret_arr))\n",
    "    n_tr = int(mask.sum())\n",
    "\n",
    "    return {\n",
    "        \"n\": n,\n",
    "        \"n_trades\": n_tr,\n",
    "        \"trade_rate\": float(n_tr / max(1, n)),\n",
    "        \"pnl_sum\": float(pnl.sum()),\n",
    "        \"pnl_mean\": float(pnl.mean()) if n else np.nan,\n",
    "        \"pnl_per_trade\": float(pnl.sum() / max(1, n_tr)),\n",
    "        \"pnl_sharpe\": float((pnl.mean() / (pnl.std() + 1e-12)) * np.sqrt(288)) if n else np.nan,\n",
    "    }\n",
    "\n",
    "\n",
    "def sweep_thresholds(\n",
    "    prob_trade: np.ndarray,\n",
    "    prob_dir: np.ndarray,\n",
    "    exit_ret_arr: np.ndarray,\n",
    "    cfg: Dict[str, Any],\n",
    "    min_trades: int = 0,\n",
    "    target_trade_rate: Optional[float] = None,\n",
    ") -> pd.DataFrame:\n",
    "    p_trade = prob_trade[:, 1]\n",
    "    thr_trade_grid = build_trade_threshold_grid(\n",
    "        p_trade=p_trade,\n",
    "        base_grid=cfg.get(\"thr_trade_grid\", [0.5]),\n",
    "        target_trades_list=cfg.get(\"proxy_target_trades\", None),\n",
    "        min_thr=0.01,\n",
    "        max_thr=0.99,\n",
    "    )\n",
    "    thr_dir_grid = cfg.get(\"thr_dir_grid\", [0.5])\n",
    "\n",
    "    obj = str(cfg.get(\"thr_objective\", \"pnl_sum\"))\n",
    "    max_rate = cfg.get(\"max_trade_rate_val\", None)\n",
    "    penalty = float(cfg.get(\"trade_rate_penalty\", 0.0))\n",
    "\n",
    "    rows = []\n",
    "    for thr_t in thr_trade_grid:\n",
    "        for thr_d in thr_dir_grid:\n",
    "            m = two_stage_pnl_by_threshold(prob_trade, prob_dir, exit_ret_arr, thr_t, thr_d, cfg[\"cost_bps\"])\n",
    "            if int(m[\"n_trades\"]) < int(min_trades):\n",
    "                continue\n",
    "            if max_rate is not None and float(m[\"trade_rate\"]) > float(max_rate):\n",
    "                continue\n",
    "\n",
    "            base = float(m.get(obj, np.nan))\n",
    "            if not np.isfinite(base):\n",
    "                continue\n",
    "\n",
    "            if target_trade_rate is not None:\n",
    "                score = base - penalty * abs(float(m[\"trade_rate\"]) - float(target_trade_rate))\n",
    "            else:\n",
    "                score = base - penalty * float(m[\"trade_rate\"])\n",
    "\n",
    "            rows.append({\"thr_trade\": float(thr_t), \"thr_dir\": float(thr_d), \"score\": float(score), **m})\n",
    "\n",
    "    if not rows:\n",
    "        return sweep_thresholds(prob_trade, prob_dir, exit_ret_arr, cfg, min_trades=1, target_trade_rate=target_trade_rate)\n",
    "\n",
    "    df_ = pd.DataFrame(rows).sort_values([\"score\", \"pnl_sum\"], ascending=False)\n",
    "    return df_\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_probs_on_indices(\n",
    "    model: nn.Module,\n",
    "    X_scaled: np.ndarray,\n",
    "    edge_scaled: np.ndarray,\n",
    "    indices: np.ndarray,\n",
    "    cfg: Dict[str, Any]\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    ds = LobGraphSequenceDataset2Stage(X_scaled, edge_scaled, y_trade, y_dir, exit_ret, sample_t, indices, cfg[\"lookback\"])\n",
    "    loader = DataLoader(ds, batch_size=int(cfg[\"batch_size\"]), shuffle=False, collate_fn=collate_fn_2stage, num_workers=0)\n",
    "\n",
    "    model.eval()\n",
    "    probs = []\n",
    "    ers = []\n",
    "    for x, e, _yt, _yd, er in loader:\n",
    "        x = x.to(DEVICE).float()\n",
    "        e = e.to(DEVICE).float()\n",
    "        logits = model(x, e, EDGE_INDEX.to(DEVICE))\n",
    "        p = torch.softmax(logits, dim=-1).cpu().numpy()\n",
    "        probs.append(p)\n",
    "        ers.append(er.cpu().numpy())\n",
    "\n",
    "    return np.concatenate(probs, axis=0), np.concatenate(ers, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FOLD 1/4 sizes: train=5652 val=1130 test=1130\n",
      "True trade ratio (val):  0.365\n",
      "True trade ratio (test): 0.304\n",
      "[trade] ep 01 lr=9.84e-05 tr_loss=0.7989 va_loss=0.7867 va_auc=0.521 best=0.521@ep01\n",
      "[trade] ep 02 lr=2.34e-04 tr_loss=0.7124 va_loss=0.7432 va_auc=0.516 best=0.521@ep01\n",
      "[trade] ep 03 lr=3.00e-04 tr_loss=0.6825 va_loss=0.7221 va_auc=0.487 best=0.521@ep01\n",
      "[trade] ep 04 lr=2.97e-04 tr_loss=0.6653 va_loss=0.7241 va_auc=0.535 best=0.535@ep04\n",
      "[trade] ep 05 lr=2.90e-04 tr_loss=0.6529 va_loss=0.7315 va_auc=0.501 best=0.535@ep04\n",
      "[trade] ep 06 lr=2.77e-04 tr_loss=0.6446 va_loss=0.7373 va_auc=0.508 best=0.535@ep04\n",
      "[trade] ep 07 lr=2.61e-04 tr_loss=0.6388 va_loss=0.7269 va_auc=0.516 best=0.535@ep04\n",
      "[trade] ep 08 lr=2.40e-04 tr_loss=0.6156 va_loss=0.7664 va_auc=0.519 best=0.535@ep04\n",
      "[trade] ep 09 lr=2.16e-04 tr_loss=0.6196 va_loss=0.7484 va_auc=0.499 best=0.535@ep04\n",
      "[trade] ep 10 lr=1.91e-04 tr_loss=0.6034 va_loss=0.7574 va_auc=0.480 best=0.535@ep04\n",
      "[trade] ep 11 lr=1.64e-04 tr_loss=0.5802 va_loss=0.8075 va_auc=0.484 best=0.535@ep04\n",
      "[dir] ep 01 lr=1.00e-04 tr_loss=0.8472 va_loss=0.7606 va_auc=0.494 best=0.494@ep01\n",
      "[dir] ep 02 lr=2.37e-04 tr_loss=0.8147 va_loss=0.7073 va_auc=0.495 best=0.495@ep02\n",
      "[dir] ep 03 lr=3.00e-04 tr_loss=0.7215 va_loss=0.7210 va_auc=0.446 best=0.495@ep02\n",
      "[dir] ep 04 lr=2.97e-04 tr_loss=0.6933 va_loss=0.6620 va_auc=0.493 best=0.495@ep02\n",
      "[dir] ep 05 lr=2.89e-04 tr_loss=0.6913 va_loss=0.6818 va_auc=0.488 best=0.495@ep02\n",
      "[dir] ep 06 lr=2.77e-04 tr_loss=0.6699 va_loss=0.6573 va_auc=0.508 best=0.508@ep06\n",
      "[dir] ep 07 lr=2.60e-04 tr_loss=0.6677 va_loss=0.7219 va_auc=0.438 best=0.508@ep06\n",
      "[dir] ep 08 lr=2.39e-04 tr_loss=0.6626 va_loss=0.7047 va_auc=0.475 best=0.508@ep06\n",
      "[dir] ep 09 lr=2.16e-04 tr_loss=0.6560 va_loss=0.7088 va_auc=0.515 best=0.515@ep09\n",
      "[dir] ep 10 lr=1.90e-04 tr_loss=0.6483 va_loss=0.7273 va_auc=0.475 best=0.515@ep09\n",
      "[dir] ep 11 lr=1.62e-04 tr_loss=0.6522 va_loss=0.7555 va_auc=0.459 best=0.515@ep09\n",
      "[dir] ep 12 lr=1.35e-04 tr_loss=0.6297 va_loss=0.7403 va_auc=0.479 best=0.515@ep09\n",
      "[dir] ep 13 lr=1.08e-04 tr_loss=0.6190 va_loss=0.7319 va_auc=0.484 best=0.515@ep09\n",
      "[dir] ep 14 lr=8.21e-05 tr_loss=0.6083 va_loss=0.7356 va_auc=0.466 best=0.515@ep09\n",
      "[dir] ep 15 lr=5.88e-05 tr_loss=0.6060 va_loss=0.7906 va_auc=0.449 best=0.515@ep09\n",
      "[dir] ep 16 lr=3.86e-05 tr_loss=0.5865 va_loss=0.7418 va_auc=0.464 best=0.515@ep09\n",
      "\n",
      "Chosen thresholds (from VAL):\n",
      "  thr_trade*=0.500 thr_dir*=0.700 | score=0.0336\n",
      "  val trade_rate(pred)=0.151 | val pnl_sum=0.0550 | val sharpe=0.615\n",
      "\n",
      "Top-5 VAL threshold candidates:\n",
      "    thr_trade  thr_dir     score  trade_rate   pnl_sum  pnl_sharpe  n_trades\n",
      "2        0.50     0.70  0.033605    0.151327  0.055021    0.615284       171\n",
      "6        0.55     0.70  0.029681    0.144248  0.051805    0.599636       163\n",
      "12       0.65     0.50  0.016448    0.204425  0.032555    0.307168       231\n",
      "1        0.50     0.65 -0.005893    0.262832  0.004373    0.037878       297\n",
      "11       0.60     0.70 -0.009329    0.096460  0.017574    0.251380       109\n",
      "\n",
      "TEST (fixed thr from VAL):\n",
      "  trade_rate(pred)=0.204 | pnl_sum=-0.1236 | pnl_mean=-0.000109 | trades=230\n",
      "\n",
      "================================================================================\n",
      "FOLD 2/4 sizes: train=6782 val=1130 test=1130\n",
      "True trade ratio (val):  0.304\n",
      "True trade ratio (test): 0.463\n",
      "[trade] ep 01 lr=9.83e-05 tr_loss=0.8059 va_loss=0.7976 va_auc=0.577 best=0.577@ep01\n",
      "[trade] ep 02 lr=2.34e-04 tr_loss=0.7058 va_loss=0.8209 va_auc=0.572 best=0.577@ep01\n",
      "[trade] ep 03 lr=3.00e-04 tr_loss=0.6724 va_loss=0.8262 va_auc=0.600 best=0.600@ep03\n",
      "[trade] ep 04 lr=2.97e-04 tr_loss=0.6583 va_loss=0.8548 va_auc=0.599 best=0.600@ep03\n",
      "[trade] ep 05 lr=2.90e-04 tr_loss=0.6542 va_loss=0.8324 va_auc=0.585 best=0.600@ep03\n",
      "[trade] ep 06 lr=2.77e-04 tr_loss=0.6420 va_loss=0.8047 va_auc=0.599 best=0.600@ep03\n",
      "[trade] ep 07 lr=2.61e-04 tr_loss=0.6312 va_loss=0.7440 va_auc=0.622 best=0.622@ep07\n",
      "[trade] ep 08 lr=2.40e-04 tr_loss=0.6199 va_loss=0.8394 va_auc=0.590 best=0.622@ep07\n",
      "[trade] ep 09 lr=2.17e-04 tr_loss=0.5994 va_loss=0.7617 va_auc=0.583 best=0.622@ep07\n",
      "[trade] ep 10 lr=1.91e-04 tr_loss=0.5714 va_loss=0.7707 va_auc=0.555 best=0.622@ep07\n",
      "[trade] ep 11 lr=1.64e-04 tr_loss=0.5655 va_loss=0.8943 va_auc=0.585 best=0.622@ep07\n",
      "[trade] ep 12 lr=1.36e-04 tr_loss=0.5201 va_loss=0.8194 va_auc=0.634 best=0.634@ep12\n",
      "[trade] ep 13 lr=1.09e-04 tr_loss=0.4933 va_loss=0.8245 va_auc=0.597 best=0.634@ep12\n",
      "[trade] ep 14 lr=8.31e-05 tr_loss=0.4904 va_loss=0.7986 va_auc=0.598 best=0.634@ep12\n",
      "[trade] ep 15 lr=5.97e-05 tr_loss=0.4522 va_loss=0.8965 va_auc=0.570 best=0.634@ep12\n",
      "[trade] ep 16 lr=3.93e-05 tr_loss=0.4547 va_loss=0.8942 va_auc=0.600 best=0.634@ep12\n",
      "[trade] ep 17 lr=2.27e-05 tr_loss=0.4475 va_loss=0.9004 va_auc=0.585 best=0.634@ep12\n",
      "[trade] ep 18 lr=1.05e-05 tr_loss=0.4477 va_loss=0.8999 va_auc=0.582 best=0.634@ep12\n",
      "[trade] ep 19 lr=3.05e-06 tr_loss=0.4217 va_loss=0.9330 va_auc=0.590 best=0.634@ep12\n",
      "[dir] ep 01 lr=9.96e-05 tr_loss=0.9032 va_loss=0.8421 va_auc=0.546 best=0.546@ep01\n",
      "[dir] ep 02 lr=2.37e-04 tr_loss=0.8365 va_loss=0.7622 va_auc=0.585 best=0.585@ep02\n",
      "[dir] ep 03 lr=3.00e-04 tr_loss=0.7602 va_loss=0.6939 va_auc=0.610 best=0.610@ep03\n",
      "[dir] ep 04 lr=2.97e-04 tr_loss=0.7401 va_loss=0.6853 va_auc=0.561 best=0.610@ep03\n",
      "[dir] ep 05 lr=2.89e-04 tr_loss=0.7077 va_loss=0.6927 va_auc=0.581 best=0.610@ep03\n",
      "[dir] ep 06 lr=2.77e-04 tr_loss=0.6930 va_loss=0.6722 va_auc=0.546 best=0.610@ep03\n",
      "[dir] ep 07 lr=2.60e-04 tr_loss=0.6796 va_loss=0.6888 va_auc=0.519 best=0.610@ep03\n",
      "[dir] ep 08 lr=2.39e-04 tr_loss=0.6778 va_loss=0.6905 va_auc=0.544 best=0.610@ep03\n",
      "[dir] ep 09 lr=2.16e-04 tr_loss=0.6826 va_loss=0.6894 va_auc=0.535 best=0.610@ep03\n",
      "[dir] ep 10 lr=1.90e-04 tr_loss=0.6609 va_loss=0.7714 va_auc=0.547 best=0.610@ep03\n",
      "\n",
      "Chosen thresholds (from VAL):\n",
      "  thr_trade*=0.650 thr_dir*=0.500 | score=0.2203\n",
      "  val trade_rate(pred)=0.448 | val pnl_sum=0.2348 | val sharpe=1.166\n",
      "\n",
      "Top-5 VAL threshold candidates:\n",
      "    thr_trade  thr_dir     score  trade_rate   pnl_sum  pnl_sharpe  n_trades\n",
      "12       0.65      0.5  0.220325    0.447788  0.234750    1.165994       506\n",
      "8        0.60      0.5  0.199307    0.499115  0.218865    1.042187       564\n",
      "16       0.70      0.5  0.193144    0.383186  0.201109    1.056294       433\n",
      "20       0.75      0.5  0.191810    0.322124  0.193669    1.110140       364\n",
      "4        0.55      0.5  0.189621    0.560177  0.215285    0.976225       633\n",
      "\n",
      "TEST (fixed thr from VAL):\n",
      "  trade_rate(pred)=0.718 | pnl_sum=0.0519 | pnl_mean=0.000046 | trades=811\n",
      "\n",
      "================================================================================\n",
      "FOLD 3/4 sizes: train=7912 val=1130 test=1130\n",
      "True trade ratio (val):  0.463\n",
      "True trade ratio (test): 0.558\n",
      "[trade] ep 01 lr=9.82e-05 tr_loss=0.7505 va_loss=0.7391 va_auc=0.501 best=0.501@ep01\n",
      "[trade] ep 02 lr=2.34e-04 tr_loss=0.6935 va_loss=0.7035 va_auc=0.504 best=0.504@ep02\n",
      "[trade] ep 03 lr=3.00e-04 tr_loss=0.6566 va_loss=0.7110 va_auc=0.507 best=0.507@ep03\n",
      "[trade] ep 04 lr=2.97e-04 tr_loss=0.6458 va_loss=0.7683 va_auc=0.508 best=0.508@ep04\n",
      "[trade] ep 05 lr=2.90e-04 tr_loss=0.6448 va_loss=0.8014 va_auc=0.525 best=0.525@ep05\n",
      "[trade] ep 06 lr=2.77e-04 tr_loss=0.6191 va_loss=0.7782 va_auc=0.530 best=0.530@ep06\n",
      "[trade] ep 07 lr=2.61e-04 tr_loss=0.6098 va_loss=0.7803 va_auc=0.545 best=0.545@ep07\n",
      "[trade] ep 08 lr=2.40e-04 tr_loss=0.5865 va_loss=0.8339 va_auc=0.566 best=0.566@ep08\n",
      "[trade] ep 09 lr=2.17e-04 tr_loss=0.5602 va_loss=0.8305 va_auc=0.575 best=0.575@ep09\n",
      "[trade] ep 10 lr=1.91e-04 tr_loss=0.5152 va_loss=0.9058 va_auc=0.580 best=0.580@ep10\n",
      "[trade] ep 11 lr=1.64e-04 tr_loss=0.4977 va_loss=0.8417 va_auc=0.559 best=0.580@ep10\n",
      "[trade] ep 12 lr=1.36e-04 tr_loss=0.4745 va_loss=0.8584 va_auc=0.576 best=0.580@ep10\n",
      "[trade] ep 13 lr=1.09e-04 tr_loss=0.4671 va_loss=0.8698 va_auc=0.565 best=0.580@ep10\n",
      "[trade] ep 14 lr=8.32e-05 tr_loss=0.4422 va_loss=1.0220 va_auc=0.547 best=0.580@ep10\n",
      "[trade] ep 15 lr=5.97e-05 tr_loss=0.4380 va_loss=0.9119 va_auc=0.549 best=0.580@ep10\n",
      "[trade] ep 16 lr=3.94e-05 tr_loss=0.4249 va_loss=0.9494 va_auc=0.551 best=0.580@ep10\n",
      "[trade] ep 17 lr=2.28e-05 tr_loss=0.4249 va_loss=0.9609 va_auc=0.549 best=0.580@ep10\n",
      "[dir] ep 01 lr=9.93e-05 tr_loss=0.9112 va_loss=0.8746 va_auc=0.517 best=0.517@ep01\n",
      "[dir] ep 02 lr=2.36e-04 tr_loss=0.7990 va_loss=0.7081 va_auc=0.520 best=0.520@ep02\n",
      "[dir] ep 03 lr=3.00e-04 tr_loss=0.7498 va_loss=0.6796 va_auc=0.476 best=0.520@ep02\n",
      "[dir] ep 04 lr=2.97e-04 tr_loss=0.7040 va_loss=0.6593 va_auc=0.525 best=0.525@ep04\n",
      "[dir] ep 05 lr=2.89e-04 tr_loss=0.6951 va_loss=0.6698 va_auc=0.518 best=0.525@ep04\n",
      "[dir] ep 06 lr=2.77e-04 tr_loss=0.6901 va_loss=0.6635 va_auc=0.514 best=0.525@ep04\n",
      "[dir] ep 07 lr=2.60e-04 tr_loss=0.6854 va_loss=0.6816 va_auc=0.524 best=0.525@ep04\n",
      "[dir] ep 08 lr=2.40e-04 tr_loss=0.6796 va_loss=0.6828 va_auc=0.524 best=0.525@ep04\n",
      "[dir] ep 09 lr=2.16e-04 tr_loss=0.6622 va_loss=0.6878 va_auc=0.504 best=0.525@ep04\n",
      "[dir] ep 10 lr=1.90e-04 tr_loss=0.6713 va_loss=0.6821 va_auc=0.520 best=0.525@ep04\n",
      "[dir] ep 11 lr=1.63e-04 tr_loss=0.6663 va_loss=0.6805 va_auc=0.523 best=0.525@ep04\n",
      "\n",
      "Chosen thresholds (from VAL):\n",
      "  thr_trade*=0.550 thr_dir*=0.600 | score=0.4630\n",
      "  val trade_rate(pred)=0.479 | val pnl_sum=0.4646 | val sharpe=1.987\n",
      "\n",
      "Top-5 VAL threshold candidates:\n",
      "    thr_trade  thr_dir     score  trade_rate   pnl_sum  pnl_sharpe  n_trades\n",
      "3        0.55     0.60  0.463034    0.478761  0.464627    1.986950       541\n",
      "7        0.60     0.60  0.457125    0.458407  0.457568    1.991105       518\n",
      "0        0.50     0.60  0.455208    0.491150  0.458040    1.950361       555\n",
      "11       0.65     0.60  0.443711    0.436283  0.446366    1.971348       493\n",
      "6        0.60     0.55  0.438652    0.638053  0.456174    1.692761       721\n",
      "\n",
      "TEST (fixed thr from VAL):\n",
      "  trade_rate(pred)=0.477 | pnl_sum=-0.2144 | pnl_mean=-0.000190 | trades=539\n",
      "\n",
      "================================================================================\n",
      "FOLD 4/4 sizes: train=9042 val=1130 test=1130\n",
      "True trade ratio (val):  0.558\n",
      "True trade ratio (test): 0.506\n",
      "[trade] ep 01 lr=9.81e-05 tr_loss=0.7909 va_loss=0.6693 va_auc=0.520 best=0.520@ep01\n",
      "[trade] ep 02 lr=2.34e-04 tr_loss=0.7022 va_loss=0.6417 va_auc=0.574 best=0.574@ep02\n",
      "[trade] ep 03 lr=3.00e-04 tr_loss=0.6720 va_loss=0.6138 va_auc=0.656 best=0.656@ep03\n",
      "[trade] ep 04 lr=2.97e-04 tr_loss=0.6616 va_loss=0.6249 va_auc=0.680 best=0.680@ep04\n",
      "[trade] ep 05 lr=2.90e-04 tr_loss=0.6544 va_loss=0.6549 va_auc=0.699 best=0.699@ep05\n",
      "[trade] ep 06 lr=2.77e-04 tr_loss=0.6497 va_loss=0.6400 va_auc=0.683 best=0.699@ep05\n",
      "[trade] ep 07 lr=2.61e-04 tr_loss=0.6359 va_loss=0.6423 va_auc=0.681 best=0.699@ep05\n",
      "[trade] ep 08 lr=2.40e-04 tr_loss=0.6313 va_loss=0.6710 va_auc=0.674 best=0.699@ep05\n",
      "[trade] ep 09 lr=2.17e-04 tr_loss=0.6201 va_loss=0.7265 va_auc=0.665 best=0.699@ep05\n",
      "[trade] ep 10 lr=1.91e-04 tr_loss=0.6041 va_loss=0.7761 va_auc=0.673 best=0.699@ep05\n",
      "[trade] ep 11 lr=1.64e-04 tr_loss=0.5734 va_loss=0.7731 va_auc=0.669 best=0.699@ep05\n",
      "[trade] ep 12 lr=1.36e-04 tr_loss=0.5542 va_loss=0.8191 va_auc=0.663 best=0.699@ep05\n",
      "[dir] ep 01 lr=9.90e-05 tr_loss=0.8494 va_loss=0.8132 va_auc=0.461 best=0.461@ep01\n",
      "[dir] ep 02 lr=2.36e-04 tr_loss=0.7844 va_loss=0.7547 va_auc=0.489 best=0.489@ep02\n",
      "[dir] ep 03 lr=3.00e-04 tr_loss=0.7370 va_loss=0.7212 va_auc=0.506 best=0.506@ep03\n",
      "[dir] ep 04 lr=2.97e-04 tr_loss=0.7125 va_loss=0.7371 va_auc=0.499 best=0.506@ep03\n",
      "[dir] ep 05 lr=2.90e-04 tr_loss=0.7024 va_loss=0.7104 va_auc=0.498 best=0.506@ep03\n",
      "[dir] ep 06 lr=2.77e-04 tr_loss=0.6846 va_loss=0.7413 va_auc=0.483 best=0.506@ep03\n",
      "[dir] ep 07 lr=2.60e-04 tr_loss=0.6918 va_loss=0.7251 va_auc=0.455 best=0.506@ep03\n",
      "[dir] ep 08 lr=2.40e-04 tr_loss=0.6734 va_loss=0.7774 va_auc=0.458 best=0.506@ep03\n",
      "[dir] ep 09 lr=2.16e-04 tr_loss=0.6645 va_loss=0.7401 va_auc=0.487 best=0.506@ep03\n",
      "[dir] ep 10 lr=1.90e-04 tr_loss=0.6518 va_loss=0.7799 va_auc=0.495 best=0.506@ep03\n",
      "\n",
      "Chosen thresholds (from VAL):\n",
      "  thr_trade*=0.700 thr_dir*=0.550 | score=0.0107\n",
      "  val trade_rate(pred)=0.465 | val pnl_sum=0.0199 | val sharpe=0.066\n",
      "\n",
      "Top-5 VAL threshold candidates:\n",
      "    thr_trade  thr_dir     score  trade_rate   pnl_sum  pnl_sharpe  n_trades\n",
      "15   0.700000     0.55  0.010680    0.465487  0.019883    0.066043       526\n",
      "19   0.750000     0.50  0.002338    0.634513  0.010037    0.028000       717\n",
      "28   0.948127     0.55 -0.011661    0.053097  0.038781    0.330175        60\n",
      "25   0.931841     0.55 -0.043526    0.111504  0.001076    0.006620       126\n",
      "20   0.750000     0.55 -0.052745    0.413274 -0.038320   -0.131241       467\n",
      "\n",
      "TEST (fixed thr from VAL):\n",
      "  trade_rate(pred)=0.288 | pnl_sum=-0.2410 | pnl_mean=-0.000213 | trades=325\n",
      "\n",
      "================================================================================\n",
      "CV summary (fold TEST, fixed thresholds from VAL):\n",
      "   fold  trade_test_auc  dir_test_auc  test_trade_rate_pred  test_pnl_sum  \\\n",
      "0     1        0.553169      0.588319              0.203540     -0.123618   \n",
      "1     2        0.568905      0.513732              0.717699      0.051851   \n",
      "2     3        0.616990      0.475813              0.476991     -0.214436   \n",
      "3     4        0.678579      0.500850              0.287611     -0.241040   \n",
      "\n",
      "   test_pnl_mean  thr_trade  thr_dir  n_trades  best_val_score  \n",
      "0      -0.000109       0.50     0.70       230        0.033605  \n",
      "1       0.000046       0.65     0.50       811        0.220325  \n",
      "2      -0.000190       0.55     0.60       539        0.463034  \n",
      "3      -0.000213       0.70     0.55       325        0.010680  \n",
      "\n",
      "Means:\n",
      "fold                      2.500000\n",
      "trade_test_auc            0.604411\n",
      "dir_test_auc              0.519679\n",
      "test_trade_rate_pred      0.421460\n",
      "test_pnl_sum             -0.131811\n",
      "test_pnl_mean            -0.000117\n",
      "thr_trade                 0.600000\n",
      "thr_dir                   0.587500\n",
      "n_trades                476.250000\n",
      "best_val_score            0.181911\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Step 10 — Walk-forward CV on CV-part (90%): train trade → train dir → test PnL + store artifacts\n",
    "\n",
    "def _state_dict_to_cpu(sd: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "    return {k: v.detach().cpu().clone() for k, v in sd.items()}\n",
    "\n",
    "\n",
    "def run_walk_forward_cv() -> Tuple[pd.DataFrame, List[Dict[str, Any]], nn.Module, nn.Module]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      - cv_summary: per-fold test metrics (using fold-val thresholds)\n",
    "      - fold_artifacts: list of dicts per fold:\n",
    "          {\n",
    "            fold, idx_tr, idx_va, idx_te,\n",
    "            thr_trade, thr_dir, best_val_score,\n",
    "            trade_state, dir_state,\n",
    "            prob_trade_val, prob_dir_val, er_val, val_true_trade_rate\n",
    "          }\n",
    "      - m_trade_last, m_dir_last: last fold trained models (convenience variables)\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    fold_artifacts: List[Dict[str, Any]] = []\n",
    "\n",
    "    m_trade_last = None\n",
    "    m_dir_last = None\n",
    "\n",
    "    for fi, (idx_tr, idx_va, idx_te) in enumerate(walk_splits, 1):\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(f\"FOLD {fi}/{len(walk_splits)} sizes: train={len(idx_tr)} val={len(idx_va)} test={len(idx_te)}\")\n",
    "        true_val_trade = split_trade_ratio(idx_va, sample_t, y_trade)\n",
    "        true_te_trade = split_trade_ratio(idx_te, sample_t, y_trade)\n",
    "        print(f\"True trade ratio (val):  {true_val_trade:.3f}\")\n",
    "        print(f\"True trade ratio (test): {true_te_trade:.3f}\")\n",
    "\n",
    "        # scale per fold (fit only on train timeline)\n",
    "        X_scaled, _ = fit_scale_nodes_train_only(X_node_raw, sample_t, idx_tr, max_abs=CFG[\"max_abs_feat\"])\n",
    "\n",
    "        # edge scaling per fold\n",
    "        if bool(CFG.get(\"edge_scale\", True)):\n",
    "            edge_scaled, _ = fit_scale_edges_train_only(edge_feat, sample_t, idx_tr, max_abs=CFG[\"max_abs_edge\"])\n",
    "        else:\n",
    "            edge_scaled = edge_feat\n",
    "\n",
    "        # Stage A\n",
    "        m_trade, r_trade = train_binary_classifier(\n",
    "            X_scaled, edge_scaled, y_trade, y_dir, exit_ret, sample_t,\n",
    "            idx_tr, idx_va, idx_te, CFG, stage_name=\"trade\"\n",
    "        )\n",
    "\n",
    "        # Stage B (trade-only indices)\n",
    "        idx_tr_T = subset_trade_indices(idx_tr, sample_t, y_trade)\n",
    "        idx_va_T = subset_trade_indices(idx_va, sample_t, y_trade)\n",
    "        idx_te_T = subset_trade_indices(idx_te, sample_t, y_trade)\n",
    "\n",
    "        if len(idx_tr_T) < max(200, 2 * CFG[\"batch_size\"]) or len(idx_te_T) < 50 or len(idx_va_T) < 50:\n",
    "            print(\"[dir] skip: not enough trade samples in this fold.\")\n",
    "            rows.append({\n",
    "                \"fold\": fi,\n",
    "                \"trade_test_auc\": r_trade[\"test\"][\"auc\"],\n",
    "                \"dir_test_auc\": np.nan,\n",
    "                \"test_trade_rate_pred\": np.nan,\n",
    "                \"test_pnl_sum\": np.nan,\n",
    "                \"test_pnl_mean\": np.nan,\n",
    "                \"thr_trade\": np.nan,\n",
    "                \"thr_dir\": np.nan,\n",
    "                \"n_trades\": np.nan,\n",
    "                \"best_val_score\": np.nan,\n",
    "            })\n",
    "            # still store trade-only part for completeness\n",
    "            fold_artifacts.append({\n",
    "                \"fold\": fi,\n",
    "                \"idx_tr\": idx_tr, \"idx_va\": idx_va, \"idx_te\": idx_te,\n",
    "                \"thr_trade\": np.nan, \"thr_dir\": np.nan,\n",
    "                \"best_val_score\": np.nan,\n",
    "                \"trade_state\": _state_dict_to_cpu(m_trade.state_dict()),\n",
    "                \"dir_state\": None,\n",
    "                \"prob_trade_val\": None, \"prob_dir_val\": None, \"er_val\": None,\n",
    "                \"val_true_trade_rate\": float(true_val_trade),\n",
    "            })\n",
    "            m_trade_last = m_trade\n",
    "            m_dir_last = None\n",
    "            continue\n",
    "\n",
    "        m_dir, r_dir = train_binary_classifier(\n",
    "            X_scaled, edge_scaled, y_trade, y_dir, exit_ret, sample_t,\n",
    "            idx_tr_T, idx_va_T, idx_te_T, CFG, stage_name=\"dir\"\n",
    "        )\n",
    "\n",
    "        # Choose thresholds on VAL with anti-overtrading constraint\n",
    "        prob_trade_val, er_val = predict_probs_on_indices(m_trade, X_scaled, edge_scaled, idx_va, CFG)\n",
    "        prob_dir_val, _ = predict_probs_on_indices(m_dir, X_scaled, edge_scaled, idx_va, CFG)\n",
    "\n",
    "        sweep_val = sweep_thresholds(\n",
    "            prob_trade_val, prob_dir_val, er_val, CFG,\n",
    "            min_trades=int(CFG[\"eval_min_trades\"]),\n",
    "            target_trade_rate=float(true_val_trade),\n",
    "        )\n",
    "        best_val = sweep_val.iloc[0].to_dict()\n",
    "\n",
    "        thr_trade_star = float(best_val[\"thr_trade\"])\n",
    "        thr_dir_star = float(best_val[\"thr_dir\"])\n",
    "\n",
    "        val_metrics = two_stage_pnl_by_threshold(prob_trade_val, prob_dir_val, er_val, thr_trade_star, thr_dir_star, CFG[\"cost_bps\"])\n",
    "        print(\"\\nChosen thresholds (from VAL):\")\n",
    "        print(f\"  thr_trade*={thr_trade_star:.3f} thr_dir*={thr_dir_star:.3f} | score={best_val['score']:.4f}\")\n",
    "        print(f\"  val trade_rate(pred)={val_metrics['trade_rate']:.3f} | val pnl_sum={val_metrics['pnl_sum']:.4f} | val sharpe={val_metrics['pnl_sharpe']:.3f}\")\n",
    "\n",
    "        print(\"\\nTop-5 VAL threshold candidates:\")\n",
    "        print(sweep_val.head(5)[[\"thr_trade\", \"thr_dir\", \"score\", \"trade_rate\", \"pnl_sum\", \"pnl_sharpe\", \"n_trades\"]])\n",
    "\n",
    "        # Evaluate on TEST with fixed thresholds\n",
    "        prob_trade_te, er_te = predict_probs_on_indices(m_trade, X_scaled, edge_scaled, idx_te, CFG)\n",
    "        prob_dir_te, _ = predict_probs_on_indices(m_dir, X_scaled, edge_scaled, idx_te, CFG)\n",
    "        te_metrics = two_stage_pnl_by_threshold(prob_trade_te, prob_dir_te, er_te, thr_trade_star, thr_dir_star, CFG[\"cost_bps\"])\n",
    "\n",
    "        print(\"\\nTEST (fixed thr from VAL):\")\n",
    "        print(f\"  trade_rate(pred)={te_metrics['trade_rate']:.3f} | pnl_sum={te_metrics['pnl_sum']:.4f} | pnl_mean={te_metrics['pnl_mean']:.6f} | trades={te_metrics['n_trades']}\")\n",
    "\n",
    "        rows.append({\n",
    "            \"fold\": fi,\n",
    "            \"trade_test_auc\": r_trade[\"test\"][\"auc\"],\n",
    "            \"dir_test_auc\": r_dir[\"test\"][\"auc\"],\n",
    "            \"test_trade_rate_pred\": te_metrics[\"trade_rate\"],\n",
    "            \"test_pnl_sum\": te_metrics[\"pnl_sum\"],\n",
    "            \"test_pnl_mean\": te_metrics[\"pnl_mean\"],\n",
    "            \"thr_trade\": thr_trade_star,\n",
    "            \"thr_dir\": thr_dir_star,\n",
    "            \"n_trades\": te_metrics[\"n_trades\"],\n",
    "            \"best_val_score\": float(best_val[\"score\"]),\n",
    "        })\n",
    "\n",
    "        fold_artifacts.append({\n",
    "            \"fold\": fi,\n",
    "            \"idx_tr\": idx_tr, \"idx_va\": idx_va, \"idx_te\": idx_te,\n",
    "            \"thr_trade\": thr_trade_star, \"thr_dir\": thr_dir_star,\n",
    "            \"best_val_score\": float(best_val[\"score\"]),\n",
    "            \"trade_state\": _state_dict_to_cpu(m_trade.state_dict()),\n",
    "            \"dir_state\": _state_dict_to_cpu(m_dir.state_dict()),\n",
    "            \"prob_trade_val\": prob_trade_val,\n",
    "            \"prob_dir_val\": prob_dir_val,\n",
    "            \"er_val\": er_val,\n",
    "            \"val_true_trade_rate\": float(true_val_trade),\n",
    "        })\n",
    "\n",
    "        m_trade_last = m_trade\n",
    "        m_dir_last = m_dir\n",
    "\n",
    "    cv_summary = pd.DataFrame(rows)\n",
    "    return cv_summary, fold_artifacts, m_trade_last, m_dir_last\n",
    "\n",
    "\n",
    "cv_summary, fold_artifacts, m_trade_last, m_dir_last = run_walk_forward_cv()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CV summary (fold TEST, fixed thresholds from VAL):\")\n",
    "print(cv_summary)\n",
    "print(\"\\nMeans:\")\n",
    "print(cv_summary.mean(numeric_only=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 11 — HOLDOUT CHECKS WITHOUT ANY REFIT (3 METHODS)\n",
      "Holdout size: 1256 samples | time range: sidx 11305..12560\n",
      "\n",
      "Results:\n",
      "                                                                                  method  thr_trade  thr_dir  holdout_trade_auc  holdout_dir_auc  trade_rate_pred   pnl_sum  pnl_sharpe  n_trades\n",
      "                                      1) LAST fold model + LAST fold thresholds (fold=4)       0.70     0.55           0.478012         0.510843         0.496815  0.173364    0.553044       624\n",
      "                 2) BEST-VAL fold model + BEST-VAL thresholds (fold=3, val_score=0.4630)       0.55     0.60           0.446548         0.559995         0.511146 -0.460827   -1.438569       642\n",
      "3) LAST fold model + GLOBAL thresholds (VAL-concat; true_val_trade=0.422) (model_fold=4)       0.70     0.50           0.478012         0.510843         0.705414  0.270295    0.727069       886\n",
      "\n",
      "Global thresholds (method 3) top-5 candidates (VAL-concat):\n",
      "    thr_trade  thr_dir     score  trade_rate   pnl_sum  pnl_sharpe  n_trades\n",
      "17       0.70     0.50  0.606384    0.459735  0.610123    0.602277      2078\n",
      "22       0.75     0.50  0.584184    0.398451  0.586574    0.608324      1801\n",
      "12       0.65     0.50  0.577276    0.551327  0.590174    0.550731      2492\n",
      "18       0.70     0.55  0.548676    0.332080  0.557702    0.651968      1501\n",
      "13       0.65     0.55  0.506789    0.399336  0.509090    0.561590      1805\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 11 — Post-CV checks on FINAL holdout (10%) WITHOUT refit (3 methods)\n",
    "\n",
    "def _safe_auc_binary(y_true: np.ndarray, p1: np.ndarray) -> float:\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    p1 = np.asarray(p1, dtype=np.float64)\n",
    "    if y_true.size == 0 or len(np.unique(y_true)) < 2:\n",
    "        return float(\"nan\")\n",
    "    return float(roc_auc_score(y_true, p1))\n",
    "\n",
    "\n",
    "def _build_model_from_state(node_in: int, edge_dim: int, cfg: Dict[str, Any], state: Dict[str, torch.Tensor]) -> nn.Module:\n",
    "    m = GNN_TCN_Classifier(node_in=node_in, edge_dim=edge_dim, cfg=cfg, target_node=TARGET_NODE).to(DEVICE)\n",
    "    m.load_state_dict(state)\n",
    "    m.eval()\n",
    "    return m\n",
    "\n",
    "\n",
    "def _get_scaled_arrays_for_fold(idx_tr_fold: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    X_scaled, _ = fit_scale_nodes_train_only(X_node_raw, sample_t, idx_tr_fold, max_abs=CFG[\"max_abs_feat\"])\n",
    "    if bool(CFG.get(\"edge_scale\", True)):\n",
    "        edge_scaled, _ = fit_scale_edges_train_only(edge_feat, sample_t, idx_tr_fold, max_abs=CFG[\"max_abs_edge\"])\n",
    "    else:\n",
    "        edge_scaled = edge_feat\n",
    "    return X_scaled, edge_scaled\n",
    "\n",
    "\n",
    "def _eval_holdout_with_models_and_thresholds(\n",
    "    method: str,\n",
    "    m_trade: nn.Module,\n",
    "    m_dir: nn.Module,\n",
    "    thr_trade: float,\n",
    "    thr_dir: float,\n",
    "    X_scaled: np.ndarray,\n",
    "    edge_scaled: np.ndarray,\n",
    "    idx_holdout: np.ndarray,\n",
    ") -> Dict[str, Any]:\n",
    "    # predictions\n",
    "    prob_trade_hold, er_hold = predict_probs_on_indices(m_trade, X_scaled, edge_scaled, idx_holdout, CFG)\n",
    "    prob_dir_hold, _ = predict_probs_on_indices(m_dir, X_scaled, edge_scaled, idx_holdout, CFG)\n",
    "\n",
    "    # true labels\n",
    "    t_hold = sample_t[idx_holdout]\n",
    "    y_trade_hold = y_trade[t_hold].astype(np.int64)\n",
    "    y_dir_hold = y_dir[t_hold].astype(np.int64)\n",
    "\n",
    "    trade_auc = _safe_auc_binary(y_trade_hold, prob_trade_hold[:, 1])\n",
    "    mask_true_trade = (y_trade_hold == 1)\n",
    "    dir_auc = _safe_auc_binary(y_dir_hold[mask_true_trade], prob_dir_hold[mask_true_trade, 1])\n",
    "\n",
    "    pnl = two_stage_pnl_by_threshold(\n",
    "        prob_trade_hold, prob_dir_hold, er_hold,\n",
    "        thr_trade=thr_trade, thr_dir=thr_dir,\n",
    "        cost_bps=CFG[\"cost_bps\"],\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"method\": method,\n",
    "        \"thr_trade\": float(thr_trade),\n",
    "        \"thr_dir\": float(thr_dir),\n",
    "        \"holdout_trade_auc\": float(trade_auc) if np.isfinite(trade_auc) else np.nan,\n",
    "        \"holdout_dir_auc\": float(dir_auc) if np.isfinite(dir_auc) else np.nan,\n",
    "        \"trade_rate_pred\": float(pnl[\"trade_rate\"]),\n",
    "        \"pnl_sum\": float(pnl[\"pnl_sum\"]),\n",
    "        \"pnl_mean\": float(pnl[\"pnl_mean\"]),\n",
    "        \"pnl_sharpe\": float(pnl[\"pnl_sharpe\"]),\n",
    "        \"n_trades\": int(pnl[\"n_trades\"]),\n",
    "    }\n",
    "\n",
    "\n",
    "def run_post_cv_holdout_checks() -> pd.DataFrame:\n",
    "    idx_holdout = idx_final_test.astype(np.int64)\n",
    "    node_in = int(X_node_raw.shape[-1])\n",
    "    edge_dim = int(edge_feat.shape[-1])\n",
    "\n",
    "    # Filter only folds that have both models\n",
    "    ok_folds = [fa for fa in fold_artifacts if fa.get(\"dir_state\") is not None and np.isfinite(fa.get(\"thr_trade\", np.nan))]\n",
    "    if len(ok_folds) == 0:\n",
    "        raise RuntimeError(\"No folds with a trained direction model were stored; cannot run Step 11 checks.\")\n",
    "\n",
    "    # 1) LAST fold model + LAST fold thresholds\n",
    "    fa_last = ok_folds[-1]\n",
    "    X_last, E_last = _get_scaled_arrays_for_fold(fa_last[\"idx_tr\"])\n",
    "    m_trade_last_ = _build_model_from_state(node_in, edge_dim, CFG, fa_last[\"trade_state\"])\n",
    "    m_dir_last_ = _build_model_from_state(node_in, edge_dim, CFG, fa_last[\"dir_state\"])\n",
    "    r1 = _eval_holdout_with_models_and_thresholds(\n",
    "        method=f\"1) LAST fold model + LAST fold thresholds (fold={fa_last['fold']})\",\n",
    "        m_trade=m_trade_last_,\n",
    "        m_dir=m_dir_last_,\n",
    "        thr_trade=float(fa_last[\"thr_trade\"]),\n",
    "        thr_dir=float(fa_last[\"thr_dir\"]),\n",
    "        X_scaled=X_last,\n",
    "        edge_scaled=E_last,\n",
    "        idx_holdout=idx_holdout,\n",
    "    )\n",
    "\n",
    "    # 2) BEST-VAL fold model + BEST-VAL thresholds\n",
    "    fa_best = max(ok_folds, key=lambda d: float(d.get(\"best_val_score\", -1e18)))\n",
    "    X_best, E_best = _get_scaled_arrays_for_fold(fa_best[\"idx_tr\"])\n",
    "    m_trade_best = _build_model_from_state(node_in, edge_dim, CFG, fa_best[\"trade_state\"])\n",
    "    m_dir_best = _build_model_from_state(node_in, edge_dim, CFG, fa_best[\"dir_state\"])\n",
    "    r2 = _eval_holdout_with_models_and_thresholds(\n",
    "        method=f\"2) BEST-VAL fold model + BEST-VAL thresholds (fold={fa_best['fold']}, val_score={fa_best['best_val_score']:.4f})\",\n",
    "        m_trade=m_trade_best,\n",
    "        m_dir=m_dir_best,\n",
    "        thr_trade=float(fa_best[\"thr_trade\"]),\n",
    "        thr_dir=float(fa_best[\"thr_dir\"]),\n",
    "        X_scaled=X_best,\n",
    "        edge_scaled=E_best,\n",
    "        idx_holdout=idx_holdout,\n",
    "    )\n",
    "\n",
    "    # 3) LAST fold model + GLOBAL thresholds fitted on concatenated fold-VAL predictions\n",
    "    #    (uses only per-fold val predictions produced by that fold's own models; no holdout information)\n",
    "    prob_trade_all = np.concatenate([fa[\"prob_trade_val\"] for fa in ok_folds], axis=0)\n",
    "    prob_dir_all = np.concatenate([fa[\"prob_dir_val\"] for fa in ok_folds], axis=0)\n",
    "    er_all = np.concatenate([fa[\"er_val\"] for fa in ok_folds], axis=0)\n",
    "\n",
    "    # global target trade rate on concatenated VAL (true)\n",
    "    idx_va_all = np.concatenate([fa[\"idx_va\"] for fa in ok_folds], axis=0)\n",
    "    true_trade_rate_all = split_trade_ratio(idx_va_all, sample_t, y_trade)\n",
    "\n",
    "    sweep_global = sweep_thresholds(\n",
    "        prob_trade_all, prob_dir_all, er_all, CFG,\n",
    "        min_trades=int(CFG[\"eval_min_trades\"]),\n",
    "        target_trade_rate=float(true_trade_rate_all),\n",
    "    )\n",
    "    best_global = sweep_global.iloc[0].to_dict()\n",
    "    thr_trade_global = float(best_global[\"thr_trade\"])\n",
    "    thr_dir_global = float(best_global[\"thr_dir\"])\n",
    "\n",
    "    r3 = _eval_holdout_with_models_and_thresholds(\n",
    "        method=f\"3) LAST fold model + GLOBAL thresholds (VAL-concat; true_val_trade={true_trade_rate_all:.3f}) (model_fold={fa_last['fold']})\",\n",
    "        m_trade=m_trade_last_,\n",
    "        m_dir=m_dir_last_,\n",
    "        thr_trade=thr_trade_global,\n",
    "        thr_dir=thr_dir_global,\n",
    "        X_scaled=X_last,\n",
    "        edge_scaled=E_last,\n",
    "        idx_holdout=idx_holdout,\n",
    "    )\n",
    "\n",
    "    out = pd.DataFrame([r1, r2, r3])\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STEP 11 — HOLDOUT CHECKS WITHOUT ANY REFIT (3 METHODS)\")\n",
    "    print(f\"Holdout size: {len(idx_holdout)} samples | time range: sidx {int(idx_holdout[0])}..{int(idx_holdout[-1])}\")\n",
    "    print(\"\\nResults:\")\n",
    "    print(out[[\n",
    "        \"method\", \"thr_trade\", \"thr_dir\",\n",
    "        \"holdout_trade_auc\", \"holdout_dir_auc\",\n",
    "        \"trade_rate_pred\", \"pnl_sum\", \"pnl_sharpe\", \"n_trades\"\n",
    "    ]].to_string(index=False))\n",
    "\n",
    "    print(\"\\nGlobal thresholds (method 3) top-5 candidates (VAL-concat):\")\n",
    "    print(sweep_global.head(5)[[\"thr_trade\", \"thr_dir\", \"score\", \"trade_rate\", \"pnl_sum\", \"pnl_sharpe\", \"n_trades\"]])\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    return out\n",
    "\n",
    "\n",
    "post_cv_holdout = run_post_cv_holdout_checks()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 12 — PRODUCTION-FIT (TRAIN ON CV(90%) → SELECT THR ON val_final → EVAL ON FINAL HOLDOUT(10%))\n",
      "Final split sizes:\n",
      "  train_final: 10175\n",
      "  val_final  : 1130\n",
      "  holdout    : 1256\n",
      "True trade ratio (val_final): 0.504\n",
      "True trade ratio (holdout):   0.576\n",
      "[trade] ep 01 lr=9.80e-05 tr_loss=0.7926 va_loss=0.7257 va_auc=0.533 best=0.533@ep01\n",
      "[trade] ep 02 lr=2.34e-04 tr_loss=0.7055 va_loss=0.6760 va_auc=0.578 best=0.578@ep02\n",
      "[trade] ep 03 lr=3.00e-04 tr_loss=0.6743 va_loss=0.6593 va_auc=0.661 best=0.661@ep03\n",
      "[trade] ep 04 lr=2.97e-04 tr_loss=0.6573 va_loss=0.6468 va_auc=0.644 best=0.661@ep03\n",
      "[trade] ep 05 lr=2.90e-04 tr_loss=0.6460 va_loss=0.6468 va_auc=0.668 best=0.668@ep05\n",
      "[trade] ep 06 lr=2.77e-04 tr_loss=0.6406 va_loss=0.6530 va_auc=0.678 best=0.678@ep06\n",
      "[trade] ep 07 lr=2.61e-04 tr_loss=0.6340 va_loss=0.6591 va_auc=0.621 best=0.678@ep06\n",
      "[trade] ep 08 lr=2.40e-04 tr_loss=0.6200 va_loss=0.6881 va_auc=0.667 best=0.678@ep06\n",
      "[trade] ep 09 lr=2.17e-04 tr_loss=0.6153 va_loss=0.7026 va_auc=0.626 best=0.678@ep06\n",
      "[trade] ep 10 lr=1.91e-04 tr_loss=0.5887 va_loss=0.7582 va_auc=0.590 best=0.678@ep06\n",
      "[trade] ep 11 lr=1.64e-04 tr_loss=0.5629 va_loss=0.8046 va_auc=0.573 best=0.678@ep06\n",
      "[trade] ep 12 lr=1.36e-04 tr_loss=0.5486 va_loss=0.8459 va_auc=0.537 best=0.678@ep06\n",
      "[trade] ep 13 lr=1.09e-04 tr_loss=0.5140 va_loss=0.8949 va_auc=0.540 best=0.678@ep06\n",
      "\n",
      "Trade-only sizes for DIR:\n",
      "  train_final_T: 3980\n",
      "  val_final_T  : 569\n",
      "  holdout_T    : 724\n",
      "[dir] ep 01 lr=9.88e-05 tr_loss=0.8635 va_loss=0.7521 va_auc=0.507 best=0.507@ep01\n",
      "[dir] ep 02 lr=2.35e-04 tr_loss=0.7843 va_loss=0.7247 va_auc=0.481 best=0.507@ep01\n",
      "[dir] ep 03 lr=3.00e-04 tr_loss=0.7247 va_loss=0.7030 va_auc=0.514 best=0.514@ep03\n",
      "[dir] ep 04 lr=2.97e-04 tr_loss=0.7034 va_loss=0.6957 va_auc=0.565 best=0.565@ep04\n",
      "[dir] ep 05 lr=2.90e-04 tr_loss=0.6975 va_loss=0.6785 va_auc=0.594 best=0.594@ep05\n",
      "[dir] ep 06 lr=2.77e-04 tr_loss=0.6878 va_loss=0.6802 va_auc=0.587 best=0.594@ep05\n",
      "[dir] ep 07 lr=2.60e-04 tr_loss=0.6882 va_loss=0.6835 va_auc=0.593 best=0.594@ep05\n",
      "[dir] ep 08 lr=2.40e-04 tr_loss=0.6838 va_loss=0.6866 va_auc=0.558 best=0.594@ep05\n",
      "[dir] ep 09 lr=2.16e-04 tr_loss=0.6724 va_loss=0.6891 va_auc=0.574 best=0.594@ep05\n",
      "[dir] ep 10 lr=1.90e-04 tr_loss=0.6700 va_loss=0.6925 va_auc=0.574 best=0.594@ep05\n",
      "[dir] ep 11 lr=1.63e-04 tr_loss=0.6577 va_loss=0.6998 va_auc=0.567 best=0.594@ep05\n",
      "[dir] ep 12 lr=1.36e-04 tr_loss=0.6420 va_loss=0.7217 va_auc=0.570 best=0.594@ep05\n",
      "\n",
      "Chosen thresholds on val_final:\n",
      "  thr_trade*=0.600 thr_dir*=0.500 | score=0.5248\n",
      "  val trade_rate(pred)=0.479 | val pnl_sum=0.5273 | val sharpe=1.748 | trades=541\n",
      "\n",
      "FINAL HOLDOUT RESULT (production-fit, fixed thresholds from val_final):\n",
      "  AUC trade=0.481 | AUC dir(trade-only)=0.512\n",
      "  trade_rate(pred)=0.791\n",
      "  pnl_sum=-0.6364 | pnl_mean=-0.000507 | trades=994\n",
      "  sharpe(per-bar proxy)=-1.613\n",
      "\n",
      "AUC summary (internal val_final vs holdout):\n",
      "  TRADE: val_auc=0.678 | holdout_auc=0.481\n",
      "  DIR  : val_auc=0.594 | holdout_auc=0.512\n",
      "\n",
      "Top-5 val_final threshold candidates:\n",
      "    thr_trade  thr_dir     score  trade_rate   pnl_sum  pnl_sharpe  n_trades\n",
      "6        0.60     0.50  0.524838    0.478761  0.527316    1.747783       541\n",
      "0        0.50     0.55  0.492649    0.467257  0.496278    1.791301       528\n",
      "10       0.65     0.50  0.467174    0.378761  0.479651    1.764755       428\n",
      "3        0.55     0.55  0.412860    0.371681  0.426046    1.651085       420\n",
      "13       0.70     0.50  0.366785    0.307080  0.386431    1.570939       347\n",
      "================================================================================\n",
      "\n",
      "Production-fit summary dict:\n",
      "{'thr_trade': 0.6, 'thr_dir': 0.5, 'val_true_trade_rate': 0.5035398230088496, 'hold_true_trade_rate': 0.5764331210191083, 'holdout_trade_auc': 0.48149898226228555, 'holdout_dir_auc': 0.5116117157490396, 'n': 1256, 'n_trades': 994, 'trade_rate': 0.7914012738853503, 'pnl_sum': -0.6364453434944153, 'pnl_mean': -0.0005067239981144667, 'pnl_per_trade': -0.0006402870640158653, 'pnl_sharpe': -1.6130572938788232}\n"
     ]
    }
   ],
   "source": [
    "# Step 12 — Production-fit: train on CV-part (90%) and evaluate on FINAL holdout (10%)\n",
    "\n",
    "def run_production_fit() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Train on the full CV-part (90%) with a final validation window (val_final),\n",
    "    select thresholds on val_final, then evaluate on FINAL holdout (10%).\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STEP 12 — PRODUCTION-FIT (TRAIN ON CV(90%) → SELECT THR ON val_final → EVAL ON FINAL HOLDOUT(10%))\")\n",
    "\n",
    "    val_w = max(1, int(CFG[\"val_window_frac\"] * n_samples_cv))\n",
    "    train_end = n_samples_cv - val_w\n",
    "\n",
    "    idx_train_final = np.arange(0, train_end, dtype=np.int64)\n",
    "    idx_val_final = np.arange(train_end, n_samples_cv, dtype=np.int64)\n",
    "    idx_holdout = idx_final_test.astype(np.int64)\n",
    "\n",
    "    true_val_trade = split_trade_ratio(idx_val_final, sample_t, y_trade)\n",
    "    true_hold_trade = split_trade_ratio(idx_holdout, sample_t, y_trade)\n",
    "\n",
    "    print(\"Final split sizes:\")\n",
    "    print(\"  train_final:\", len(idx_train_final))\n",
    "    print(\"  val_final  :\", len(idx_val_final))\n",
    "    print(\"  holdout    :\", len(idx_holdout))\n",
    "    print(f\"True trade ratio (val_final): {true_val_trade:.3f}\")\n",
    "    print(f\"True trade ratio (holdout):   {true_hold_trade:.3f}\")\n",
    "\n",
    "    X_scaled, _ = fit_scale_nodes_train_only(X_node_raw, sample_t, idx_train_final, max_abs=CFG[\"max_abs_feat\"])\n",
    "    if bool(CFG.get(\"edge_scale\", True)):\n",
    "        edge_scaled, _ = fit_scale_edges_train_only(edge_feat, sample_t, idx_train_final, max_abs=CFG[\"max_abs_edge\"])\n",
    "    else:\n",
    "        edge_scaled = edge_feat\n",
    "\n",
    "    # Stage A\n",
    "    m_trade, r_trade = train_binary_classifier(\n",
    "        X_scaled, edge_scaled, y_trade, y_dir, exit_ret, sample_t,\n",
    "        idx_train_final, idx_val_final, idx_holdout, CFG, stage_name=\"trade\"\n",
    "    )\n",
    "\n",
    "    # Stage B (trade-only)\n",
    "    idx_train_T = subset_trade_indices(idx_train_final, sample_t, y_trade)\n",
    "    idx_val_T = subset_trade_indices(idx_val_final, sample_t, y_trade)\n",
    "    idx_hold_T = subset_trade_indices(idx_holdout, sample_t, y_trade)\n",
    "\n",
    "    print(\"\\nTrade-only sizes for DIR:\")\n",
    "    print(\"  train_final_T:\", len(idx_train_T))\n",
    "    print(\"  val_final_T  :\", len(idx_val_T))\n",
    "    print(\"  holdout_T    :\", len(idx_hold_T))\n",
    "\n",
    "    m_dir, r_dir = train_binary_classifier(\n",
    "        X_scaled, edge_scaled, y_trade, y_dir, exit_ret, sample_t,\n",
    "        idx_train_T, idx_val_T, idx_hold_T, CFG, stage_name=\"dir\"\n",
    "    )\n",
    "\n",
    "    # Choose thresholds on val_final\n",
    "    prob_trade_val, er_val = predict_probs_on_indices(m_trade, X_scaled, edge_scaled, idx_val_final, CFG)\n",
    "    prob_dir_val, _ = predict_probs_on_indices(m_dir, X_scaled, edge_scaled, idx_val_final, CFG)\n",
    "\n",
    "    sweep_val = sweep_thresholds(\n",
    "        prob_trade_val, prob_dir_val, er_val, CFG,\n",
    "        min_trades=int(CFG[\"eval_min_trades\"]),\n",
    "        target_trade_rate=float(true_val_trade),\n",
    "    )\n",
    "    best_val = sweep_val.iloc[0].to_dict()\n",
    "    thr_trade_star = float(best_val[\"thr_trade\"])\n",
    "    thr_dir_star = float(best_val[\"thr_dir\"])\n",
    "\n",
    "    val_metrics = two_stage_pnl_by_threshold(prob_trade_val, prob_dir_val, er_val, thr_trade_star, thr_dir_star, CFG[\"cost_bps\"])\n",
    "\n",
    "    print(\"\\nChosen thresholds on val_final:\")\n",
    "    print(f\"  thr_trade*={thr_trade_star:.3f} thr_dir*={thr_dir_star:.3f} | score={best_val['score']:.4f}\")\n",
    "    print(f\"  val trade_rate(pred)={val_metrics['trade_rate']:.3f} | val pnl_sum={val_metrics['pnl_sum']:.4f} | val sharpe={val_metrics['pnl_sharpe']:.3f} | trades={val_metrics['n_trades']}\")\n",
    "\n",
    "    # Evaluate holdout with fixed thresholds\n",
    "    prob_trade_hold, er_hold = predict_probs_on_indices(m_trade, X_scaled, edge_scaled, idx_holdout, CFG)\n",
    "    prob_dir_hold, _ = predict_probs_on_indices(m_dir, X_scaled, edge_scaled, idx_holdout, CFG)\n",
    "    hold_metrics = two_stage_pnl_by_threshold(prob_trade_hold, prob_dir_hold, er_hold, thr_trade_star, thr_dir_star, CFG[\"cost_bps\"])\n",
    "\n",
    "    # Holdout AUCs\n",
    "    t_hold = sample_t[idx_holdout]\n",
    "    y_trade_hold = y_trade[t_hold].astype(np.int64)\n",
    "    y_dir_hold = y_dir[t_hold].astype(np.int64)\n",
    "\n",
    "    trade_auc_hold = _safe_auc_binary(y_trade_hold, prob_trade_hold[:, 1])\n",
    "    mask_true_trade = (y_trade_hold == 1)\n",
    "    dir_auc_hold = _safe_auc_binary(y_dir_hold[mask_true_trade], prob_dir_hold[mask_true_trade, 1])\n",
    "\n",
    "    print(\"\\nFINAL HOLDOUT RESULT (production-fit, fixed thresholds from val_final):\")\n",
    "    print(f\"  AUC trade={trade_auc_hold:.3f} | AUC dir(trade-only)={dir_auc_hold:.3f}\")\n",
    "    print(f\"  trade_rate(pred)={hold_metrics['trade_rate']:.3f}\")\n",
    "    print(f\"  pnl_sum={hold_metrics['pnl_sum']:.4f} | pnl_mean={hold_metrics['pnl_mean']:.6f} | trades={hold_metrics['n_trades']}\")\n",
    "    print(f\"  sharpe(per-bar proxy)={hold_metrics['pnl_sharpe']:.3f}\")\n",
    "\n",
    "    print(\"\\nAUC summary (internal val_final vs holdout):\")\n",
    "    print(f\"  TRADE: val_auc={r_trade['val']['auc']:.3f} | holdout_auc={trade_auc_hold:.3f}\")\n",
    "    print(f\"  DIR  : val_auc={r_dir['val']['auc']:.3f} | holdout_auc={dir_auc_hold:.3f}\")\n",
    "\n",
    "    print(\"\\nTop-5 val_final threshold candidates:\")\n",
    "    print(sweep_val.head(5)[[\"thr_trade\", \"thr_dir\", \"score\", \"trade_rate\", \"pnl_sum\", \"pnl_sharpe\", \"n_trades\"]])\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    return {\n",
    "        \"thr_trade\": thr_trade_star,\n",
    "        \"thr_dir\": thr_dir_star,\n",
    "        \"val_true_trade_rate\": float(true_val_trade),\n",
    "        \"hold_true_trade_rate\": float(true_hold_trade),\n",
    "        \"holdout_trade_auc\": float(trade_auc_hold) if np.isfinite(trade_auc_hold) else np.nan,\n",
    "        \"holdout_dir_auc\": float(dir_auc_hold) if np.isfinite(dir_auc_hold) else np.nan,\n",
    "        **hold_metrics,\n",
    "    }\n",
    "\n",
    "\n",
    "prod_fit_result = run_production_fit()\n",
    "print(\"\\nProduction-fit summary dict:\")\n",
    "print(prod_fit_result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recbole_new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
