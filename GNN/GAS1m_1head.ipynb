{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two-stage LOB GNN (SGA-TCN) -> Single-head 3-class (down/flat/up)\n",
    "# VS Code / Jupyter compatible (.py) using # %% cells\n",
    "#\n",
    "# Key principles:\n",
    "# - Time-ordered splits (no leakage)\n",
    "# - Scaling fit on train timeline only\n",
    "# - Single-head 3-class CE/Focal loss\n",
    "# - Thresholds (thr_trade, thr_dir) tuned ONLY on validation, then fixed on test/holdout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0 — Imports + reproducibility + config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cpu\n",
      "EDGE_INDEX (with self-loops): [[0, 1], [0, 2], [2, 1], [0, 0], [1, 1], [2, 2]]\n"
     ]
    }
   ],
   "source": [
    "# Step 0 — Imports + reproducibility + config\n",
    "\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# Reproducibility / performance\n",
    "# ----------------------------\n",
    "def seed_everything(seed: int = 100) -> None:\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # deterministic (safe defaults)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # precision/perf knobs (harmless on CPU; useful on CUDA)\n",
    "    try:\n",
    "        torch.set_float32_matmul_precision(\"high\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    if torch.cuda.is_available():\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "seed_everything(100)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "\n",
    "# CPU threading: avoid oversubscription; allow override via env\n",
    "_cpu = os.cpu_count() or 4\n",
    "torch_threads = int(os.environ.get(\"TORCH_NUM_THREADS\", str(min(8, _cpu))))\n",
    "interop_threads = int(os.environ.get(\"TORCH_NUM_INTEROP_THREADS\", \"1\"))\n",
    "torch.set_num_threads(max(1, torch_threads))\n",
    "try:\n",
    "    torch.set_num_interop_threads(max(1, interop_threads))\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ----------------------------\n",
    "# Config (tuned defaults)\n",
    "# ----------------------------\n",
    "def _grid(start: float, stop: float, step: float) -> List[float]:\n",
    "    # inclusive stop (with float safety)\n",
    "    xs = []\n",
    "    x = start\n",
    "    while x <= stop + 1e-12:\n",
    "        xs.append(float(round(x, 3)))\n",
    "        x += step\n",
    "    return xs\n",
    "\n",
    "CFG: Dict = {\n",
    "    # data\n",
    "    \"freq\": \"1min\",\n",
    "    \"data_dir\": Path(\"../dataset\"),\n",
    "    \"final_test_frac\": 0.10,  # 10% final holdout by time\n",
    "\n",
    "    # order book\n",
    "    \"book_levels\": 15,\n",
    "    \"top_levels\": 5,\n",
    "    \"near_levels\": 5,\n",
    "\n",
    "    # walk-forward windows (in sample-space)\n",
    "    # чуть больше train, чуть мельче шаг => больше фолдов и стабильнее оценка\n",
    "    \"train_min_frac\": 0.55,\n",
    "    \"val_window_frac\": 0.10,\n",
    "    \"test_window_frac\": 0.10,\n",
    "    \"step_window_frac\": 0.05,\n",
    "\n",
    "    # scaling\n",
    "    \"max_abs_feat\": 10.0,\n",
    "\n",
    "    # correlations\n",
    "    \"corr_windows\": [6 * 5, 12 * 5, 24 * 5, 48 * 5, 84 * 5],  # 30m,1h,2h,4h,7h\n",
    "    \"edges\": [(\"ADA\", \"BTC\"), (\"ADA\", \"ETH\"), (\"ETH\", \"BTC\")],\n",
    "\n",
    "    # triple-barrier (оставил как есть, чтобы не менять смысл разметки)\n",
    "    \"tb_horizon\": 1 * 15,      # 15 min\n",
    "    \"lookback\": 2 * 12 * 5,    # 2 hours => 120\n",
    "    \"tb_pt_mult\": 1.2,\n",
    "    \"tb_sl_mult\": 1.1,\n",
    "    \"tb_min_barrier\": 0.001,\n",
    "    \"tb_max_barrier\": 0.006,\n",
    "\n",
    "    # training (CPU-friendly, но чуть “живее” по оптимизации)\n",
    "    \"batch_size\": 128,\n",
    "    \"epochs\": 60,              # early stopping внутри train_3c_classifier всё равно остановит раньше\n",
    "    \"lr\": 4e-4,\n",
    "    \"weight_decay\": 3e-4,\n",
    "    \"grad_clip\": 0.7,\n",
    "    \"dropout\": 0.15,\n",
    "\n",
    "    # loss (лучше калибровка вероятностей для thresholding)\n",
    "    \"loss_name\": \"ce\",         # \"ce\" or \"focal\"\n",
    "    \"label_smoothing\": 0.00,   # было 0.02; для порогов/калибровки чаще лучше 0\n",
    "    \"focal_gamma\": 1.5,        # если переключишь loss_name=\"focal\"\n",
    "    \"focal_alpha\": 1.00,       # в твоей реализации это глобальный множитель\n",
    "\n",
    "    # model\n",
    "    \"hidden\": 128,\n",
    "    \"gnn_layers\": 2,\n",
    "\n",
    "    # --- SGA (spatial)\n",
    "    \"gat_heads\": 4,\n",
    "\n",
    "    # --- TCN (temporal) — увеличиваем receptive field под lookback=120\n",
    "    \"tcn_channels\": 128,\n",
    "    \"tcn_layers\": 5,\n",
    "    \"tcn_kernel\": 3,\n",
    "    \"tcn_dropout\": 0.15,\n",
    "    \"tcn_causal\": True,\n",
    "    \"tcn_pool\": \"last\",        # causal-safe\n",
    "\n",
    "    # trading eval\n",
    "    \"cost_bps\": 1.0,\n",
    "\n",
    "    # threshold sweep grids (val only) — более плотные и покрывают твои найденные значения\n",
    "    \"thr_trade_grid\": _grid(0.20, 0.90, 0.05),\n",
    "    \"thr_dir_grid\":   _grid(0.50, 0.85, 0.05),\n",
    "\n",
    "    # min trades constraints (avoid \"best=0 trades\" / слишком малые выборки)\n",
    "    \"eval_min_trades\": 100,\n",
    "\n",
    "    # dynamic quantile thresholds for thr_trade based on predicted p_trade\n",
    "    \"proxy_target_trades\": [50, 100, 200, 300],\n",
    "}\n",
    "\n",
    "ASSETS = [\"ADA\", \"BTC\", \"ETH\"]\n",
    "ASSET2IDX = {a: i for i, a in enumerate(ASSETS)}\n",
    "TARGET_ASSET = \"ETH\"\n",
    "TARGET_NODE = ASSET2IDX[TARGET_ASSET]\n",
    "\n",
    "EDGE_INDEX = torch.tensor(\n",
    "    [[ASSET2IDX[s], ASSET2IDX[t]] for (s, t) in CFG[\"edges\"]],\n",
    "    dtype=torch.long\n",
    ")  # (E,2)\n",
    "\n",
    "def add_self_loops_edge_index(edge_index: torch.Tensor, num_nodes: int) -> torch.Tensor:\n",
    "    loops = torch.arange(num_nodes, dtype=edge_index.dtype).view(-1, 1)\n",
    "    loops = torch.cat([loops, loops], dim=1)\n",
    "    return torch.cat([edge_index, loops], dim=0)\n",
    "\n",
    "EDGE_INDEX = add_self_loops_edge_index(EDGE_INDEX, num_nodes=len(ASSETS))\n",
    "print(\"EDGE_INDEX (with self-loops):\", EDGE_INDEX.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 — Load data + log returns (keep format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded df: (12831, 106)\n",
      "Columns example: ['timestamp', 'ADA', 'spread_ADA', 'buys_ADA', 'sells_ADA', 'bids_vol_ADA_0', 'bids_vol_ADA_1', 'bids_vol_ADA_2', 'bids_vol_ADA_3', 'bids_vol_ADA_4', 'bids_vol_ADA_5', 'bids_vol_ADA_6', 'bids_vol_ADA_7', 'bids_vol_ADA_8', 'bids_vol_ADA_9', 'bids_vol_ADA_10', 'bids_vol_ADA_11', 'bids_vol_ADA_12', 'bids_vol_ADA_13', 'bids_vol_ADA_14']\n",
      "Time range: 2021-04-07 11:34:00+00:00 -> 2021-04-16 10:15:00+00:00\n",
      "                  timestamp      ADA  spread_ADA      buys_ADA      sells_ADA  \\\n",
      "0 2021-04-07 11:34:00+00:00  1.16205      0.0001  56936.467913  258248.957367   \n",
      "1 2021-04-07 11:35:00+00:00  1.16800      0.0022  56491.336799   78665.286640   \n",
      "\n",
      "   bids_vol_ADA_0  bids_vol_ADA_1  bids_vol_ADA_2  bids_vol_ADA_3  \\\n",
      "0      876.869995     5984.169922        5.810000       18.240000   \n",
      "1    33769.671875    23137.169922      550.299988      550.299988   \n",
      "\n",
      "   bids_vol_ADA_4  ...  asks_vol_ETH_8  asks_vol_ETH_9  asks_vol_ETH_10  \\\n",
      "0    19844.640625  ...      373.700012      196.699997      2059.709961   \n",
      "1    19012.320312  ...     3873.709961     1954.630005       197.039993   \n",
      "\n",
      "   asks_vol_ETH_11  asks_vol_ETH_12  asks_vol_ETH_13  asks_vol_ETH_14  \\\n",
      "0      3874.989990      5901.209961       178.289993     28512.160156   \n",
      "1     12661.990234     20006.970703     28562.310547      3874.379883   \n",
      "\n",
      "     lr_ADA    lr_BTC    lr_ETH  \n",
      "0  0.000000  0.000000  0.000000  \n",
      "1  0.005107  0.000937  0.001931  \n",
      "\n",
      "[2 rows x 106 columns]\n"
     ]
    }
   ],
   "source": [
    "def load_asset(asset: str, freq: str, data_dir: Path, book_levels: int, part: Tuple[int, int] = (0, 80)) -> pd.DataFrame:\n",
    "    path = data_dir / f\"{asset}_{freq}.csv\"\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.iloc[int(len(df) * part[0] / 100): int(len(df) * part[1] / 100)]\n",
    "\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"system_time\"]).dt.round(\"min\")\n",
    "    df = df.sort_values(\"timestamp\").set_index(\"timestamp\")\n",
    "\n",
    "    bid_cols = [f\"bids_notional_{i}\" for i in range(book_levels)]\n",
    "    ask_cols = [f\"asks_notional_{i}\" for i in range(book_levels)]\n",
    "\n",
    "    needed = [\"midpoint\", \"spread\", \"buys\", \"sells\"] + bid_cols + ask_cols\n",
    "    missing = [c for c in needed if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"{asset}: missing columns in CSV: {missing[:10]}{'...' if len(missing) > 10 else ''}\")\n",
    "\n",
    "    return df[needed]\n",
    "\n",
    "def load_all_assets() -> pd.DataFrame:\n",
    "    freq = CFG[\"freq\"]\n",
    "    data_dir = CFG[\"data_dir\"]\n",
    "    book_levels = CFG[\"book_levels\"]\n",
    "\n",
    "    def rename_cols(df_one: pd.DataFrame, asset: str) -> pd.DataFrame:\n",
    "        rename_map = {\n",
    "            \"midpoint\": asset,\n",
    "            \"buys\": f\"buys_{asset}\",\n",
    "            \"sells\": f\"sells_{asset}\",\n",
    "            \"spread\": f\"spread_{asset}\",\n",
    "        }\n",
    "        for i in range(book_levels):\n",
    "            rename_map[f\"bids_notional_{i}\"] = f\"bids_vol_{asset}_{i}\"\n",
    "            rename_map[f\"asks_notional_{i}\"] = f\"asks_vol_{asset}_{i}\"\n",
    "        return df_one.rename(columns=rename_map)\n",
    "\n",
    "    df_ada = rename_cols(load_asset(\"ADA\", freq, data_dir, book_levels, part=(0, 75)), \"ADA\")\n",
    "    df_btc = rename_cols(load_asset(\"BTC\", freq, data_dir, book_levels, part=(0, 75)), \"BTC\")\n",
    "    df_eth = rename_cols(load_asset(\"ETH\", freq, data_dir, book_levels, part=(0, 75)), \"ETH\")\n",
    "\n",
    "    df = df_ada.join(df_btc).join(df_eth).reset_index()\n",
    "    return df\n",
    "\n",
    "df = load_all_assets()\n",
    "for a in ASSETS:\n",
    "    df[f\"lr_{a}\"] = np.log(df[a]).diff().fillna(0.0)\n",
    "\n",
    "print(\"Loaded df:\", df.shape)\n",
    "print(\"Columns example:\", df.columns[:20].tolist())\n",
    "print(\"Time range:\", df[\"timestamp\"].min(), \"->\", df[\"timestamp\"].max())\n",
    "print(df.head(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 — Multi-window correlations -> edge features (T,E,W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corr_array shape: (12831, 3, 5) (T,E,W)\n",
      "edge_feat sample [t=100, all edges, all windows]:\n",
      " [[0.59103256 0.65377206 0.68285054 0.68285054 0.68285054]\n",
      " [0.5606864  0.5984133  0.7032436  0.7032436  0.7032436 ]\n",
      " [0.7517272  0.8926965  0.8496662  0.8496662  0.8496662 ]]\n"
     ]
    }
   ],
   "source": [
    "def build_corr_array(df_: pd.DataFrame, corr_windows: List[int], edges: List[Tuple[str, str]]) -> np.ndarray:\n",
    "    T_ = len(df_)\n",
    "    n_edges = len(edges)\n",
    "    n_w = len(corr_windows)\n",
    "\n",
    "    out = np.zeros((T_, n_edges, n_w), dtype=np.float32)\n",
    "\n",
    "    # Fast explicit version for the default 3 edges\n",
    "    # If you add edges, switch to the generic loop version below.\n",
    "    if edges == [(\"ADA\", \"BTC\"), (\"ADA\", \"ETH\"), (\"ETH\", \"BTC\")]:\n",
    "        for wi, w in enumerate(corr_windows):\n",
    "            r_ada_btc = df_[\"lr_ADA\"].rolling(w, min_periods=1).corr(df_[\"lr_BTC\"])\n",
    "            r_ada_eth = df_[\"lr_ADA\"].rolling(w, min_periods=1).corr(df_[\"lr_ETH\"])\n",
    "            r_eth_btc = df_[\"lr_ETH\"].rolling(w, min_periods=1).corr(df_[\"lr_BTC\"])\n",
    "\n",
    "            out[:, 0, wi] = np.nan_to_num(r_ada_btc)\n",
    "            out[:, 1, wi] = np.nan_to_num(r_ada_eth)\n",
    "            out[:, 2, wi] = np.nan_to_num(r_eth_btc)\n",
    "        return out\n",
    "\n",
    "    # Generic version for any edge list\n",
    "    for wi, w in enumerate(corr_windows):\n",
    "        for ei, (s, t) in enumerate(edges):\n",
    "            rs = df_[f\"lr_{s}\"].rolling(w, min_periods=1).corr(df_[f\"lr_{t}\"])\n",
    "            out[:, ei, wi] = np.nan_to_num(rs)\n",
    "\n",
    "    return out\n",
    "\n",
    "corr_array = build_corr_array(df, CFG[\"corr_windows\"], CFG[\"edges\"])\n",
    "edge_feat = np.nan_to_num(corr_array.astype(np.float32), nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "print(\"corr_array shape:\", corr_array.shape, \"(T,E,W)\")\n",
    "print(\"edge_feat sample [t=100, all edges, all windows]:\\n\", edge_feat[100])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 — Triple-barrier labels -> y_tb (0=down,1=flat,2=up) + exit_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TB dist [down,flat,up]: [2503 8148 2180]\n",
      "Trade ratio (true): 0.3649754500818331\n"
     ]
    }
   ],
   "source": [
    "def triple_barrier_labels_from_lr(\n",
    "    lr: pd.Series,\n",
    "    horizon: int,\n",
    "    vol_window: int,\n",
    "    pt_mult: float,\n",
    "    sl_mult: float,\n",
    "    min_barrier: float,\n",
    "    max_barrier: float,\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      y_tb: {0=down, 1=flat/no-trade, 2=up}\n",
    "      exit_ret: realized log-return to exit (tp/sl/timeout)\n",
    "      exit_t: exit index\n",
    "      thr: barrier per t (float array, len T)\n",
    "    No leakage: vol is shift(1).\n",
    "    \"\"\"\n",
    "    lr = lr.astype(float).copy()\n",
    "    T = len(lr)\n",
    "\n",
    "    vol = lr.rolling(vol_window, min_periods=max(10, vol_window // 10)).std().shift(1)\n",
    "    thr = (vol * np.sqrt(horizon)).clip(lower=min_barrier, upper=max_barrier)\n",
    "\n",
    "    y = np.ones(T, dtype=np.int64)\n",
    "    exit_ret = np.zeros(T, dtype=np.float32)\n",
    "    exit_t = np.arange(T, dtype=np.int64)\n",
    "\n",
    "    lr_np = lr.fillna(0.0).to_numpy(dtype=np.float64)\n",
    "    thr_np = thr.fillna(min_barrier).to_numpy(dtype=np.float64)\n",
    "\n",
    "    for t in range(T - horizon - 1):\n",
    "        up = pt_mult * thr_np[t]\n",
    "        dn = -sl_mult * thr_np[t]\n",
    "\n",
    "        cum = 0.0\n",
    "        hit = 1\n",
    "        et = t + horizon\n",
    "        er = 0.0\n",
    "\n",
    "        for dt in range(1, horizon + 1):\n",
    "            cum += lr_np[t + dt]\n",
    "            if cum >= up:\n",
    "                hit, et, er = 2, t + dt, cum\n",
    "                break\n",
    "            if cum <= dn:\n",
    "                hit, et, er = 0, t + dt, cum\n",
    "                break\n",
    "\n",
    "        if hit == 1:\n",
    "            er = float(np.sum(lr_np[t + 1: t + horizon + 1]))\n",
    "            et = t + horizon\n",
    "\n",
    "        y[t] = hit\n",
    "        exit_ret[t] = er\n",
    "        exit_t[t] = et\n",
    "\n",
    "    return y, exit_ret, exit_t, thr_np\n",
    "\n",
    "y_tb, exit_ret, exit_t, tb_thr = triple_barrier_labels_from_lr(\n",
    "    df[\"lr_ETH\"],\n",
    "    horizon=CFG[\"tb_horizon\"],\n",
    "    vol_window=CFG[\"lookback\"],\n",
    "    pt_mult=CFG[\"tb_pt_mult\"],\n",
    "    sl_mult=CFG[\"tb_sl_mult\"],\n",
    "    min_barrier=CFG[\"tb_min_barrier\"],\n",
    "    max_barrier=CFG[\"tb_max_barrier\"],\n",
    ")\n",
    "\n",
    "dist = np.bincount(y_tb, minlength=3)\n",
    "print(\"TB dist [down,flat,up]:\", dist)\n",
    "print(\"Trade ratio (true):\", float((y_tb != 1).mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 — Build node tensor (T,N,F) + sample_t (valid indices in sample-space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_node_raw: (12831, 3, 15) edge_feat: (12831, 3, 5)\n",
      "node_feat_names: ['lr', 'spread', 'log_buys', 'log_sells', 'ofi', 'DI_15', 'DI_L0', 'DI_L1', 'DI_L2', 'DI_L3', 'DI_L4', 'near_ratio_bid', 'near_ratio_ask', 'di_near', 'di_far']\n",
      "n_samples: 12696 | t range: 119 -> 12814\n",
      "Feature stats (TARGET asset, lr): mean= 1.5748046280350536e-05 std= 0.0010532913729548454\n"
     ]
    }
   ],
   "source": [
    "EPS = 1e-6\n",
    "\n",
    "def safe_log1p(x: np.ndarray) -> np.ndarray:\n",
    "    return np.log1p(np.maximum(x, 0.0))\n",
    "\n",
    "def build_node_tensor(df_: pd.DataFrame) -> Tuple[np.ndarray, List[str]]:\n",
    "    \"\"\"\n",
    "    Features per asset:\n",
    "      lr, spread,\n",
    "      log_buys, log_sells, ofi,\n",
    "      DI_15,\n",
    "      DI_L0..DI_L4,\n",
    "      near_ratio_bid, near_ratio_ask,\n",
    "      di_near, di_far\n",
    "    \"\"\"\n",
    "    book_levels = CFG[\"book_levels\"]\n",
    "    top_k = CFG[\"top_levels\"]\n",
    "    near_k = CFG[\"near_levels\"]\n",
    "\n",
    "    if near_k >= book_levels:\n",
    "        raise ValueError(\"CFG['near_levels'] must be < CFG['book_levels']\")\n",
    "\n",
    "    feat_names = [\n",
    "        \"lr\", \"spread\",\n",
    "        \"log_buys\", \"log_sells\", \"ofi\",\n",
    "        \"DI_15\",\n",
    "        \"DI_L0\", \"DI_L1\", \"DI_L2\", \"DI_L3\", \"DI_L4\",\n",
    "        \"near_ratio_bid\", \"near_ratio_ask\",\n",
    "        \"di_near\", \"di_far\",\n",
    "    ]\n",
    "\n",
    "    feats_all = []\n",
    "    for a in ASSETS:\n",
    "        lr = df_[f\"lr_{a}\"].values.astype(np.float32)\n",
    "        spread = df_[f\"spread_{a}\"].values.astype(np.float32)\n",
    "\n",
    "        buys = df_[f\"buys_{a}\"].values.astype(np.float32)\n",
    "        sells = df_[f\"sells_{a}\"].values.astype(np.float32)\n",
    "\n",
    "        log_buys = safe_log1p(buys).astype(np.float32)\n",
    "        log_sells = safe_log1p(sells).astype(np.float32)\n",
    "\n",
    "        ofi = ((buys - sells) / (buys + sells + EPS)).astype(np.float32)\n",
    "\n",
    "        bids_lvls = np.stack([df_[f\"bids_vol_{a}_{i}\"].values.astype(np.float32) for i in range(book_levels)], axis=1)\n",
    "        asks_lvls = np.stack([df_[f\"asks_vol_{a}_{i}\"].values.astype(np.float32) for i in range(book_levels)], axis=1)\n",
    "\n",
    "        bid_sum = bids_lvls.sum(axis=1)\n",
    "        ask_sum = asks_lvls.sum(axis=1)\n",
    "        di_15 = ((bid_sum - ask_sum) / (bid_sum + ask_sum + EPS)).astype(np.float32)\n",
    "\n",
    "        di_levels = []\n",
    "        for i in range(top_k):\n",
    "            b = bids_lvls[:, i]\n",
    "            s = asks_lvls[:, i]\n",
    "            di_levels.append(((b - s) / (b + s + EPS)).astype(np.float32))\n",
    "        di_l0_4 = np.stack(di_levels, axis=1)  # (T,5)\n",
    "\n",
    "        bid_near = bids_lvls[:, :near_k].sum(axis=1)\n",
    "        ask_near = asks_lvls[:, :near_k].sum(axis=1)\n",
    "        bid_far = bids_lvls[:, near_k:].sum(axis=1)\n",
    "        ask_far = asks_lvls[:, near_k:].sum(axis=1)\n",
    "\n",
    "        near_ratio_bid = (bid_near / (bid_far + EPS)).astype(np.float32)\n",
    "        near_ratio_ask = (ask_near / (ask_far + EPS)).astype(np.float32)\n",
    "\n",
    "        di_near = ((bid_near - ask_near) / (bid_near + ask_near + EPS)).astype(np.float32)\n",
    "        di_far = ((bid_far - ask_far) / (bid_far + ask_far + EPS)).astype(np.float32)\n",
    "\n",
    "        Xa = np.column_stack([\n",
    "            lr, spread,\n",
    "            log_buys, log_sells, ofi,\n",
    "            di_15,\n",
    "            di_l0_4[:, 0], di_l0_4[:, 1], di_l0_4[:, 2], di_l0_4[:, 3], di_l0_4[:, 4],\n",
    "            near_ratio_bid, near_ratio_ask,\n",
    "            di_near, di_far,\n",
    "        ]).astype(np.float32)\n",
    "\n",
    "        feats_all.append(Xa)\n",
    "\n",
    "    X = np.stack(feats_all, axis=1).astype(np.float32)  # (T,N,F)\n",
    "    return X, feat_names\n",
    "\n",
    "X_node_raw, node_feat_names = build_node_tensor(df)\n",
    "T = len(df)\n",
    "L = CFG[\"lookback\"]\n",
    "H = CFG[\"tb_horizon\"]\n",
    "\n",
    "t_min = L - 1\n",
    "t_max = T - H - 2\n",
    "sample_t = np.arange(t_min, t_max + 1)\n",
    "n_samples = len(sample_t)\n",
    "\n",
    "print(\"X_node_raw:\", X_node_raw.shape, \"edge_feat:\", edge_feat.shape)\n",
    "print(\"node_feat_names:\", node_feat_names)\n",
    "print(\"n_samples:\", n_samples, \"| t range:\", int(sample_t[0]), \"->\", int(sample_t[-1]))\n",
    "print(\"Feature stats (TARGET asset, lr):\",\n",
    "      \"mean=\", float(X_node_raw[:, TARGET_NODE, node_feat_names.index(\"lr\")].mean()),\n",
    "      \"std=\", float(X_node_raw[:, TARGET_NODE, node_feat_names.index(\"lr\")].std()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 — Final holdout split (time-ordered) + walk-forward splits (CV-part only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holdout split:\n",
      "  n_samples total: 12696\n",
      "  n_samples CV   : 11426 (90.0%)\n",
      "  n_samples FINAL: 1270 (10.0%)\n",
      "  CV range   : 0 11425\n",
      "  FINAL range: 11426 12695\n",
      "\n",
      "Walk-forward folds: 6\n",
      "  fold 1: train=6284 | val=1142 | test=1142\n",
      "  fold 2: train=6855 | val=1142 | test=1142\n",
      "  fold 3: train=7426 | val=1142 | test=1142\n",
      "  fold 4: train=7997 | val=1142 | test=1142\n",
      "  fold 5: train=8568 | val=1142 | test=1142\n",
      "  fold 6: train=9139 | val=1142 | test=1142\n"
     ]
    }
   ],
   "source": [
    "def make_final_holdout_split(n_samples_: int, final_test_frac: float) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    if not (0.0 < final_test_frac < 0.5):\n",
    "        raise ValueError(\"final_test_frac should be in (0, 0.5)\")\n",
    "    n_final = max(1, int(round(final_test_frac * n_samples_)))\n",
    "    n_cv = n_samples_ - n_final\n",
    "    if n_cv <= 50:\n",
    "        raise ValueError(\"Too few samples left for CV after holdout split.\")\n",
    "    idx_cv = np.arange(0, n_cv, dtype=np.int64)\n",
    "    idx_final = np.arange(n_cv, n_samples_, dtype=np.int64)\n",
    "    return idx_cv, idx_final\n",
    "\n",
    "def make_walk_forward_splits(\n",
    "    n_samples_: int,\n",
    "    train_min_frac: float,\n",
    "    val_window_frac: float,\n",
    "    test_window_frac: float,\n",
    "    step_window_frac: float,\n",
    ") -> List[Tuple[np.ndarray, np.ndarray, np.ndarray]]:\n",
    "    train_min = int(train_min_frac * n_samples_)\n",
    "    val_w = max(1, int(val_window_frac * n_samples_))\n",
    "    test_w = max(1, int(test_window_frac * n_samples_))\n",
    "    step_w = max(1, int(step_window_frac * n_samples_))\n",
    "\n",
    "    splits = []\n",
    "    start = train_min\n",
    "    while True:\n",
    "        tr_end = start\n",
    "        va_end = tr_end + val_w\n",
    "        te_end = va_end + test_w\n",
    "        if te_end > n_samples_:\n",
    "            break\n",
    "\n",
    "        idx_train = np.arange(0, tr_end, dtype=np.int64)\n",
    "        idx_val = np.arange(tr_end, va_end, dtype=np.int64)\n",
    "        idx_test = np.arange(va_end, te_end, dtype=np.int64)\n",
    "        splits.append((idx_train, idx_val, idx_test))\n",
    "\n",
    "        start += step_w\n",
    "\n",
    "    return splits\n",
    "\n",
    "idx_cv_all, idx_final_test = make_final_holdout_split(n_samples, CFG[\"final_test_frac\"])\n",
    "n_samples_cv = len(idx_cv_all)\n",
    "n_samples_final = len(idx_final_test)\n",
    "\n",
    "print(\"Holdout split:\")\n",
    "print(f\"  n_samples total: {n_samples}\")\n",
    "print(f\"  n_samples CV   : {n_samples_cv} ({100 * n_samples_cv / n_samples:.1f}%)\")\n",
    "print(f\"  n_samples FINAL: {n_samples_final} ({100 * n_samples_final / n_samples:.1f}%)\")\n",
    "print(\"  CV range   :\", int(idx_cv_all[0]), int(idx_cv_all[-1]))\n",
    "print(\"  FINAL range:\", int(idx_final_test[0]), int(idx_final_test[-1]))\n",
    "\n",
    "walk_splits = make_walk_forward_splits(\n",
    "    n_samples_=n_samples_cv,\n",
    "    train_min_frac=CFG[\"train_min_frac\"],\n",
    "    val_window_frac=CFG[\"val_window_frac\"],\n",
    "    test_window_frac=CFG[\"test_window_frac\"],\n",
    "    step_window_frac=CFG[\"step_window_frac\"],\n",
    ")\n",
    "\n",
    "print(\"\\nWalk-forward folds:\", len(walk_splits))\n",
    "for i, (a, b, c) in enumerate(walk_splits, 1):\n",
    "    print(f\"  fold {i}: train={len(a)} | val={len(b)} | test={len(c)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 — Dataset + scaling (train-only) + helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity: example class dist on all samples: [2477 8065 2154]\n"
     ]
    }
   ],
   "source": [
    "class LobGraphSequenceDataset3C(Dataset):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      x_seq: (L,N,F)\n",
    "      e_seq: (L,E,W)\n",
    "      y_tb : scalar in {0,1,2} (down/flat/up)\n",
    "      exit_ret: scalar\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        X_node: np.ndarray,\n",
    "        E_feat: np.ndarray,\n",
    "        y_tb_arr: np.ndarray,\n",
    "        exit_ret_arr: np.ndarray,\n",
    "        sample_t_: np.ndarray,\n",
    "        indices: np.ndarray,\n",
    "        lookback: int,\n",
    "    ):\n",
    "        self.X_node = X_node\n",
    "        self.E_feat = E_feat\n",
    "        self.y_tb = y_tb_arr\n",
    "        self.exit_ret = exit_ret_arr\n",
    "        self.sample_t = sample_t_\n",
    "        self.indices = indices.astype(np.int64)\n",
    "        self.L = int(lookback)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return int(len(self.indices))\n",
    "\n",
    "    def __getitem__(self, i: int):\n",
    "        sidx = int(self.indices[i])\n",
    "        t = int(self.sample_t[sidx])\n",
    "        t0 = t - self.L + 1\n",
    "\n",
    "        x_seq = self.X_node[t0:t + 1]  # (L,N,F)\n",
    "        e_seq = self.E_feat[t0:t + 1]  # (L,E,W)\n",
    "\n",
    "        y = int(self.y_tb[t])\n",
    "        er = float(self.exit_ret[t])\n",
    "\n",
    "        return (\n",
    "            torch.from_numpy(x_seq),\n",
    "            torch.from_numpy(e_seq),\n",
    "            torch.tensor(y, dtype=torch.long),\n",
    "            torch.tensor(er, dtype=torch.float32),\n",
    "        )\n",
    "\n",
    "def collate_fn_3c(batch):\n",
    "    xs, es, ys, ers = zip(*batch)\n",
    "    return (\n",
    "        torch.stack(xs, 0),   # (B,L,N,F)\n",
    "        torch.stack(es, 0),   # (B,L,E,W)\n",
    "        torch.stack(ys, 0),   # (B,)\n",
    "        torch.stack(ers, 0),  # (B,)\n",
    "    )\n",
    "\n",
    "def fit_scale_nodes_train_only(\n",
    "    X_node_raw_: np.ndarray,\n",
    "    sample_t_: np.ndarray,\n",
    "    idx_train: np.ndarray,\n",
    "    max_abs: float = 10.0\n",
    ") -> Tuple[np.ndarray, RobustScaler]:\n",
    "    \"\"\"\n",
    "    Fit scaler on all times up to last train sample time (no leakage).\n",
    "    \"\"\"\n",
    "    last_train_t = int(sample_t_[int(idx_train[-1])])\n",
    "    train_time_mask = np.arange(0, last_train_t + 1)\n",
    "\n",
    "    X_train_time = X_node_raw_[train_time_mask]  # (Ttr,N,F)\n",
    "    Ttr, N, Fdim = X_train_time.shape\n",
    "\n",
    "    scaler = RobustScaler(with_centering=True, with_scaling=True, quantile_range=(5.0, 95.0))\n",
    "    scaler.fit(X_train_time.reshape(-1, Fdim))\n",
    "\n",
    "    X_scaled = scaler.transform(X_node_raw_.reshape(-1, Fdim)).reshape(X_node_raw_.shape).astype(np.float32)\n",
    "    X_scaled = np.clip(X_scaled, -max_abs, max_abs).astype(np.float32)\n",
    "    X_scaled = np.nan_to_num(X_scaled, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n",
    "    return X_scaled, scaler\n",
    "\n",
    "def split_trade_ratio_3c(indices: np.ndarray, sample_t_: np.ndarray, y_tb_arr: np.ndarray) -> float:\n",
    "    tt = sample_t_[indices]\n",
    "    return float((y_tb_arr[tt] != 1).mean()) if len(tt) else float(\"nan\")\n",
    "\n",
    "def class_distribution_3c(indices: np.ndarray, sample_t_: np.ndarray, y_tb_arr: np.ndarray) -> np.ndarray:\n",
    "    tt = sample_t_[indices]\n",
    "    y = y_tb_arr[tt].astype(np.int64)\n",
    "    return np.bincount(y, minlength=3)\n",
    "\n",
    "print(\"Sanity: example class dist on all samples:\", np.bincount(y_tb[sample_t], minlength=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 — Model (SGA-TCN) — single-head 3-class logits (B,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model sanity logits: torch.Size([2, 3]) | finite: True\n"
     ]
    }
   ],
   "source": [
    "class SpatialGraphAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Attention with edge_attr in attention scorer:\n",
    "      score_e = a^T [h_src || h_dst || edge_emb]\n",
    "      attention normalized per-dst over incoming edges\n",
    "      msg = W_msg(h_src)\n",
    "      agg_dst = sum(attn * msg)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim: int, out_dim: int, edge_dim: int, heads: int = 1, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.in_dim = int(in_dim)\n",
    "        self.out_dim = int(out_dim)\n",
    "        self.heads = max(1, int(heads))\n",
    "        self.dropout = float(dropout)\n",
    "\n",
    "        self.head_dim = max(1, int(math.ceil(out_dim / self.heads)))\n",
    "        self.inner_dim = self.heads * self.head_dim\n",
    "\n",
    "        self.lin_node = nn.Linear(self.in_dim, self.inner_dim, bias=False)\n",
    "        self.lin_edge = nn.Linear(edge_dim, self.inner_dim, bias=False)\n",
    "        self.lin_msg = nn.Linear(self.inner_dim, self.inner_dim, bias=False)\n",
    "\n",
    "        self.attn_vec = nn.Parameter(torch.empty(self.heads, 3 * self.head_dim))\n",
    "\n",
    "        self.out_proj = nn.Linear(self.inner_dim, self.out_dim, bias=False)\n",
    "        self.res_proj = nn.Identity() if self.in_dim == self.out_dim else nn.Linear(self.in_dim, self.out_dim, bias=False)\n",
    "\n",
    "        self.ln = nn.LayerNorm(self.out_dim)\n",
    "        self.attn_drop = nn.Dropout(self.dropout)\n",
    "        self.out_drop = nn.Dropout(self.dropout)\n",
    "        self.act = nn.LeakyReLU(0.2)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        for m in [self.lin_node, self.lin_edge, self.lin_msg, self.out_proj]:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "        if isinstance(self.res_proj, nn.Linear):\n",
    "            nn.init.xavier_uniform_(self.res_proj.weight)\n",
    "        nn.init.xavier_uniform_(self.attn_vec)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, edge_attr: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (B,N,Fin)\n",
    "        edge_attr: (B,E_attr,W)\n",
    "        edge_index: (E_index,2)\n",
    "        \"\"\"\n",
    "        x = torch.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        edge_attr = torch.nan_to_num(edge_attr, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "        B, N, _ = x.shape\n",
    "        E_index = edge_index.shape[0]\n",
    "        E_attr = edge_attr.shape[1]\n",
    "        W = edge_attr.shape[2]\n",
    "\n",
    "        # pad edge_attr for self-loops if needed\n",
    "        if E_attr < E_index:\n",
    "            pad = torch.zeros((B, E_index - E_attr, W), device=edge_attr.device, dtype=edge_attr.dtype)\n",
    "            edge_attr = torch.cat([edge_attr, pad], dim=1)\n",
    "        elif E_attr > E_index:\n",
    "            edge_attr = edge_attr[:, :E_index, :]\n",
    "\n",
    "        src_idx = edge_index[:, 0]\n",
    "        dst_idx = edge_index[:, 1]\n",
    "\n",
    "        h = self.lin_node(x).view(B, N, self.heads, self.head_dim)                 # (B,N,Hh,dh)\n",
    "        eemb = self.lin_edge(edge_attr).view(B, E_index, self.heads, self.head_dim) # (B,E,Hh,dh)\n",
    "\n",
    "        h_src = h[:, src_idx, :, :]  # (B,E,Hh,dh)\n",
    "        h_dst = h[:, dst_idx, :, :]  # (B,E,Hh,dh)\n",
    "\n",
    "        cat = torch.cat([h_src, h_dst, eemb], dim=-1)  # (B,E,Hh,3*dh)\n",
    "        scores = (cat * self.attn_vec[None, None, :, :]).sum(dim=-1)  # (B,E,Hh)\n",
    "        scores = self.act(scores)\n",
    "\n",
    "        # softmax per destination node\n",
    "        alphas = torch.zeros_like(scores)\n",
    "        for n in range(N):\n",
    "            mask = (dst_idx == n)\n",
    "            if int(mask.sum()) == 0:\n",
    "                continue\n",
    "            s = scores[:, mask, :]\n",
    "            a = torch.softmax(s, dim=1)\n",
    "            a = self.attn_drop(a)\n",
    "            alphas[:, mask, :] = a\n",
    "\n",
    "        msg = self.lin_msg(h_src.reshape(B, E_index, self.inner_dim)).view(B, E_index, self.heads, self.head_dim)\n",
    "\n",
    "        agg = torch.zeros((B, N, self.heads, self.head_dim), device=x.device, dtype=x.dtype)\n",
    "        for e_i in range(E_index):\n",
    "            dst = int(dst_idx[e_i].item())\n",
    "            agg[:, dst, :, :] += alphas[:, e_i, :].unsqueeze(-1) * msg[:, e_i, :, :]\n",
    "\n",
    "        out = agg.reshape(B, N, self.inner_dim)\n",
    "        out = self.out_proj(out)\n",
    "        out = self.out_drop(out)\n",
    "\n",
    "        res = self.res_proj(x)\n",
    "        y = self.ln(res + out)\n",
    "        return torch.nan_to_num(y, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "class SpatialGraphAttentionMP(nn.Module):\n",
    "    \"\"\"Applies SpatialGraphAttentionLayer independently at each timestep.\"\"\"\n",
    "    def __init__(self, in_dim: int, hidden: int, edge_dim: int, heads: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.gat = SpatialGraphAttentionLayer(in_dim=in_dim, out_dim=hidden, edge_dim=edge_dim, heads=heads, dropout=dropout)\n",
    "\n",
    "    def forward(self, x_seq: torch.Tensor, e_seq: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:\n",
    "        B, L_, N, F_ = x_seq.shape\n",
    "        x_flat = x_seq.reshape(B * L_, N, F_)\n",
    "        e_flat = e_seq.reshape(B * L_, e_seq.size(2), e_seq.size(3))\n",
    "        h_flat = self.gat(x_flat, e_flat, edge_index)  # (B*L,N,H)\n",
    "        return h_flat.reshape(B, L_, N, -1)\n",
    "\n",
    "class CausalConv1d(nn.Module):\n",
    "    \"\"\"Causal Conv1d: pads only on the left => no future leakage.\"\"\"\n",
    "    def __init__(self, in_ch: int, out_ch: int, kernel_size: int, dilation: int = 1):\n",
    "        super().__init__()\n",
    "        self.kernel_size = int(kernel_size)\n",
    "        self.dilation = int(dilation)\n",
    "        self.conv = nn.Conv1d(in_ch, out_ch, kernel_size=self.kernel_size, dilation=self.dilation)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        pad_left = (self.kernel_size - 1) * self.dilation\n",
    "        x = F.pad(x, (pad_left, 0))\n",
    "        return self.conv(x)\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, in_ch: int, out_ch: int, kernel_size: int, dilation: int, dropout: float, causal: bool = True):\n",
    "        super().__init__()\n",
    "        self.causal = bool(causal)\n",
    "\n",
    "        if self.causal:\n",
    "            self.conv1 = CausalConv1d(in_ch, out_ch, kernel_size, dilation=dilation)\n",
    "            self.conv2 = CausalConv1d(out_ch, out_ch, kernel_size, dilation=dilation)\n",
    "        else:\n",
    "            pad = ((kernel_size - 1) * dilation) // 2\n",
    "            self.conv1 = nn.Conv1d(in_ch, out_ch, kernel_size, dilation=dilation, padding=pad)\n",
    "            self.conv2 = nn.Conv1d(out_ch, out_ch, kernel_size, dilation=dilation, padding=pad)\n",
    "\n",
    "        self.act = nn.GELU()\n",
    "        self.drop = nn.Dropout(float(dropout))\n",
    "        self.downsample = nn.Identity() if in_ch == out_ch else nn.Conv1d(in_ch, out_ch, kernel_size=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = torch.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        y = self.drop(self.act(self.conv1(x)))\n",
    "        y = self.drop(self.act(self.conv2(y)))\n",
    "        res = self.downsample(x)\n",
    "        return torch.nan_to_num(self.act(y + res), nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "class TemporalConvNet(nn.Module):\n",
    "    def __init__(self, in_ch: int, channels: List[int], kernel_size: int, dropout: float, causal: bool = True):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        cur = int(in_ch)\n",
    "        for i, out_ch in enumerate(channels):\n",
    "            dilation = 2 ** i\n",
    "            layers.append(TemporalBlock(cur, int(out_ch), int(kernel_size), int(dilation), float(dropout), causal=causal))\n",
    "            cur = int(out_ch)\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "class GNN_TCN_Classifier3C(nn.Module):\n",
    "    \"\"\"\n",
    "    input:\n",
    "      x_seq (B,L,N,F), e_seq (B,L,E,W), edge_index (E,2)\n",
    "    output:\n",
    "      logits (B,3) for {down, flat, up}\n",
    "    \"\"\"\n",
    "    def __init__(self, node_in: int, edge_dim: int, cfg: Dict, target_node: int, n_classes: int = 3):\n",
    "        super().__init__()\n",
    "        self.target_node = int(target_node)\n",
    "\n",
    "        hidden = int(cfg[\"hidden\"])\n",
    "        dropout = float(cfg[\"dropout\"])\n",
    "\n",
    "        gat_heads = int(cfg[\"gat_heads\"])\n",
    "        tcn_channels = int(cfg[\"tcn_channels\"])\n",
    "        tcn_layers_n = int(cfg[\"tcn_layers\"])\n",
    "        tcn_kernel = int(cfg[\"tcn_kernel\"])\n",
    "        tcn_dropout = float(cfg[\"tcn_dropout\"])\n",
    "        tcn_causal = bool(cfg[\"tcn_causal\"])\n",
    "        self.tcn_pool = str(cfg[\"tcn_pool\"])\n",
    "\n",
    "        # spatial stack\n",
    "        self.gnns = nn.ModuleList()\n",
    "        for i in range(int(cfg[\"gnn_layers\"])):\n",
    "            in_dim = int(node_in) if i == 0 else hidden\n",
    "            self.gnns.append(\n",
    "                SpatialGraphAttentionMP(\n",
    "                    in_dim=in_dim,\n",
    "                    hidden=hidden,\n",
    "                    edge_dim=int(edge_dim),\n",
    "                    heads=gat_heads,\n",
    "                    dropout=dropout,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # temporal\n",
    "        self.tcn_in = nn.Linear(hidden, tcn_channels)\n",
    "        self.tcn = TemporalConvNet(\n",
    "            in_ch=tcn_channels,\n",
    "            channels=[tcn_channels] * tcn_layers_n,\n",
    "            kernel_size=tcn_kernel,\n",
    "            dropout=tcn_dropout,\n",
    "            causal=tcn_causal,\n",
    "        )\n",
    "\n",
    "        # head\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(tcn_channels),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(tcn_channels, tcn_channels),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(tcn_channels, n_classes),\n",
    "        )\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, e: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:\n",
    "        x = torch.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        e = torch.nan_to_num(e, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "        h = x\n",
    "        for gnn in self.gnns:\n",
    "            h = gnn(h, e, edge_index)  # (B,L,N,H)\n",
    "\n",
    "        h_tgt = h[:, :, self.target_node, :]  # (B,L,H)\n",
    "        z = self.tcn_in(h_tgt)                # (B,L,C)\n",
    "        z = z.transpose(1, 2)                 # (B,C,L)\n",
    "\n",
    "        y = self.tcn(z)                       # (B,C,L)\n",
    "        emb = y[:, :, -1] if self.tcn_pool == \"last\" else y.mean(dim=-1)\n",
    "        logits = self.head(emb)               # (B,3)\n",
    "        return torch.nan_to_num(logits, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "# quick sanity check\n",
    "B_ = 2\n",
    "Fdim = X_node_raw.shape[-1]\n",
    "E_ = EDGE_INDEX.shape[0]\n",
    "W_ = edge_feat.shape[-1]\n",
    "x_dummy = torch.randn(B_, L, len(ASSETS), Fdim)\n",
    "e_dummy = torch.randn(B_, L, E_, W_)\n",
    "m_dummy = GNN_TCN_Classifier3C(node_in=Fdim, edge_dim=W_, cfg=CFG, target_node=TARGET_NODE).to(DEVICE)\n",
    "with torch.no_grad():\n",
    "    out = m_dummy(x_dummy.to(DEVICE), e_dummy.to(DEVICE), EDGE_INDEX.to(DEVICE))\n",
    "print(\"Model sanity logits:\", out.shape, \"| finite:\", bool(torch.isfinite(out).all().item()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8 — Train/Eval helpers (3-class + derived AUCs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ce_weights_3c(y_np: np.ndarray) -> torch.Tensor:\n",
    "    y_np = np.asarray(y_np, dtype=np.int64)\n",
    "    counts = np.bincount(y_np, minlength=3).astype(np.float64)\n",
    "    counts = np.maximum(counts, 1.0)\n",
    "    w = counts.sum() / (3.0 * counts)\n",
    "    return torch.tensor(w, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "class FocalLossMultiClass(nn.Module):\n",
    "    def __init__(self, weight: Optional[torch.Tensor] = None, gamma: float = 2.0, alpha: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.weight = weight\n",
    "        self.gamma = float(gamma)\n",
    "        self.alpha = float(alpha)\n",
    "\n",
    "    def forward(self, logits: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "        # logits: (B,C), y: (B,)\n",
    "        logp = F.log_softmax(logits, dim=-1)\n",
    "        p = torch.exp(logp)\n",
    "        y_onehot = F.one_hot(y, num_classes=logits.size(-1)).float()\n",
    "\n",
    "        pt = (p * y_onehot).sum(dim=-1).clamp_min(1e-12)\n",
    "        logpt = (logp * y_onehot).sum(dim=-1)\n",
    "\n",
    "        focal = ((1.0 - pt) ** self.gamma) * (-logpt)\n",
    "        if self.weight is not None:\n",
    "            w = self.weight[y].detach()\n",
    "            focal = focal * w\n",
    "        focal = self.alpha * focal\n",
    "        return focal.mean()\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_3c(model: nn.Module, loader: DataLoader, loss_fn: nn.Module) -> Dict:\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    n = 0\n",
    "\n",
    "    ys = []\n",
    "    probs = []\n",
    "    ers = []\n",
    "\n",
    "    for x, e, y, er in loader:\n",
    "        x = x.to(DEVICE).float()\n",
    "        e = e.to(DEVICE).float()\n",
    "        y = y.to(DEVICE).long()\n",
    "\n",
    "        logits = model(x, e, EDGE_INDEX.to(DEVICE))\n",
    "        loss = loss_fn(logits, y)\n",
    "\n",
    "        total_loss += float(loss.item()) * int(y.size(0))\n",
    "        n += int(y.size(0))\n",
    "\n",
    "        p = torch.softmax(logits, dim=-1).detach().cpu().numpy()\n",
    "        ys.append(y.detach().cpu().numpy())\n",
    "        probs.append(p)\n",
    "        ers.append(er.detach().cpu().numpy())\n",
    "\n",
    "    ys = np.concatenate(ys) if ys else np.array([], dtype=np.int64)\n",
    "    probs = np.concatenate(probs) if probs else np.zeros((0, 3), dtype=np.float32)\n",
    "    ers = np.concatenate(ers) if ers else np.array([], dtype=np.float32)\n",
    "\n",
    "    if len(ys) == 0:\n",
    "        return {\"loss\": np.nan, \"acc\": np.nan, \"f1m\": np.nan, \"cm\": None, \"trade_auc\": np.nan, \"dir_auc\": np.nan,\n",
    "                \"y\": ys, \"prob\": probs, \"er\": ers}\n",
    "\n",
    "    y_pred = probs.argmax(axis=1)\n",
    "    acc = accuracy_score(ys, y_pred)\n",
    "    f1m = f1_score(ys, y_pred, average=\"macro\")\n",
    "    cm = confusion_matrix(ys, y_pred, labels=[0,1,2])\n",
    "\n",
    "    # Derived AUCs:\n",
    "    # trade_auc: (trade vs flat) where trade means {down,up}\n",
    "    y_trade = (ys != 1).astype(np.int64)\n",
    "    p_trade = (probs[:, 0] + probs[:, 2]).astype(np.float64)\n",
    "    try:\n",
    "        trade_auc = roc_auc_score(y_trade, p_trade) if len(np.unique(y_trade)) == 2 else np.nan\n",
    "    except Exception:\n",
    "        trade_auc = np.nan\n",
    "\n",
    "    # dir_auc on trade-only: up vs down using conditional prob p_up / (p_up + p_down)\n",
    "    mask_tr = (ys != 1)\n",
    "    if mask_tr.sum() >= 10 and len(np.unique(ys[mask_tr])) == 2:\n",
    "        y_dir = (ys[mask_tr] == 2).astype(np.int64)\n",
    "        p_up = probs[mask_tr, 2].astype(np.float64)\n",
    "        p_dn = probs[mask_tr, 0].astype(np.float64)\n",
    "        p_up_cond = p_up / (p_up + p_dn + 1e-12)\n",
    "        try:\n",
    "            dir_auc = roc_auc_score(y_dir, p_up_cond) if len(np.unique(y_dir)) == 2 else np.nan\n",
    "        except Exception:\n",
    "            dir_auc = np.nan\n",
    "    else:\n",
    "        dir_auc = np.nan\n",
    "\n",
    "    return {\n",
    "        \"loss\": total_loss / max(1, n),\n",
    "        \"acc\": float(acc),\n",
    "        \"f1m\": float(f1m),\n",
    "        \"cm\": cm,\n",
    "        \"trade_auc\": float(trade_auc) if np.isfinite(trade_auc) else np.nan,\n",
    "        \"dir_auc\": float(dir_auc) if np.isfinite(dir_auc) else np.nan,\n",
    "        \"y\": ys,\n",
    "        \"prob\": probs,\n",
    "        \"er\": ers,\n",
    "    }\n",
    "\n",
    "def build_loss_3c(cfg: Dict, class_w: torch.Tensor) -> nn.Module:\n",
    "    name = str(cfg.get(\"loss_name\", \"ce\")).lower().strip()\n",
    "    if name == \"focal\":\n",
    "        return FocalLossMultiClass(weight=class_w, gamma=float(cfg[\"focal_gamma\"]), alpha=float(cfg[\"focal_alpha\"]))\n",
    "    # default CE\n",
    "    ls = float(cfg.get(\"label_smoothing\", 0.0))\n",
    "    return nn.CrossEntropyLoss(weight=class_w, label_smoothing=ls)\n",
    "\n",
    "def train_3c_classifier(\n",
    "    X_scaled: np.ndarray,\n",
    "    edge_feat_: np.ndarray,\n",
    "    y_tb_arr: np.ndarray,\n",
    "    exit_ret_arr: np.ndarray,\n",
    "    sample_t_: np.ndarray,\n",
    "    idx_train: np.ndarray,\n",
    "    idx_val: np.ndarray,\n",
    "    idx_test: np.ndarray,\n",
    "    cfg: Dict,\n",
    ") -> Tuple[nn.Module, Dict]:\n",
    "    L_ = int(cfg[\"lookback\"])\n",
    "    bs = int(cfg[\"batch_size\"])\n",
    "\n",
    "    tr_ds = LobGraphSequenceDataset3C(X_scaled, edge_feat_, y_tb_arr, exit_ret_arr, sample_t_, idx_train, L_)\n",
    "    va_ds = LobGraphSequenceDataset3C(X_scaled, edge_feat_, y_tb_arr, exit_ret_arr, sample_t_, idx_val,   L_)\n",
    "    te_ds = LobGraphSequenceDataset3C(X_scaled, edge_feat_, y_tb_arr, exit_ret_arr, sample_t_, idx_test,  L_)\n",
    "\n",
    "    tr_loader = DataLoader(tr_ds, batch_size=bs, shuffle=True,  drop_last=False, collate_fn=collate_fn_3c, num_workers=0)\n",
    "    va_loader = DataLoader(va_ds, batch_size=bs, shuffle=False, drop_last=False, collate_fn=collate_fn_3c, num_workers=0)\n",
    "    te_loader = DataLoader(te_ds, batch_size=bs, shuffle=False, drop_last=False, collate_fn=collate_fn_3c, num_workers=0)\n",
    "\n",
    "    node_in = int(X_scaled.shape[-1])\n",
    "    edge_dim = int(edge_feat_.shape[-1])\n",
    "    model = GNN_TCN_Classifier3C(node_in=node_in, edge_dim=edge_dim, cfg=cfg, target_node=TARGET_NODE).to(DEVICE)\n",
    "\n",
    "    # class weights from TRAIN only\n",
    "    t_train = sample_t_[idx_train]\n",
    "    y_train_np = y_tb_arr[t_train].astype(np.int64)\n",
    "    class_w = make_ce_weights_3c(y_train_np)\n",
    "    loss_fn = build_loss_3c(cfg, class_w)\n",
    "\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=float(cfg[\"lr\"]), weight_decay=float(cfg[\"weight_decay\"]))\n",
    "    sch = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"max\", factor=0.5, patience=3)\n",
    "\n",
    "    best_score = -1e18\n",
    "    best_state = None\n",
    "    best_epoch = -1\n",
    "\n",
    "    patience = 8\n",
    "    bad = 0\n",
    "\n",
    "    for ep in range(1, int(cfg[\"epochs\"]) + 1):\n",
    "        model.train()\n",
    "        tot_loss = 0.0\n",
    "        n = 0\n",
    "\n",
    "        for x, e, y, _er in tr_loader:\n",
    "            x = x.to(DEVICE).float()\n",
    "            e = e.to(DEVICE).float()\n",
    "            y = y.to(DEVICE).long()\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            logits = model(x, e, EDGE_INDEX.to(DEVICE))\n",
    "            loss = loss_fn(logits, y)\n",
    "            if not torch.isfinite(loss):\n",
    "                continue\n",
    "\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), float(cfg[\"grad_clip\"]))\n",
    "            opt.step()\n",
    "\n",
    "            tot_loss += float(loss.item()) * int(y.size(0))\n",
    "            n += int(y.size(0))\n",
    "\n",
    "        tr_loss = tot_loss / max(1, n)\n",
    "        va = eval_3c(model, va_loader, loss_fn)\n",
    "\n",
    "        # selection metric: macro-F1 (primary) + trade_auc (secondary)\n",
    "        sel = float(va[\"f1m\"]) + 0.10 * (float(va[\"trade_auc\"]) if np.isfinite(va[\"trade_auc\"]) else 0.0)\n",
    "\n",
    "        if sel > best_score:\n",
    "            best_score = sel\n",
    "            best_epoch = ep\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            bad = 0\n",
    "        else:\n",
    "            bad += 1\n",
    "\n",
    "        sch.step(sel)\n",
    "        lr_now = opt.param_groups[0][\"lr\"]\n",
    "\n",
    "        print(\n",
    "            f\"[3c] ep {ep:02d} lr={lr_now:.2e} \"\n",
    "            f\"tr_loss={tr_loss:.4f} va_loss={va['loss']:.4f} \"\n",
    "            f\"va_f1m={va['f1m']:.3f} trade_auc={va['trade_auc']:.3f} dir_auc={va['dir_auc']:.3f} \"\n",
    "            f\"best={best_score:.3f}@ep{best_epoch:02d}\"\n",
    "        )\n",
    "\n",
    "        if bad >= patience:\n",
    "            break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    va = eval_3c(model, va_loader, loss_fn)\n",
    "    te = eval_3c(model, te_loader, loss_fn)\n",
    "\n",
    "    res = {\n",
    "        \"best_epoch\": int(best_epoch),\n",
    "        \"best_val_score\": float(best_score) if np.isfinite(best_score) else np.nan,\n",
    "        \"val\": va,\n",
    "        \"test\": te,\n",
    "    }\n",
    "    return model, res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9 — Trading/PnL + threshold sweep (val only) using 3-class probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_trade_threshold_grid(\n",
    "    p_trade: np.ndarray,\n",
    "    base_grid: Optional[List[float]] = None,\n",
    "    target_trades_list: Optional[List[int]] = None,\n",
    "    min_thr: float = 0.01,\n",
    "    max_thr: float = 0.99,\n",
    ") -> List[float]:\n",
    "    p_trade = np.asarray(p_trade, dtype=np.float64)\n",
    "    p_trade = p_trade[np.isfinite(p_trade)]\n",
    "    if p_trade.size == 0:\n",
    "        return base_grid or [0.5]\n",
    "\n",
    "    thrs = set(float(t) for t in (base_grid or []))\n",
    "\n",
    "    if target_trades_list:\n",
    "        N = int(p_trade.size)\n",
    "        for k in target_trades_list:\n",
    "            k = int(k)\n",
    "            if k <= 0:\n",
    "                continue\n",
    "            if k >= N:\n",
    "                thr = float(np.min(p_trade))\n",
    "            else:\n",
    "                q = 1.0 - (k / N)\n",
    "                thr = float(np.quantile(p_trade, q))\n",
    "            thrs.add(float(np.clip(thr, min_thr, max_thr)))\n",
    "\n",
    "    out = sorted(thrs)\n",
    "    cleaned = []\n",
    "    for t in out:\n",
    "        if not cleaned or abs(t - cleaned[-1]) > 1e-6:\n",
    "            cleaned.append(float(t))\n",
    "    return cleaned\n",
    "\n",
    "def trade_mask_3c(prob_3c: np.ndarray, thr_trade: float, thr_dir: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    prob_3c: (N,3) [p_down, p_flat, p_up]\n",
    "    Trade decision:\n",
    "      p_trade = 1 - p_flat\n",
    "      dir_conf = max(p_up, p_down) / (p_up + p_down)\n",
    "      trade if p_trade >= thr_trade AND dir_conf >= thr_dir\n",
    "    \"\"\"\n",
    "    p_dn = prob_3c[:, 0]\n",
    "    p_fl = prob_3c[:, 1]\n",
    "    p_up = prob_3c[:, 2]\n",
    "\n",
    "    p_trade = 1.0 - p_fl\n",
    "    denom = (p_up + p_dn) + 1e-12\n",
    "    dir_conf = np.maximum(p_up, p_dn) / denom\n",
    "\n",
    "    return (p_trade >= float(thr_trade)) & (dir_conf >= float(thr_dir))\n",
    "\n",
    "def pnl_by_threshold_3c(\n",
    "    prob_3c: np.ndarray,\n",
    "    exit_ret_arr: np.ndarray,\n",
    "    thr_trade: float,\n",
    "    thr_dir: float,\n",
    "    cost_bps: float,\n",
    ") -> Dict:\n",
    "    p_dn = prob_3c[:, 0]\n",
    "    p_up = prob_3c[:, 2]\n",
    "    mask = trade_mask_3c(prob_3c, thr_trade, thr_dir)\n",
    "\n",
    "    action = np.zeros_like(exit_ret_arr, dtype=np.float32)\n",
    "    action[mask] = np.where(p_up[mask] >= p_dn[mask], 1.0, -1.0).astype(np.float32)\n",
    "\n",
    "    cost = (float(cost_bps) * 1e-4) * mask.astype(np.float32)\n",
    "    pnl = action * exit_ret_arr - cost\n",
    "\n",
    "    n = int(len(exit_ret_arr))\n",
    "    n_tr = int(mask.sum())\n",
    "\n",
    "    return {\n",
    "        \"n\": n,\n",
    "        \"n_trades\": n_tr,\n",
    "        \"trade_rate\": float(n_tr / max(1, n)),\n",
    "        \"pnl_sum\": float(pnl.sum()),\n",
    "        \"pnl_mean\": float(pnl.mean()) if n else np.nan,\n",
    "        \"pnl_per_trade\": float(pnl.sum() / max(1, n_tr)),\n",
    "        \"pnl_sharpe\": float((pnl.mean() / (pnl.std() + 1e-12)) * np.sqrt(288)) if n else np.nan,\n",
    "    }\n",
    "\n",
    "def sweep_thresholds_3c(\n",
    "    prob_3c: np.ndarray,\n",
    "    exit_ret_arr: np.ndarray,\n",
    "    cfg: Dict,\n",
    "    min_trades: int = 0,\n",
    ") -> pd.DataFrame:\n",
    "    p_trade = (1.0 - prob_3c[:, 1]).astype(np.float64)\n",
    "    thr_trade_grid = build_trade_threshold_grid(\n",
    "        p_trade=p_trade,\n",
    "        base_grid=cfg.get(\"thr_trade_grid\", [0.5]),\n",
    "        target_trades_list=cfg.get(\"proxy_target_trades\", None),\n",
    "        min_thr=0.01,\n",
    "        max_thr=0.99,\n",
    "    )\n",
    "    thr_dir_grid = cfg.get(\"thr_dir_grid\", [0.5])\n",
    "\n",
    "    rows = []\n",
    "    for thr_t in thr_trade_grid:\n",
    "        for thr_d in thr_dir_grid:\n",
    "            m = pnl_by_threshold_3c(prob_3c, exit_ret_arr, thr_t, thr_d, cfg[\"cost_bps\"])\n",
    "            if int(m[\"n_trades\"]) < int(min_trades):\n",
    "                continue\n",
    "            rows.append({\"thr_trade\": float(thr_t), \"thr_dir\": float(thr_d), **m})\n",
    "\n",
    "    if not rows and min_trades > 0:\n",
    "        return sweep_thresholds_3c(prob_3c, exit_ret_arr, cfg, min_trades=1)\n",
    "\n",
    "    if not rows:\n",
    "        for thr_t in thr_trade_grid:\n",
    "            for thr_d in thr_dir_grid:\n",
    "                m = pnl_by_threshold_3c(prob_3c, exit_ret_arr, thr_t, thr_d, cfg[\"cost_bps\"])\n",
    "                rows.append({\"thr_trade\": float(thr_t), \"thr_dir\": float(thr_d), **m})\n",
    "\n",
    "    df_ = pd.DataFrame(rows).sort_values([\"pnl_sum\", \"pnl_mean\"], ascending=False)\n",
    "    return df_\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_probs_on_indices_3c(model: nn.Module, X_scaled: np.ndarray, edge_feat_: np.ndarray, y_tb_arr: np.ndarray,\n",
    "                               exit_ret_arr: np.ndarray, indices: np.ndarray, cfg: Dict) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    ds = LobGraphSequenceDataset3C(X_scaled, edge_feat_, y_tb_arr, exit_ret_arr, sample_t, indices, cfg[\"lookback\"])\n",
    "    loader = DataLoader(ds, batch_size=int(cfg[\"batch_size\"]), shuffle=False, collate_fn=collate_fn_3c, num_workers=0)\n",
    "\n",
    "    model.eval()\n",
    "    probs = []\n",
    "    ers = []\n",
    "    for x, e, _y, er in loader:\n",
    "        x = x.to(DEVICE).float()\n",
    "        e = e.to(DEVICE).float()\n",
    "        logits = model(x, e, EDGE_INDEX.to(DEVICE))\n",
    "        p = torch.softmax(logits, dim=-1).cpu().numpy()\n",
    "        probs.append(p)\n",
    "        ers.append(er.cpu().numpy())\n",
    "\n",
    "    return np.concatenate(probs, axis=0), np.concatenate(ers, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10 — Run walk-forward folds (CV-part): train 3-class -> choose thresholds on VAL -> test PnL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FOLD 1/6 sizes: train=6284 val=1142 test=1142\n",
      "True trade ratio (val):  0.368\n",
      "True trade ratio (test): 0.304\n",
      "Train class dist: [1162, 4205, 917]\n",
      "Val   class dist: [224, 722, 196]\n",
      "Test  class dist: [142, 795, 205]\n",
      "[3c] ep 01 lr=4.00e-04 tr_loss=1.2229 va_loss=1.1030 va_f1m=0.296 trade_auc=0.528 dir_auc=0.522 best=0.349@ep01\n",
      "[3c] ep 02 lr=4.00e-04 tr_loss=1.1085 va_loss=1.1004 va_f1m=0.318 trade_auc=0.547 dir_auc=0.495 best=0.373@ep02\n",
      "[3c] ep 03 lr=4.00e-04 tr_loss=1.0876 va_loss=1.1082 va_f1m=0.261 trade_auc=0.583 dir_auc=0.537 best=0.373@ep02\n",
      "[3c] ep 04 lr=4.00e-04 tr_loss=1.0647 va_loss=1.0966 va_f1m=0.350 trade_auc=0.587 dir_auc=0.537 best=0.408@ep04\n",
      "[3c] ep 05 lr=4.00e-04 tr_loss=1.0335 va_loss=1.0889 va_f1m=0.362 trade_auc=0.610 dir_auc=0.566 best=0.423@ep05\n",
      "[3c] ep 06 lr=4.00e-04 tr_loss=0.9886 va_loss=1.0924 va_f1m=0.363 trade_auc=0.605 dir_auc=0.619 best=0.424@ep06\n",
      "[3c] ep 07 lr=4.00e-04 tr_loss=0.9040 va_loss=1.1596 va_f1m=0.392 trade_auc=0.620 dir_auc=0.603 best=0.454@ep07\n",
      "[3c] ep 08 lr=4.00e-04 tr_loss=0.8372 va_loss=1.2663 va_f1m=0.378 trade_auc=0.622 dir_auc=0.599 best=0.454@ep07\n",
      "[3c] ep 09 lr=4.00e-04 tr_loss=0.7550 va_loss=1.3324 va_f1m=0.393 trade_auc=0.594 dir_auc=0.639 best=0.454@ep07\n",
      "[3c] ep 10 lr=4.00e-04 tr_loss=0.6771 va_loss=1.4126 va_f1m=0.394 trade_auc=0.618 dir_auc=0.582 best=0.456@ep10\n",
      "[3c] ep 11 lr=4.00e-04 tr_loss=0.5936 va_loss=1.6369 va_f1m=0.400 trade_auc=0.607 dir_auc=0.566 best=0.461@ep11\n",
      "[3c] ep 12 lr=4.00e-04 tr_loss=0.5179 va_loss=1.5623 va_f1m=0.411 trade_auc=0.627 dir_auc=0.562 best=0.473@ep12\n",
      "[3c] ep 13 lr=4.00e-04 tr_loss=0.4515 va_loss=1.9909 va_f1m=0.372 trade_auc=0.618 dir_auc=0.548 best=0.473@ep12\n",
      "[3c] ep 14 lr=4.00e-04 tr_loss=0.3890 va_loss=2.1943 va_f1m=0.394 trade_auc=0.608 dir_auc=0.565 best=0.473@ep12\n",
      "[3c] ep 15 lr=4.00e-04 tr_loss=0.3576 va_loss=2.2009 va_f1m=0.371 trade_auc=0.586 dir_auc=0.537 best=0.473@ep12\n",
      "[3c] ep 16 lr=2.00e-04 tr_loss=0.3111 va_loss=2.1611 va_f1m=0.387 trade_auc=0.603 dir_auc=0.551 best=0.473@ep12\n",
      "[3c] ep 17 lr=2.00e-04 tr_loss=0.2445 va_loss=2.6103 va_f1m=0.391 trade_auc=0.603 dir_auc=0.548 best=0.473@ep12\n",
      "[3c] ep 18 lr=2.00e-04 tr_loss=0.2004 va_loss=2.8423 va_f1m=0.378 trade_auc=0.601 dir_auc=0.549 best=0.473@ep12\n",
      "[3c] ep 19 lr=2.00e-04 tr_loss=0.1935 va_loss=3.0384 va_f1m=0.389 trade_auc=0.595 dir_auc=0.527 best=0.473@ep12\n",
      "[3c] ep 20 lr=1.00e-04 tr_loss=0.1732 va_loss=3.2129 va_f1m=0.382 trade_auc=0.598 dir_auc=0.512 best=0.473@ep12\n",
      "\n",
      "Chosen thresholds (from VAL):\n",
      "  thr_trade*=0.200 thr_dir*=0.500 | val trade_rate(pred)=0.842 | val pnl_sum=0.3933\n",
      "TEST (fixed thr from VAL):\n",
      "  trade_rate(pred)=0.882 | pnl_sum=-0.1577 | pnl_mean=-0.000138 | trades=1007\n",
      "3C TEST metrics: acc=0.412 f1m=0.342 trade_auc=0.588 dir_auc=0.482\n",
      "\n",
      "================================================================================\n",
      "FOLD 2/6 sizes: train=6855 val=1142 test=1142\n",
      "True trade ratio (val):  0.308\n",
      "True trade ratio (test): 0.391\n",
      "Train class dist: [1237, 4552, 1066]\n",
      "Val   class dist: [215, 790, 137]\n",
      "Test  class dist: [194, 695, 253]\n",
      "[3c] ep 01 lr=4.00e-04 tr_loss=1.2437 va_loss=1.0834 va_f1m=0.386 trade_auc=0.556 dir_auc=0.538 best=0.442@ep01\n",
      "[3c] ep 02 lr=4.00e-04 tr_loss=1.1180 va_loss=1.1019 va_f1m=0.229 trade_auc=0.576 dir_auc=0.612 best=0.442@ep01\n",
      "[3c] ep 03 lr=4.00e-04 tr_loss=1.0989 va_loss=1.1231 va_f1m=0.218 trade_auc=0.563 dir_auc=0.583 best=0.442@ep01\n",
      "[3c] ep 04 lr=4.00e-04 tr_loss=1.0896 va_loss=1.0603 va_f1m=0.386 trade_auc=0.612 dir_auc=0.635 best=0.447@ep04\n",
      "[3c] ep 05 lr=4.00e-04 tr_loss=1.0449 va_loss=1.0978 va_f1m=0.325 trade_auc=0.617 dir_auc=0.613 best=0.447@ep04\n",
      "[3c] ep 06 lr=4.00e-04 tr_loss=1.0154 va_loss=1.1046 va_f1m=0.339 trade_auc=0.591 dir_auc=0.680 best=0.447@ep04\n",
      "[3c] ep 07 lr=4.00e-04 tr_loss=0.9312 va_loss=1.1137 va_f1m=0.348 trade_auc=0.604 dir_auc=0.685 best=0.447@ep04\n",
      "[3c] ep 08 lr=2.00e-04 tr_loss=0.8826 va_loss=1.1324 va_f1m=0.382 trade_auc=0.612 dir_auc=0.667 best=0.447@ep04\n",
      "[3c] ep 09 lr=2.00e-04 tr_loss=0.7738 va_loss=1.2836 va_f1m=0.366 trade_auc=0.599 dir_auc=0.664 best=0.447@ep04\n",
      "[3c] ep 10 lr=2.00e-04 tr_loss=0.7035 va_loss=1.2904 va_f1m=0.370 trade_auc=0.594 dir_auc=0.674 best=0.447@ep04\n",
      "[3c] ep 11 lr=2.00e-04 tr_loss=0.6441 va_loss=1.4157 va_f1m=0.369 trade_auc=0.591 dir_auc=0.688 best=0.447@ep04\n",
      "[3c] ep 12 lr=2.00e-04 tr_loss=0.5906 va_loss=1.3986 va_f1m=0.390 trade_auc=0.609 dir_auc=0.678 best=0.451@ep12\n",
      "[3c] ep 13 lr=2.00e-04 tr_loss=0.5482 va_loss=1.4257 va_f1m=0.397 trade_auc=0.604 dir_auc=0.701 best=0.458@ep13\n",
      "[3c] ep 14 lr=2.00e-04 tr_loss=0.5103 va_loss=1.6271 va_f1m=0.339 trade_auc=0.581 dir_auc=0.688 best=0.458@ep13\n",
      "[3c] ep 15 lr=2.00e-04 tr_loss=0.4693 va_loss=1.6498 va_f1m=0.382 trade_auc=0.583 dir_auc=0.682 best=0.458@ep13\n",
      "[3c] ep 16 lr=2.00e-04 tr_loss=0.4289 va_loss=1.7584 va_f1m=0.402 trade_auc=0.603 dir_auc=0.681 best=0.462@ep16\n",
      "[3c] ep 17 lr=2.00e-04 tr_loss=0.4033 va_loss=1.8052 va_f1m=0.382 trade_auc=0.597 dir_auc=0.674 best=0.462@ep16\n",
      "[3c] ep 18 lr=2.00e-04 tr_loss=0.3715 va_loss=1.9720 va_f1m=0.404 trade_auc=0.585 dir_auc=0.690 best=0.463@ep18\n",
      "[3c] ep 19 lr=2.00e-04 tr_loss=0.3415 va_loss=1.9005 va_f1m=0.428 trade_auc=0.600 dir_auc=0.688 best=0.488@ep19\n",
      "[3c] ep 20 lr=2.00e-04 tr_loss=0.3248 va_loss=1.9925 va_f1m=0.383 trade_auc=0.577 dir_auc=0.664 best=0.488@ep19\n",
      "[3c] ep 21 lr=2.00e-04 tr_loss=0.3004 va_loss=2.0829 va_f1m=0.389 trade_auc=0.570 dir_auc=0.687 best=0.488@ep19\n",
      "[3c] ep 22 lr=2.00e-04 tr_loss=0.2943 va_loss=2.0611 va_f1m=0.401 trade_auc=0.600 dir_auc=0.675 best=0.488@ep19\n",
      "[3c] ep 23 lr=1.00e-04 tr_loss=0.2589 va_loss=2.1557 va_f1m=0.385 trade_auc=0.587 dir_auc=0.679 best=0.488@ep19\n",
      "[3c] ep 24 lr=1.00e-04 tr_loss=0.2110 va_loss=2.3695 va_f1m=0.398 trade_auc=0.585 dir_auc=0.674 best=0.488@ep19\n",
      "[3c] ep 25 lr=1.00e-04 tr_loss=0.2108 va_loss=2.3462 va_f1m=0.396 trade_auc=0.584 dir_auc=0.683 best=0.488@ep19\n",
      "[3c] ep 26 lr=1.00e-04 tr_loss=0.1880 va_loss=2.5248 va_f1m=0.390 trade_auc=0.585 dir_auc=0.672 best=0.488@ep19\n",
      "[3c] ep 27 lr=5.00e-05 tr_loss=0.1846 va_loss=2.4614 va_f1m=0.400 trade_auc=0.587 dir_auc=0.683 best=0.488@ep19\n",
      "\n",
      "Chosen thresholds (from VAL):\n",
      "  thr_trade*=0.550 thr_dir*=0.500 | val trade_rate(pred)=0.449 | val pnl_sum=0.2953\n",
      "TEST (fixed thr from VAL):\n",
      "  trade_rate(pred)=0.440 | pnl_sum=-0.1056 | pnl_mean=-0.000092 | trades=502\n",
      "3C TEST metrics: acc=0.456 f1m=0.355 trade_auc=0.578 dir_auc=0.465\n",
      "\n",
      "================================================================================\n",
      "FOLD 3/6 sizes: train=7426 val=1142 test=1142\n",
      "True trade ratio (val):  0.304\n",
      "True trade ratio (test): 0.405\n",
      "Train class dist: [1386, 4927, 1113]\n",
      "Val   class dist: [142, 795, 205]\n",
      "Test  class dist: [188, 679, 275]\n",
      "[3c] ep 01 lr=4.00e-04 tr_loss=1.1993 va_loss=1.1136 va_f1m=0.179 trade_auc=0.506 dir_auc=0.519 best=0.229@ep01\n",
      "[3c] ep 02 lr=4.00e-04 tr_loss=1.1125 va_loss=1.0891 va_f1m=0.263 trade_auc=0.560 dir_auc=0.507 best=0.319@ep02\n",
      "[3c] ep 03 lr=4.00e-04 tr_loss=1.0958 va_loss=1.0840 va_f1m=0.255 trade_auc=0.544 dir_auc=0.500 best=0.319@ep02\n",
      "[3c] ep 04 lr=4.00e-04 tr_loss=1.0704 va_loss=1.0839 va_f1m=0.355 trade_auc=0.562 dir_auc=0.482 best=0.411@ep04\n",
      "[3c] ep 05 lr=4.00e-04 tr_loss=1.0150 va_loss=1.1621 va_f1m=0.173 trade_auc=0.591 dir_auc=0.490 best=0.411@ep04\n",
      "[3c] ep 06 lr=4.00e-04 tr_loss=0.9724 va_loss=1.1942 va_f1m=0.266 trade_auc=0.584 dir_auc=0.466 best=0.411@ep04\n",
      "[3c] ep 07 lr=4.00e-04 tr_loss=0.8944 va_loss=1.2162 va_f1m=0.332 trade_auc=0.607 dir_auc=0.502 best=0.411@ep04\n",
      "[3c] ep 08 lr=4.00e-04 tr_loss=0.8183 va_loss=1.3990 va_f1m=0.349 trade_auc=0.637 dir_auc=0.476 best=0.413@ep08\n",
      "[3c] ep 09 lr=4.00e-04 tr_loss=0.7284 va_loss=1.5506 va_f1m=0.358 trade_auc=0.612 dir_auc=0.497 best=0.419@ep09\n",
      "[3c] ep 10 lr=4.00e-04 tr_loss=0.6478 va_loss=1.6792 va_f1m=0.349 trade_auc=0.622 dir_auc=0.504 best=0.419@ep09\n",
      "[3c] ep 11 lr=4.00e-04 tr_loss=0.5809 va_loss=2.1625 va_f1m=0.336 trade_auc=0.607 dir_auc=0.474 best=0.419@ep09\n",
      "[3c] ep 12 lr=4.00e-04 tr_loss=0.5259 va_loss=2.1808 va_f1m=0.340 trade_auc=0.626 dir_auc=0.447 best=0.419@ep09\n",
      "[3c] ep 13 lr=2.00e-04 tr_loss=0.4374 va_loss=2.4948 va_f1m=0.354 trade_auc=0.629 dir_auc=0.448 best=0.419@ep09\n",
      "[3c] ep 14 lr=2.00e-04 tr_loss=0.3634 va_loss=2.4217 va_f1m=0.380 trade_auc=0.620 dir_auc=0.506 best=0.442@ep14\n",
      "[3c] ep 15 lr=2.00e-04 tr_loss=0.3258 va_loss=2.4701 va_f1m=0.376 trade_auc=0.622 dir_auc=0.497 best=0.442@ep14\n",
      "[3c] ep 16 lr=2.00e-04 tr_loss=0.3067 va_loss=2.6892 va_f1m=0.378 trade_auc=0.620 dir_auc=0.483 best=0.442@ep14\n",
      "[3c] ep 17 lr=2.00e-04 tr_loss=0.2833 va_loss=2.8958 va_f1m=0.366 trade_auc=0.614 dir_auc=0.498 best=0.442@ep14\n",
      "[3c] ep 18 lr=1.00e-04 tr_loss=0.2725 va_loss=2.6392 va_f1m=0.361 trade_auc=0.637 dir_auc=0.495 best=0.442@ep14\n",
      "[3c] ep 19 lr=1.00e-04 tr_loss=0.2263 va_loss=3.0256 va_f1m=0.369 trade_auc=0.623 dir_auc=0.501 best=0.442@ep14\n",
      "[3c] ep 20 lr=1.00e-04 tr_loss=0.2039 va_loss=3.0018 va_f1m=0.375 trade_auc=0.618 dir_auc=0.504 best=0.442@ep14\n",
      "[3c] ep 21 lr=1.00e-04 tr_loss=0.1972 va_loss=3.0977 va_f1m=0.365 trade_auc=0.615 dir_auc=0.502 best=0.442@ep14\n",
      "[3c] ep 22 lr=5.00e-05 tr_loss=0.1854 va_loss=3.2347 va_f1m=0.368 trade_auc=0.617 dir_auc=0.502 best=0.442@ep14\n",
      "\n",
      "Chosen thresholds (from VAL):\n",
      "  thr_trade*=0.800 thr_dir*=0.750 | val trade_rate(pred)=0.253 | val pnl_sum=0.0075\n",
      "TEST (fixed thr from VAL):\n",
      "  trade_rate(pred)=0.293 | pnl_sum=0.3306 | pnl_mean=0.000289 | trades=335\n",
      "3C TEST metrics: acc=0.493 f1m=0.414 trade_auc=0.611 dir_auc=0.577\n",
      "\n",
      "================================================================================\n",
      "FOLD 4/6 sizes: train=7997 val=1142 test=1142\n",
      "True trade ratio (val):  0.391\n",
      "True trade ratio (test): 0.423\n",
      "Train class dist: [1452, 5342, 1203]\n",
      "Val   class dist: [194, 695, 253]\n",
      "Test  class dist: [249, 659, 234]\n",
      "[3c] ep 01 lr=4.00e-04 tr_loss=1.2188 va_loss=1.0980 va_f1m=0.295 trade_auc=0.513 dir_auc=0.526 best=0.346@ep01\n",
      "[3c] ep 02 lr=4.00e-04 tr_loss=1.1116 va_loss=1.0979 va_f1m=0.247 trade_auc=0.529 dir_auc=0.535 best=0.346@ep01\n",
      "[3c] ep 03 lr=4.00e-04 tr_loss=1.0927 va_loss=1.0882 va_f1m=0.297 trade_auc=0.556 dir_auc=0.528 best=0.353@ep03\n",
      "[3c] ep 04 lr=4.00e-04 tr_loss=1.0754 va_loss=1.1071 va_f1m=0.294 trade_auc=0.562 dir_auc=0.546 best=0.353@ep03\n",
      "[3c] ep 05 lr=4.00e-04 tr_loss=1.0425 va_loss=1.1270 va_f1m=0.289 trade_auc=0.557 dir_auc=0.548 best=0.353@ep03\n",
      "[3c] ep 06 lr=4.00e-04 tr_loss=0.9999 va_loss=1.1807 va_f1m=0.276 trade_auc=0.567 dir_auc=0.518 best=0.353@ep03\n",
      "[3c] ep 07 lr=4.00e-04 tr_loss=0.9315 va_loss=1.2616 va_f1m=0.296 trade_auc=0.575 dir_auc=0.510 best=0.354@ep07\n",
      "[3c] ep 08 lr=4.00e-04 tr_loss=0.8415 va_loss=1.4046 va_f1m=0.320 trade_auc=0.562 dir_auc=0.478 best=0.377@ep08\n",
      "[3c] ep 09 lr=4.00e-04 tr_loss=0.7411 va_loss=1.5832 va_f1m=0.301 trade_auc=0.559 dir_auc=0.498 best=0.377@ep08\n",
      "[3c] ep 10 lr=4.00e-04 tr_loss=0.6697 va_loss=1.7051 va_f1m=0.285 trade_auc=0.561 dir_auc=0.460 best=0.377@ep08\n",
      "[3c] ep 11 lr=4.00e-04 tr_loss=0.5884 va_loss=1.9832 va_f1m=0.331 trade_auc=0.558 dir_auc=0.440 best=0.387@ep11\n",
      "[3c] ep 12 lr=4.00e-04 tr_loss=0.5241 va_loss=2.2514 va_f1m=0.316 trade_auc=0.544 dir_auc=0.434 best=0.387@ep11\n",
      "[3c] ep 13 lr=4.00e-04 tr_loss=0.4595 va_loss=2.2788 va_f1m=0.330 trade_auc=0.561 dir_auc=0.480 best=0.387@ep11\n",
      "[3c] ep 14 lr=4.00e-04 tr_loss=0.3943 va_loss=2.8292 va_f1m=0.314 trade_auc=0.515 dir_auc=0.449 best=0.387@ep11\n",
      "[3c] ep 15 lr=2.00e-04 tr_loss=0.3552 va_loss=2.8381 va_f1m=0.314 trade_auc=0.530 dir_auc=0.431 best=0.387@ep11\n",
      "[3c] ep 16 lr=2.00e-04 tr_loss=0.2849 va_loss=3.2278 va_f1m=0.306 trade_auc=0.533 dir_auc=0.416 best=0.387@ep11\n",
      "[3c] ep 17 lr=2.00e-04 tr_loss=0.2397 va_loss=3.5369 va_f1m=0.309 trade_auc=0.524 dir_auc=0.428 best=0.387@ep11\n",
      "[3c] ep 18 lr=2.00e-04 tr_loss=0.2253 va_loss=3.5116 va_f1m=0.318 trade_auc=0.544 dir_auc=0.453 best=0.387@ep11\n",
      "[3c] ep 19 lr=1.00e-04 tr_loss=0.1993 va_loss=3.9723 va_f1m=0.321 trade_auc=0.551 dir_auc=0.427 best=0.387@ep11\n",
      "\n",
      "Chosen thresholds (from VAL):\n",
      "  thr_trade*=0.864 thr_dir*=0.500 | val trade_rate(pred)=0.088 | val pnl_sum=-0.0574\n",
      "TEST (fixed thr from VAL):\n",
      "  trade_rate(pred)=0.077 | pnl_sum=-0.0236 | pnl_mean=-0.000021 | trades=88\n",
      "3C TEST metrics: acc=0.527 f1m=0.350 trade_auc=0.632 dir_auc=0.482\n",
      "\n",
      "================================================================================\n",
      "FOLD 5/6 sizes: train=8568 val=1142 test=1142\n",
      "True trade ratio (val):  0.405\n",
      "True trade ratio (test): 0.489\n",
      "Train class dist: [1528, 5722, 1318]\n",
      "Val   class dist: [188, 679, 275]\n",
      "Test  class dist: [311, 583, 248]\n",
      "[3c] ep 01 lr=4.00e-04 tr_loss=1.2381 va_loss=1.1123 va_f1m=0.222 trade_auc=0.561 dir_auc=0.531 best=0.278@ep01\n",
      "[3c] ep 02 lr=4.00e-04 tr_loss=1.1134 va_loss=1.0782 va_f1m=0.298 trade_auc=0.555 dir_auc=0.547 best=0.353@ep02\n",
      "[3c] ep 03 lr=4.00e-04 tr_loss=1.0944 va_loss=1.1073 va_f1m=0.338 trade_auc=0.561 dir_auc=0.595 best=0.395@ep03\n",
      "[3c] ep 04 lr=4.00e-04 tr_loss=1.0688 va_loss=1.0421 va_f1m=0.359 trade_auc=0.591 dir_auc=0.648 best=0.419@ep04\n",
      "[3c] ep 05 lr=4.00e-04 tr_loss=1.0333 va_loss=1.0877 va_f1m=0.338 trade_auc=0.570 dir_auc=0.635 best=0.419@ep04\n",
      "[3c] ep 06 lr=4.00e-04 tr_loss=0.9790 va_loss=1.1238 va_f1m=0.342 trade_auc=0.570 dir_auc=0.626 best=0.419@ep04\n",
      "[3c] ep 07 lr=4.00e-04 tr_loss=0.9249 va_loss=1.2569 va_f1m=0.346 trade_auc=0.551 dir_auc=0.541 best=0.419@ep04\n",
      "[3c] ep 08 lr=2.00e-04 tr_loss=0.8285 va_loss=1.3759 va_f1m=0.286 trade_auc=0.550 dir_auc=0.571 best=0.419@ep04\n",
      "[3c] ep 09 lr=2.00e-04 tr_loss=0.7230 va_loss=1.4744 va_f1m=0.365 trade_auc=0.548 dir_auc=0.565 best=0.420@ep09\n",
      "[3c] ep 10 lr=2.00e-04 tr_loss=0.6595 va_loss=1.5277 va_f1m=0.347 trade_auc=0.557 dir_auc=0.538 best=0.420@ep09\n",
      "[3c] ep 11 lr=2.00e-04 tr_loss=0.6051 va_loss=1.6827 va_f1m=0.361 trade_auc=0.543 dir_auc=0.543 best=0.420@ep09\n",
      "[3c] ep 12 lr=2.00e-04 tr_loss=0.5574 va_loss=1.8594 va_f1m=0.370 trade_auc=0.540 dir_auc=0.524 best=0.424@ep12\n",
      "[3c] ep 13 lr=2.00e-04 tr_loss=0.5135 va_loss=1.8078 va_f1m=0.325 trade_auc=0.531 dir_auc=0.522 best=0.424@ep12\n",
      "[3c] ep 14 lr=2.00e-04 tr_loss=0.4752 va_loss=2.0812 va_f1m=0.349 trade_auc=0.509 dir_auc=0.538 best=0.424@ep12\n",
      "[3c] ep 15 lr=2.00e-04 tr_loss=0.4452 va_loss=2.1983 va_f1m=0.361 trade_auc=0.534 dir_auc=0.513 best=0.424@ep12\n",
      "[3c] ep 16 lr=1.00e-04 tr_loss=0.4017 va_loss=2.1686 va_f1m=0.357 trade_auc=0.535 dir_auc=0.524 best=0.424@ep12\n",
      "[3c] ep 17 lr=1.00e-04 tr_loss=0.3619 va_loss=2.3482 va_f1m=0.378 trade_auc=0.545 dir_auc=0.519 best=0.432@ep17\n",
      "[3c] ep 18 lr=1.00e-04 tr_loss=0.3314 va_loss=2.5756 va_f1m=0.359 trade_auc=0.527 dir_auc=0.516 best=0.432@ep17\n",
      "[3c] ep 19 lr=1.00e-04 tr_loss=0.3114 va_loss=2.6332 va_f1m=0.371 trade_auc=0.527 dir_auc=0.513 best=0.432@ep17\n",
      "[3c] ep 20 lr=1.00e-04 tr_loss=0.3115 va_loss=2.7999 va_f1m=0.351 trade_auc=0.529 dir_auc=0.503 best=0.432@ep17\n",
      "[3c] ep 21 lr=5.00e-05 tr_loss=0.2894 va_loss=2.7992 va_f1m=0.359 trade_auc=0.526 dir_auc=0.506 best=0.432@ep17\n",
      "[3c] ep 22 lr=5.00e-05 tr_loss=0.2701 va_loss=2.9349 va_f1m=0.355 trade_auc=0.526 dir_auc=0.513 best=0.432@ep17\n",
      "[3c] ep 23 lr=5.00e-05 tr_loss=0.2648 va_loss=3.1784 va_f1m=0.344 trade_auc=0.512 dir_auc=0.514 best=0.432@ep17\n",
      "[3c] ep 24 lr=5.00e-05 tr_loss=0.2489 va_loss=3.0735 va_f1m=0.355 trade_auc=0.523 dir_auc=0.519 best=0.432@ep17\n",
      "[3c] ep 25 lr=2.50e-05 tr_loss=0.2429 va_loss=3.2629 va_f1m=0.352 trade_auc=0.520 dir_auc=0.515 best=0.432@ep17\n",
      "\n",
      "Chosen thresholds (from VAL):\n",
      "  thr_trade*=0.550 thr_dir*=0.750 | val trade_rate(pred)=0.376 | val pnl_sum=0.1929\n",
      "TEST (fixed thr from VAL):\n",
      "  trade_rate(pred)=0.322 | pnl_sum=0.1085 | pnl_mean=0.000095 | trades=368\n",
      "3C TEST metrics: acc=0.480 f1m=0.380 trade_auc=0.605 dir_auc=0.566\n",
      "\n",
      "================================================================================\n",
      "FOLD 6/6 sizes: train=9139 val=1142 test=1142\n",
      "True trade ratio (val):  0.423\n",
      "True trade ratio (test): 0.428\n",
      "Train class dist: [1646, 6037, 1456]\n",
      "Val   class dist: [249, 659, 234]\n",
      "Test  class dist: [249, 653, 240]\n",
      "[3c] ep 01 lr=4.00e-04 tr_loss=1.1863 va_loss=1.0978 va_f1m=0.261 trade_auc=0.577 dir_auc=0.521 best=0.319@ep01\n",
      "[3c] ep 02 lr=4.00e-04 tr_loss=1.1072 va_loss=1.0985 va_f1m=0.217 trade_auc=0.661 dir_auc=0.526 best=0.319@ep01\n",
      "[3c] ep 03 lr=4.00e-04 tr_loss=1.0848 va_loss=1.0752 va_f1m=0.313 trade_auc=0.676 dir_auc=0.553 best=0.380@ep03\n",
      "[3c] ep 04 lr=4.00e-04 tr_loss=1.0650 va_loss=1.1248 va_f1m=0.369 trade_auc=0.659 dir_auc=0.480 best=0.435@ep04\n",
      "[3c] ep 05 lr=4.00e-04 tr_loss=1.0246 va_loss=1.1479 va_f1m=0.351 trade_auc=0.649 dir_auc=0.453 best=0.435@ep04\n",
      "[3c] ep 06 lr=4.00e-04 tr_loss=0.9635 va_loss=1.2578 va_f1m=0.309 trade_auc=0.634 dir_auc=0.410 best=0.435@ep04\n",
      "[3c] ep 07 lr=4.00e-04 tr_loss=0.8986 va_loss=1.4357 va_f1m=0.275 trade_auc=0.604 dir_auc=0.425 best=0.435@ep04\n",
      "[3c] ep 08 lr=2.00e-04 tr_loss=0.7984 va_loss=1.6513 va_f1m=0.331 trade_auc=0.617 dir_auc=0.411 best=0.435@ep04\n",
      "[3c] ep 09 lr=2.00e-04 tr_loss=0.6833 va_loss=1.8161 va_f1m=0.345 trade_auc=0.577 dir_auc=0.435 best=0.435@ep04\n",
      "[3c] ep 10 lr=2.00e-04 tr_loss=0.6190 va_loss=1.9159 va_f1m=0.345 trade_auc=0.587 dir_auc=0.457 best=0.435@ep04\n",
      "[3c] ep 11 lr=2.00e-04 tr_loss=0.5619 va_loss=2.1209 va_f1m=0.342 trade_auc=0.577 dir_auc=0.448 best=0.435@ep04\n",
      "[3c] ep 12 lr=1.00e-04 tr_loss=0.5251 va_loss=2.1096 va_f1m=0.367 trade_auc=0.610 dir_auc=0.460 best=0.435@ep04\n",
      "\n",
      "Chosen thresholds (from VAL):\n",
      "  thr_trade*=0.800 thr_dir*=0.750 | val trade_rate(pred)=0.089 | val pnl_sum=-0.0134\n",
      "TEST (fixed thr from VAL):\n",
      "  trade_rate(pred)=0.047 | pnl_sum=0.0492 | pnl_mean=0.000043 | trades=54\n",
      "3C TEST metrics: acc=0.478 f1m=0.408 trade_auc=0.543 dir_auc=0.589\n",
      "\n",
      "================================================================================\n",
      "CV summary (fold TEST, fixed thresholds from VAL):\n",
      "   fold  test_acc  test_f1m  test_trade_auc  test_dir_auc  \\\n",
      "0     1  0.411559  0.341988        0.587980      0.481965   \n",
      "1     2  0.456217  0.355145        0.577819      0.465446   \n",
      "2     3  0.492995  0.414137        0.610849      0.576634   \n",
      "3     4  0.527145  0.349502        0.632171      0.481842   \n",
      "4     5  0.479860  0.379611        0.604651      0.566448   \n",
      "5     6  0.478109  0.408442        0.542968      0.588521   \n",
      "\n",
      "   test_trade_rate_pred  test_pnl_sum  test_pnl_mean  thr_trade  thr_dir  \\\n",
      "0              0.881786     -0.157732      -0.000138   0.200000     0.50   \n",
      "1              0.439580     -0.105583      -0.000092   0.550000     0.50   \n",
      "2              0.293345      0.330599       0.000289   0.800000     0.75   \n",
      "3              0.077058     -0.023590      -0.000021   0.863986     0.50   \n",
      "4              0.322242      0.108479       0.000095   0.550000     0.75   \n",
      "5              0.047285      0.049177       0.000043   0.800000     0.75   \n",
      "\n",
      "   n_trades  \n",
      "0      1007  \n",
      "1       502  \n",
      "2       335  \n",
      "3        88  \n",
      "4       368  \n",
      "5        54  \n",
      "\n",
      "Means:\n",
      "fold                      3.500000\n",
      "test_acc                  0.474314\n",
      "test_f1m                  0.374804\n",
      "test_trade_auc            0.592740\n",
      "test_dir_auc              0.526809\n",
      "test_trade_rate_pred      0.343549\n",
      "test_pnl_sum              0.033558\n",
      "test_pnl_mean             0.000029\n",
      "thr_trade                 0.627331\n",
      "thr_dir                   0.625000\n",
      "n_trades                392.333333\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def run_walk_forward_cv_3c() -> pd.DataFrame:\n",
    "    rows = []\n",
    "\n",
    "    for fi, (idx_tr, idx_va, idx_te) in enumerate(walk_splits, 1):\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(f\"FOLD {fi}/{len(walk_splits)} sizes: train={len(idx_tr)} val={len(idx_va)} test={len(idx_te)}\")\n",
    "        print(f\"True trade ratio (val):  {split_trade_ratio_3c(idx_va, sample_t, y_tb):.3f}\")\n",
    "        print(f\"True trade ratio (test): {split_trade_ratio_3c(idx_te, sample_t, y_tb):.3f}\")\n",
    "        print(\"Train class dist:\", class_distribution_3c(idx_tr, sample_t, y_tb).tolist())\n",
    "        print(\"Val   class dist:\", class_distribution_3c(idx_va, sample_t, y_tb).tolist())\n",
    "        print(\"Test  class dist:\", class_distribution_3c(idx_te, sample_t, y_tb).tolist())\n",
    "\n",
    "        # scale per fold (fit only on train timeline)\n",
    "        X_scaled, _scaler = fit_scale_nodes_train_only(X_node_raw, sample_t, idx_tr, max_abs=CFG[\"max_abs_feat\"])\n",
    "\n",
    "        # Train single-head 3-class model\n",
    "        m_3c, r_3c = train_3c_classifier(\n",
    "            X_scaled, edge_feat, y_tb, exit_ret, sample_t,\n",
    "            idx_tr, idx_va, idx_te, CFG\n",
    "        )\n",
    "\n",
    "        # Choose thresholds on VAL\n",
    "        prob_val, er_val = predict_probs_on_indices_3c(m_3c, X_scaled, edge_feat, y_tb, exit_ret, idx_va, CFG)\n",
    "        sweep_val = sweep_thresholds_3c(prob_val, er_val, CFG, min_trades=int(CFG[\"eval_min_trades\"]))\n",
    "        best_val = sweep_val.iloc[0].to_dict()\n",
    "\n",
    "        thr_trade_star = float(best_val[\"thr_trade\"])\n",
    "        thr_dir_star = float(best_val[\"thr_dir\"])\n",
    "\n",
    "        val_metrics = pnl_by_threshold_3c(prob_val, er_val, thr_trade_star, thr_dir_star, CFG[\"cost_bps\"])\n",
    "        print(\"\\nChosen thresholds (from VAL):\")\n",
    "        print(f\"  thr_trade*={thr_trade_star:.3f} thr_dir*={thr_dir_star:.3f} | val trade_rate(pred)={val_metrics['trade_rate']:.3f} | val pnl_sum={val_metrics['pnl_sum']:.4f}\")\n",
    "\n",
    "        # Evaluate on TEST with fixed thresholds from VAL\n",
    "        prob_te, er_te = predict_probs_on_indices_3c(m_3c, X_scaled, edge_feat, y_tb, exit_ret, idx_te, CFG)\n",
    "        te_metrics = pnl_by_threshold_3c(prob_te, er_te, thr_trade_star, thr_dir_star, CFG[\"cost_bps\"])\n",
    "\n",
    "        print(\"TEST (fixed thr from VAL):\")\n",
    "        print(f\"  trade_rate(pred)={te_metrics['trade_rate']:.3f} | pnl_sum={te_metrics['pnl_sum']:.4f} | pnl_mean={te_metrics['pnl_mean']:.6f} | trades={te_metrics['n_trades']}\")\n",
    "        print(\"3C TEST metrics:\",\n",
    "              f\"acc={r_3c['test']['acc']:.3f}\",\n",
    "              f\"f1m={r_3c['test']['f1m']:.3f}\",\n",
    "              f\"trade_auc={r_3c['test']['trade_auc']:.3f}\",\n",
    "              f\"dir_auc={r_3c['test']['dir_auc']:.3f}\")\n",
    "\n",
    "        rows.append({\n",
    "            \"fold\": fi,\n",
    "            \"test_acc\": r_3c[\"test\"][\"acc\"],\n",
    "            \"test_f1m\": r_3c[\"test\"][\"f1m\"],\n",
    "            \"test_trade_auc\": r_3c[\"test\"][\"trade_auc\"],\n",
    "            \"test_dir_auc\": r_3c[\"test\"][\"dir_auc\"],\n",
    "            \"test_trade_rate_pred\": te_metrics[\"trade_rate\"],\n",
    "            \"test_pnl_sum\": te_metrics[\"pnl_sum\"],\n",
    "            \"test_pnl_mean\": te_metrics[\"pnl_mean\"],\n",
    "            \"thr_trade\": thr_trade_star,\n",
    "            \"thr_dir\": thr_dir_star,\n",
    "            \"n_trades\": te_metrics[\"n_trades\"],\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "cv_summary_3c = run_walk_forward_cv_3c()\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CV summary (fold TEST, fixed thresholds from VAL):\")\n",
    "print(cv_summary_3c)\n",
    "print(\"\\nMeans:\")\n",
    "print(cv_summary_3c.mean(numeric_only=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11 — Final train on CV(90%) and evaluate on FINAL holdout (10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FINAL TRAIN/VAL on CV-part (90%) -> EVAL on FINAL holdout (10%)\n",
      "Final split sizes:\n",
      "  train_final: 10284\n",
      "  val_final  : 1142\n",
      "  holdout    : 1270\n",
      "True trade ratio (val_final):  0.426\n",
      "True trade ratio (holdout):    0.439\n",
      "Train class dist: [1898, 6696, 1690]\n",
      "Val   class dist: [246, 656, 240]\n",
      "Hold  class dist: [333, 713, 224]\n",
      "[3c] ep 01 lr=4.00e-04 tr_loss=1.1775 va_loss=1.1029 va_f1m=0.322 trade_auc=0.534 dir_auc=0.469 best=0.375@ep01\n",
      "[3c] ep 02 lr=4.00e-04 tr_loss=1.1008 va_loss=1.1116 va_f1m=0.299 trade_auc=0.536 dir_auc=0.493 best=0.375@ep01\n",
      "[3c] ep 03 lr=4.00e-04 tr_loss=1.0836 va_loss=1.1242 va_f1m=0.338 trade_auc=0.527 dir_auc=0.517 best=0.390@ep03\n",
      "[3c] ep 04 lr=4.00e-04 tr_loss=1.0616 va_loss=1.1279 va_f1m=0.365 trade_auc=0.518 dir_auc=0.563 best=0.417@ep04\n",
      "[3c] ep 05 lr=4.00e-04 tr_loss=1.0298 va_loss=1.1397 va_f1m=0.329 trade_auc=0.512 dir_auc=0.581 best=0.417@ep04\n",
      "[3c] ep 06 lr=4.00e-04 tr_loss=0.9875 va_loss=1.2294 va_f1m=0.373 trade_auc=0.526 dir_auc=0.589 best=0.426@ep06\n",
      "[3c] ep 07 lr=4.00e-04 tr_loss=0.9322 va_loss=1.2287 va_f1m=0.361 trade_auc=0.542 dir_auc=0.605 best=0.426@ep06\n",
      "[3c] ep 08 lr=4.00e-04 tr_loss=0.8638 va_loss=1.2631 va_f1m=0.392 trade_auc=0.540 dir_auc=0.645 best=0.446@ep08\n",
      "[3c] ep 09 lr=4.00e-04 tr_loss=0.7852 va_loss=1.3761 va_f1m=0.373 trade_auc=0.535 dir_auc=0.632 best=0.446@ep08\n",
      "[3c] ep 10 lr=4.00e-04 tr_loss=0.7027 va_loss=1.5564 va_f1m=0.385 trade_auc=0.545 dir_auc=0.635 best=0.446@ep08\n",
      "[3c] ep 11 lr=4.00e-04 tr_loss=0.6329 va_loss=1.5159 va_f1m=0.379 trade_auc=0.572 dir_auc=0.642 best=0.446@ep08\n",
      "[3c] ep 12 lr=4.00e-04 tr_loss=0.5754 va_loss=1.7266 va_f1m=0.398 trade_auc=0.582 dir_auc=0.606 best=0.456@ep12\n",
      "[3c] ep 13 lr=4.00e-04 tr_loss=0.4943 va_loss=1.8205 va_f1m=0.414 trade_auc=0.581 dir_auc=0.615 best=0.472@ep13\n",
      "[3c] ep 14 lr=4.00e-04 tr_loss=0.4344 va_loss=2.1096 va_f1m=0.387 trade_auc=0.556 dir_auc=0.597 best=0.472@ep13\n",
      "[3c] ep 15 lr=4.00e-04 tr_loss=0.3919 va_loss=2.4204 va_f1m=0.386 trade_auc=0.548 dir_auc=0.588 best=0.472@ep13\n",
      "[3c] ep 16 lr=4.00e-04 tr_loss=0.3541 va_loss=2.3634 va_f1m=0.389 trade_auc=0.548 dir_auc=0.615 best=0.472@ep13\n",
      "[3c] ep 17 lr=2.00e-04 tr_loss=0.3145 va_loss=2.6369 va_f1m=0.394 trade_auc=0.561 dir_auc=0.621 best=0.472@ep13\n",
      "[3c] ep 18 lr=2.00e-04 tr_loss=0.2496 va_loss=2.7893 va_f1m=0.420 trade_auc=0.575 dir_auc=0.634 best=0.477@ep18\n",
      "[3c] ep 19 lr=2.00e-04 tr_loss=0.2161 va_loss=2.9683 va_f1m=0.400 trade_auc=0.579 dir_auc=0.580 best=0.477@ep18\n",
      "[3c] ep 20 lr=2.00e-04 tr_loss=0.2006 va_loss=3.2172 va_f1m=0.408 trade_auc=0.583 dir_auc=0.593 best=0.477@ep18\n",
      "[3c] ep 21 lr=2.00e-04 tr_loss=0.1886 va_loss=3.1752 va_f1m=0.410 trade_auc=0.571 dir_auc=0.603 best=0.477@ep18\n",
      "[3c] ep 22 lr=1.00e-04 tr_loss=0.1785 va_loss=3.4453 va_f1m=0.399 trade_auc=0.567 dir_auc=0.605 best=0.477@ep18\n",
      "[3c] ep 23 lr=1.00e-04 tr_loss=0.1459 va_loss=3.8030 va_f1m=0.399 trade_auc=0.574 dir_auc=0.602 best=0.477@ep18\n",
      "[3c] ep 24 lr=1.00e-04 tr_loss=0.1416 va_loss=3.6534 va_f1m=0.406 trade_auc=0.575 dir_auc=0.610 best=0.477@ep18\n",
      "[3c] ep 25 lr=1.00e-04 tr_loss=0.1205 va_loss=3.9130 va_f1m=0.407 trade_auc=0.574 dir_auc=0.605 best=0.477@ep18\n",
      "[3c] ep 26 lr=5.00e-05 tr_loss=0.1228 va_loss=3.8106 va_f1m=0.403 trade_auc=0.566 dir_auc=0.615 best=0.477@ep18\n",
      "\n",
      "Chosen thresholds on val_final:\n",
      "  thr_trade*=0.200\n",
      "  thr_dir*  =0.850\n",
      "  val trade_rate(pred)=0.475 | val pnl_sum=0.4625 | val pnl_mean=0.000405 | trades=543\n",
      "\n",
      "FINAL HOLDOUT RESULT (fixed thresholds from val_final):\n",
      "  trade_rate(pred)=0.588\n",
      "  pnl_sum=-0.2949 | pnl_mean=-0.000232 | trades=747\n",
      "  sharpe(per-bar proxy)=-1.156\n",
      "\n",
      "[ORACLE] best possible on holdout by sweeping thresholds (DO NOT USE for selection):\n",
      "  thr_trade=0.850 thr_dir=0.650\n",
      "  trade_rate(pred)=0.289 | pnl_sum=-0.1466 | trades=367\n",
      "\n",
      "3C metrics summary:\n",
      "  VAL:    acc=0.501 f1m=0.420 trade_auc=0.575 dir_auc=0.634\n",
      "  HOLD:   acc=0.386 f1m=0.319 trade_auc=0.525 dir_auc=0.472\n",
      "\n",
      "Confusion matrix on HOLD (rows=true, cols=pred) for [down, flat, up]:\n",
      "[[ 71 163  99]\n",
      " [172 366 175]\n",
      " [ 60 111  53]]\n"
     ]
    }
   ],
   "source": [
    "def run_final_train_holdout_3c() -> None:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"FINAL TRAIN/VAL on CV-part (90%) -> EVAL on FINAL holdout (10%)\")\n",
    "\n",
    "    val_w = max(1, int(CFG[\"val_window_frac\"] * n_samples_cv))\n",
    "    train_end = n_samples_cv - val_w\n",
    "\n",
    "    idx_train_final = np.arange(0, train_end, dtype=np.int64)\n",
    "    idx_val_final = np.arange(train_end, n_samples_cv, dtype=np.int64)\n",
    "    idx_holdout = idx_final_test.astype(np.int64)\n",
    "\n",
    "    print(\"Final split sizes:\")\n",
    "    print(\"  train_final:\", len(idx_train_final))\n",
    "    print(\"  val_final  :\", len(idx_val_final))\n",
    "    print(\"  holdout    :\", len(idx_holdout))\n",
    "    print(f\"True trade ratio (val_final):  {split_trade_ratio_3c(idx_val_final, sample_t, y_tb):.3f}\")\n",
    "    print(f\"True trade ratio (holdout):    {split_trade_ratio_3c(idx_holdout, sample_t, y_tb):.3f}\")\n",
    "    print(\"Train class dist:\", class_distribution_3c(idx_train_final, sample_t, y_tb).tolist())\n",
    "    print(\"Val   class dist:\", class_distribution_3c(idx_val_final, sample_t, y_tb).tolist())\n",
    "    print(\"Hold  class dist:\", class_distribution_3c(idx_holdout, sample_t, y_tb).tolist())\n",
    "\n",
    "    # scaling on train_final timeline only\n",
    "    X_scaled_final, _ = fit_scale_nodes_train_only(X_node_raw, sample_t, idx_train_final, max_abs=CFG[\"max_abs_feat\"])\n",
    "\n",
    "    # Train 3-class model\n",
    "    m_3c, r_3c = train_3c_classifier(\n",
    "        X_scaled_final, edge_feat, y_tb, exit_ret, sample_t,\n",
    "        idx_train_final, idx_val_final, idx_holdout, CFG\n",
    "    )\n",
    "\n",
    "    # choose thresholds on val_final\n",
    "    prob_val, er_val = predict_probs_on_indices_3c(m_3c, X_scaled_final, edge_feat, y_tb, exit_ret, idx_val_final, CFG)\n",
    "    sweep_val = sweep_thresholds_3c(prob_val, er_val, CFG, min_trades=int(CFG[\"eval_min_trades\"]))\n",
    "    best_val = sweep_val.iloc[0].to_dict()\n",
    "    thr_trade_star = float(best_val[\"thr_trade\"])\n",
    "    thr_dir_star = float(best_val[\"thr_dir\"])\n",
    "\n",
    "    val_metrics = pnl_by_threshold_3c(prob_val, er_val, thr_trade_star, thr_dir_star, CFG[\"cost_bps\"])\n",
    "    print(\"\\nChosen thresholds on val_final:\")\n",
    "    print(f\"  thr_trade*={thr_trade_star:.3f}\")\n",
    "    print(f\"  thr_dir*  ={thr_dir_star:.3f}\")\n",
    "    print(f\"  val trade_rate(pred)={val_metrics['trade_rate']:.3f} | val pnl_sum={val_metrics['pnl_sum']:.4f} | val pnl_mean={val_metrics['pnl_mean']:.6f} | trades={val_metrics['n_trades']}\")\n",
    "\n",
    "    # evaluate holdout with fixed thresholds from val_final\n",
    "    prob_hold, er_hold = predict_probs_on_indices_3c(m_3c, X_scaled_final, edge_feat, y_tb, exit_ret, idx_holdout, CFG)\n",
    "    hold_metrics = pnl_by_threshold_3c(prob_hold, er_hold, thr_trade_star, thr_dir_star, CFG[\"cost_bps\"])\n",
    "\n",
    "    print(\"\\nFINAL HOLDOUT RESULT (fixed thresholds from val_final):\")\n",
    "    print(f\"  trade_rate(pred)={hold_metrics['trade_rate']:.3f}\")\n",
    "    print(f\"  pnl_sum={hold_metrics['pnl_sum']:.4f} | pnl_mean={hold_metrics['pnl_mean']:.6f} | trades={hold_metrics['n_trades']}\")\n",
    "    print(f\"  sharpe(per-bar proxy)={hold_metrics['pnl_sharpe']:.3f}\")\n",
    "\n",
    "    # oracle (DO NOT USE for selection)\n",
    "    sweep_hold_oracle = sweep_thresholds_3c(prob_hold, er_hold, CFG, min_trades=int(CFG[\"eval_min_trades\"]))\n",
    "    best_hold_oracle = sweep_hold_oracle.iloc[0].to_dict()\n",
    "    print(\"\\n[ORACLE] best possible on holdout by sweeping thresholds (DO NOT USE for selection):\")\n",
    "    print(f\"  thr_trade={best_hold_oracle['thr_trade']:.3f} thr_dir={best_hold_oracle['thr_dir']:.3f}\")\n",
    "    print(f\"  trade_rate(pred)={best_hold_oracle['trade_rate']:.3f} | pnl_sum={best_hold_oracle['pnl_sum']:.4f} | trades={int(best_hold_oracle['n_trades'])}\")\n",
    "\n",
    "    # quick metrics summary\n",
    "    print(\"\\n3C metrics summary:\")\n",
    "    print(f\"  VAL:    acc={r_3c['val']['acc']:.3f} f1m={r_3c['val']['f1m']:.3f} trade_auc={r_3c['val']['trade_auc']:.3f} dir_auc={r_3c['val']['dir_auc']:.3f}\")\n",
    "    print(f\"  HOLD:   acc={r_3c['test']['acc']:.3f} f1m={r_3c['test']['f1m']:.3f} trade_auc={r_3c['test']['trade_auc']:.3f} dir_auc={r_3c['test']['dir_auc']:.3f}\")\n",
    "    print(\"\\nConfusion matrix on HOLD (rows=true, cols=pred) for [down, flat, up]:\")\n",
    "    print(r_3c[\"test\"][\"cm\"])\n",
    "\n",
    "run_final_train_holdout_3c()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes\n",
    "# - This is a true single-head model: one forward pass produces probabilities for {down, flat, up}\n",
    "# - Trading decision is derived from probabilities:\n",
    "#   * trade_conf = 1 - p_flat\n",
    "#   * direction_conf = max(p_up,p_down)/(p_up+p_down)\n",
    "#   * action = sign(p_up - p_down)\n",
    "# - Thresholds are selected ONLY on the validation split, then frozen for test/holdout\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recbole_new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
