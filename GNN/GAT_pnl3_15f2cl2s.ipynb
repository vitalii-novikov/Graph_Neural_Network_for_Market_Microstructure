{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45a317d0",
   "metadata": {},
   "source": [
    "## Step 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3e40529c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cpu\n",
      "EDGE_INDEX: [[0, 1], [0, 2], [2, 1]]\n",
      "TEMPORAL: xformer_cls | heads= 4 | d_model= 64\n"
     ]
    }
   ],
   "source": [
    "# ЛОГИЧЕСКИЙ БЛОК: imports + reproducibility + GLOBAL config\n",
    "# ИСПОЛНЕНИЕ БЛОКА:\n",
    "\n",
    "import os, math, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, roc_auc_score\n",
    "\n",
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(100)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "\n",
    "# -------------------------------\n",
    "# GLOBAL CONFIG (всё тут)\n",
    "# -------------------------------\n",
    "CFG = {\n",
    "    # data\n",
    "    \"freq\": \"5min\",\n",
    "    \"data_dir\": Path(\"../dataset\"),\n",
    "    # NEW: holdout final test split (по времени, на sample-space)\n",
    "    \"final_test_frac\": 0.10,\n",
    "\n",
    "    \"book_levels\": 15,         # сколько уровней стакана грузим\n",
    "    \"top_levels\": 5,           # DI_L0..DI_L4\n",
    "    \"near_levels\": 5,          # near=0..4, far=5..14\n",
    "\n",
    "    # walk-forward windows (в sample-space)\n",
    "    \"train_min_frac\": 0.50,\n",
    "    \"val_window_frac\": 0.10,\n",
    "    \"test_window_frac\": 0.10,\n",
    "    \"step_window_frac\": 0.10,\n",
    "\n",
    "    # scaling\n",
    "    \"max_abs_feat\": 10.0,\n",
    "\n",
    "    # correlations\n",
    "    \"corr_windows\": [6, 12, 24, 48, 84],  # 30m,1h,2h,4h,7h\n",
    "    \"edges\": [(\"ADA\",\"BTC\"), (\"ADA\",\"ETH\"), (\"ETH\",\"BTC\")],\n",
    "\n",
    "    # triple-barrier (labels)\n",
    "    \"tb_horizon\": 1*12,       # 1h     # нужен для sample_t (чтобы TB-exit не вылезал за конец)\n",
    "    \"lookback\": 7*12,\n",
    "    \"tb_pt_mult\": 1.2,\n",
    "    \"tb_sl_mult\": 1.1,\n",
    "    \"tb_min_barrier\": 0.001,\n",
    "    \"tb_max_barrier\": 0.006,\n",
    "\n",
    "    # training (общие)\n",
    "    \"batch_size\": 64,\n",
    "    \"epochs\": 30,\n",
    "    \"lr\": 2e-4,\n",
    "    \"weight_decay\": 1e-3,\n",
    "    \"grad_clip\": 1.0,\n",
    "    \"dropout\": 0.2,\n",
    "    \"hidden\": 64,\n",
    "    \"gnn_layers\": 2,\n",
    "\n",
    "    # --- old Attn params kept for compatibility: now used as ATTENTION dims ---\n",
    "    # Attn_hidden -> d_model (размер темпорального представления)\n",
    "    # Attn_layers -> n_layers (кол-во attention/transformer layers, если режим их использует)\n",
    "    \"Attn_hidden\": 64,\n",
    "    \"Attn_layers\": 1,\n",
    "\n",
    "    \"use_amp\": True,\n",
    "\n",
    "    # trading eval\n",
    "    \"cost_bps\": 2.0,\n",
    "\n",
    "    # confidence thresholds (для PnL по порогу)\n",
    "    \"thr_trade_grid\": [0.50, 0.55, 0.60, 0.65, 0.70],\n",
    "    \"thr_dir_grid\":   [0.50, 0.55, 0.60, 0.65, 0.70],\n",
    "\n",
    "    # ---- PnL proxy during DIR training (grid selector)\n",
    "    \"proxy_thr_trade_grid\": None,  # None -> использовать thr_trade_grid\n",
    "    \"proxy_thr_dir_grid\":   None,  # None -> использовать thr_dir_grid\n",
    "    \"proxy_min_trades\": 30,\n",
    "\n",
    "    # -------------------------------\n",
    "    # NEW: attention-based temporal encoder config\n",
    "    # -------------------------------\n",
    "    # ДВА режима (по твоему требованию):\n",
    "    # 1) \"xformer_cls\"  -> TransformerEncoder + [CLS] token, берём CLS как summary\n",
    "    # 2) \"attn_pool\"    -> learnable query + MultiHeadAttention pooling (быстрый и стабильный)\n",
    "    \"temporal_mode\": \"xformer_cls\",   # \"xformer_cls\" | \"attn_pool\"\n",
    "    \"attn_heads\": 4,                  # будет автоматически приведено к делителю d_model\n",
    "    \"attn_ff_mult\": 4,                # FFN dim = ff_mult * d_model (для xformer_cls)\n",
    "    \"attn_dropout\": None,             # None -> использовать CFG[\"dropout\"]\n",
    "    \"attn_causal\": False,             # опционально для xformer (обычно False, т.к. окно past-only)\n",
    "    \"attn_use_pos_emb\": True,         # learned positional embeddings\n",
    "}\n",
    "\n",
    "ASSETS = [\"ADA\", \"BTC\", \"ETH\"]\n",
    "ASSET2IDX = {a:i for i,a in enumerate(ASSETS)}\n",
    "TARGET_ASSET = \"ETH\"\n",
    "TARGET_NODE = ASSET2IDX[TARGET_ASSET]\n",
    "\n",
    "EDGES = CFG[\"edges\"]\n",
    "EDGE_INDEX = torch.tensor([[ASSET2IDX[s], ASSET2IDX[t]] for (s,t) in EDGES], dtype=torch.long)  # [E,2]\n",
    "print(\"EDGE_INDEX:\", EDGE_INDEX.tolist())\n",
    "\n",
    "print(\"TEMPORAL:\", CFG[\"temporal_mode\"], \"| heads=\", CFG[\"attn_heads\"], \"| d_model=\", CFG[\"Attn_hidden\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7abcf5",
   "metadata": {},
   "source": [
    "## 1. load data + basic returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5c84cb42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded df: (2693, 106)\n",
      "Example columns: ['timestamp', 'ADA', 'spread_ADA', 'buys_ADA', 'sells_ADA', 'bids_vol_ADA_0', 'bids_vol_ADA_1', 'bids_vol_ADA_2', 'bids_vol_ADA_3', 'bids_vol_ADA_4', 'bids_vol_ADA_5', 'bids_vol_ADA_6', 'bids_vol_ADA_7', 'bids_vol_ADA_8', 'bids_vol_ADA_9', 'bids_vol_ADA_10', 'bids_vol_ADA_11', 'bids_vol_ADA_12', 'bids_vol_ADA_13', 'bids_vol_ADA_14', 'asks_vol_ADA_0', 'asks_vol_ADA_1', 'asks_vol_ADA_2', 'asks_vol_ADA_3', 'asks_vol_ADA_4']\n"
     ]
    }
   ],
   "source": [
    "# ЛОГИЧЕСКИЙ БЛОК: load data + log returns (без target) + все уровни стакана\n",
    "# ИСПОЛНЕНИЕ БЛОКА:\n",
    "\n",
    "def load_asset(asset: str, freq: str, data_dir: Path, book_levels: int, part = [0,100]) -> pd.DataFrame:\n",
    "    path = data_dir / f\"{asset}_{freq}.csv\"\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.iloc[int(len(df)*part[0]/100) : int(len(df)*part[1]/100)]\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"system_time\"]).dt.round(\"min\")\n",
    "    df = df.sort_values(\"timestamp\").set_index(\"timestamp\")\n",
    "\n",
    "    bid_cols = [f\"bids_notional_{i}\" for i in range(book_levels)]\n",
    "    ask_cols = [f\"asks_notional_{i}\" for i in range(book_levels)]\n",
    "\n",
    "    needed = [\"midpoint\", \"spread\", \"buys\", \"sells\"] + bid_cols + ask_cols\n",
    "    missing = [c for c in needed if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"{asset}: missing columns in CSV: {missing[:10]}{'...' if len(missing) > 10 else ''}\")\n",
    "\n",
    "    return df[needed]\n",
    "\n",
    "\n",
    "def load_all_assets() -> pd.DataFrame:\n",
    "    freq = CFG[\"freq\"]\n",
    "    data_dir = CFG[\"data_dir\"]\n",
    "    book_levels = CFG[\"book_levels\"]\n",
    "\n",
    "    def rename_asset_cols(df_one: pd.DataFrame, asset: str) -> pd.DataFrame:\n",
    "        rename_map = {\n",
    "            \"midpoint\": asset,\n",
    "            \"buys\": f\"buys_{asset}\",\n",
    "            \"sells\": f\"sells_{asset}\",\n",
    "            \"spread\": f\"spread_{asset}\",\n",
    "        }\n",
    "        for i in range(book_levels):\n",
    "            rename_map[f\"bids_notional_{i}\"] = f\"bids_vol_{asset}_{i}\"\n",
    "            rename_map[f\"asks_notional_{i}\"] = f\"asks_vol_{asset}_{i}\"\n",
    "        return df_one.rename(columns=rename_map)\n",
    "\n",
    "    df_ADA = rename_asset_cols(load_asset(\"ADA\", freq, data_dir, book_levels, part = [0, 80]), \"ADA\")\n",
    "    df_BTC = rename_asset_cols(load_asset(\"BTC\", freq, data_dir, book_levels, part = [0, 80]), \"BTC\")\n",
    "    df_ETH = rename_asset_cols(load_asset(\"ETH\", freq, data_dir, book_levels, part = [0, 80]), \"ETH\")\n",
    "\n",
    "    df = df_ADA.join(df_BTC).join(df_ETH)\n",
    "    df = df.reset_index()  # timestamp column remains\n",
    "    return df\n",
    "\n",
    "\n",
    "df = load_all_assets()\n",
    "T = len(df)\n",
    "\n",
    "# log returns\n",
    "for a in ASSETS:\n",
    "    df[f\"lr_{a}\"] = np.log(df[a]).diff().fillna(0.0)\n",
    "\n",
    "print(\"Loaded df:\", df.shape)\n",
    "print(\"Example columns:\", df.columns[:25].tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f68a9e",
   "metadata": {},
   "source": [
    "## 2. multi-window correlations → edge features (T,E,W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f8ad360a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corr_array shape: (2693, 3, 5)\n"
     ]
    }
   ],
   "source": [
    "# ЛОГИЧЕСКИЙ БЛОК: multi-window correlations -> corr_array (T,E,W)\n",
    "# ИСПОЛНЕНИЕ БЛОКА:\n",
    "\n",
    "candidate_windows = CFG[\"corr_windows\"]\n",
    "edges = EDGES\n",
    "\n",
    "n_w = len(candidate_windows)\n",
    "n_edges = len(edges)\n",
    "T = len(df)\n",
    "\n",
    "corr_array = np.zeros((T, n_edges, n_w), dtype=np.float32)\n",
    "\n",
    "for wi, w in enumerate(candidate_windows):\n",
    "    r_ADA_BTC = df[\"lr_ADA\"].rolling(w, min_periods=1).corr(df[\"lr_BTC\"])\n",
    "    r_ADA_ETH = df[\"lr_ADA\"].rolling(w, min_periods=1).corr(df[\"lr_ETH\"])\n",
    "    r_ETH_BTC = df[\"lr_ETH\"].rolling(w, min_periods=1).corr(df[\"lr_BTC\"])\n",
    "\n",
    "    corr_array[:, 0, wi] = np.nan_to_num(r_ADA_BTC)\n",
    "    corr_array[:, 1, wi] = np.nan_to_num(r_ADA_ETH)\n",
    "    corr_array[:, 2, wi] = np.nan_to_num(r_ETH_BTC)\n",
    "\n",
    "print(\"corr_array shape:\", corr_array.shape)  # (T,E,W)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d923c7ce",
   "metadata": {},
   "source": [
    "## 3. triple-barrier → y_tb + exit_ret → two-stage labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2effb1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TB dist [down,flat,up]: [ 655 1311  727]\n",
      "Trade ratio: 0.5131823245451169\n"
     ]
    }
   ],
   "source": [
    "# ЛОГИЧЕСКИЙ БЛОК: triple-barrier labels -> y_tb + exit_ret + two-stage labels\n",
    "# ИСПОЛНЕНИЕ БЛОКА:\n",
    "\n",
    "def triple_barrier_labels_from_lr(\n",
    "    lr: pd.Series,\n",
    "    horizon: int,\n",
    "    vol_window: int,\n",
    "    pt_mult: float,\n",
    "    sl_mult: float,\n",
    "    min_barrier: float,\n",
    "    max_barrier: float,\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      y_tb: {0=down, 1=flat/no-trade, 2=up}\n",
    "      exit_ret: realized log-return to exit (tp/sl/timeout)\n",
    "      exit_t: exit index\n",
    "      thr: barrier per t\n",
    "    No leakage: vol is shift(1).\n",
    "    \"\"\"\n",
    "    lr = lr.astype(float).copy()\n",
    "    T = len(lr)\n",
    "\n",
    "    vol = lr.rolling(vol_window, min_periods=max(10, vol_window//10)).std().shift(1)\n",
    "    thr = (vol * np.sqrt(horizon)).clip(lower=min_barrier, upper=max_barrier)\n",
    "\n",
    "    y = np.ones(T, dtype=np.int64)\n",
    "    exit_ret = np.zeros(T, dtype=np.float32)\n",
    "    exit_t = np.arange(T, dtype=np.int64)\n",
    "\n",
    "    lr_np = lr.fillna(0.0).to_numpy(dtype=np.float64)\n",
    "    thr_np = thr.fillna(min_barrier).to_numpy(dtype=np.float64)\n",
    "\n",
    "    for t in range(T - horizon - 1):\n",
    "        up = pt_mult * thr_np[t]\n",
    "        dn = -sl_mult * thr_np[t]\n",
    "\n",
    "        cum = 0.0\n",
    "        hit = 1\n",
    "        et = t + horizon\n",
    "        er = 0.0\n",
    "\n",
    "        for dt in range(1, horizon + 1):\n",
    "            cum += lr_np[t + dt]\n",
    "            if cum >= up:\n",
    "                hit = 2\n",
    "                et = t + dt\n",
    "                er = cum\n",
    "                break\n",
    "            if cum <= dn:\n",
    "                hit = 0\n",
    "                et = t + dt\n",
    "                er = cum\n",
    "                break\n",
    "\n",
    "        if hit == 1:\n",
    "            er = float(np.sum(lr_np[t+1:t+horizon+1]))\n",
    "            et = t + horizon\n",
    "\n",
    "        y[t] = hit\n",
    "        exit_ret[t] = er\n",
    "        exit_t[t] = et\n",
    "\n",
    "    return y, exit_ret, exit_t, thr_np\n",
    "\n",
    "# --- build TB on ETH ---\n",
    "y_tb, exit_ret, exit_t, thr = triple_barrier_labels_from_lr(\n",
    "    df[\"lr_ETH\"],\n",
    "    horizon=1*12, \n",
    "    vol_window=7*12,\n",
    "    pt_mult=1.2,\n",
    "    sl_mult=1.1,\n",
    "    min_barrier=0.001,\n",
    "    max_barrier=0.006,\n",
    ")\n",
    "\n",
    "# two-stage labels\n",
    "y_trade = (y_tb != 1).astype(np.int64)      # 1=trade, 0=no-trade\n",
    "y_dir   = (y_tb == 2).astype(np.int64)      # 1=up, 0=down (для trade-сэмплов)\n",
    "\n",
    "print(\"TB dist [down,flat,up]:\", np.bincount(y_tb, minlength=3))\n",
    "print(\"Trade ratio:\", y_trade.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8413df19",
   "metadata": {},
   "source": [
    "## 4. build node tensor + edge tensor + sample_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0565bbdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_node_raw: (2693, 3, 15) edge_feat: (2693, 3, 5)\n",
      "node_feat_names: ['lr', 'spread', 'log_buys', 'log_sells', 'ofi', 'DI_15', 'DI_L0', 'DI_L1', 'DI_L2', 'DI_L3', 'DI_L4', 'near_ratio_bid', 'near_ratio_ask', 'di_near', 'di_far']\n",
      "n_samples: 2597 t range: 83 2679\n"
     ]
    }
   ],
   "source": [
    "# ЛОГИЧЕСКИЙ БЛОК: build node features (T,N,F) + edge features (T,E,W) + sample_t\n",
    "# ИСПОЛНЕНИЕ БЛОКА:\n",
    "\n",
    "EPS = 1e-6\n",
    "\n",
    "def safe_log1p(x: np.ndarray) -> np.ndarray:\n",
    "    return np.log1p(np.maximum(x, 0.0))\n",
    "\n",
    "def build_node_tensor(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Фичи на asset:\n",
    "      lr, spread,\n",
    "      log_buys, log_sells, ofi,\n",
    "      DI_15,\n",
    "      DI_L0..DI_L4,\n",
    "      near_ratio_bid, near_ratio_ask,\n",
    "      di_near, di_far\n",
    "    \"\"\"\n",
    "    feats = []\n",
    "    feat_names = [\n",
    "        \"lr\", \"spread\",\n",
    "        \"log_buys\", \"log_sells\", \"ofi\",\n",
    "        \"DI_15\",\n",
    "        \"DI_L0\", \"DI_L1\", \"DI_L2\", \"DI_L3\", \"DI_L4\",\n",
    "        \"near_ratio_bid\", \"near_ratio_ask\",\n",
    "        \"di_near\", \"di_far\",\n",
    "    ]\n",
    "\n",
    "    book_levels = CFG[\"book_levels\"]\n",
    "    top_k = CFG[\"top_levels\"]     # 5\n",
    "    near_k = CFG[\"near_levels\"]   # 5\n",
    "    far_k = book_levels - near_k\n",
    "    if far_k <= 0:\n",
    "        raise ValueError(\"CFG['near_levels'] must be < CFG['book_levels']\")\n",
    "\n",
    "    for a in ASSETS:\n",
    "        lr = df[f\"lr_{a}\"].values.astype(np.float32)\n",
    "        spread = df[f\"spread_{a}\"].values.astype(np.float32)\n",
    "\n",
    "        buys = df[f\"buys_{a}\"].values.astype(np.float32)\n",
    "        sells = df[f\"sells_{a}\"].values.astype(np.float32)\n",
    "\n",
    "        log_buys = safe_log1p(buys).astype(np.float32)\n",
    "        log_sells = safe_log1p(sells).astype(np.float32)\n",
    "\n",
    "        ofi = ((buys - sells) / (buys + sells + EPS)).astype(np.float32)\n",
    "\n",
    "        # уровни стакана\n",
    "        bids_lvls = np.stack([df[f\"bids_vol_{a}_{i}\"].values.astype(np.float32) for i in range(book_levels)], axis=1)  # (T,15)\n",
    "        asks_lvls = np.stack([df[f\"asks_vol_{a}_{i}\"].values.astype(np.float32) for i in range(book_levels)], axis=1)  # (T,15)\n",
    "\n",
    "        bid_sum_15 = bids_lvls.sum(axis=1)\n",
    "        ask_sum_15 = asks_lvls.sum(axis=1)\n",
    "        DI_15 = ((bid_sum_15 - ask_sum_15) / (bid_sum_15 + ask_sum_15 + EPS)).astype(np.float32)\n",
    "\n",
    "        # DI_L0..DI_L4\n",
    "        di_levels = []\n",
    "        for i in range(top_k):\n",
    "            b = bids_lvls[:, i]\n",
    "            s = asks_lvls[:, i]\n",
    "            di_levels.append(((b - s) / (b + s + EPS)).astype(np.float32))\n",
    "        DI_L0_4 = np.stack(di_levels, axis=1)  # (T,5)\n",
    "\n",
    "        # near vs far\n",
    "        bid_near = bids_lvls[:, :near_k].sum(axis=1)\n",
    "        ask_near = asks_lvls[:, :near_k].sum(axis=1)\n",
    "        bid_far = bids_lvls[:, near_k:].sum(axis=1)\n",
    "        ask_far = asks_lvls[:, near_k:].sum(axis=1)\n",
    "\n",
    "        near_ratio_bid = (bid_near / (bid_far + EPS)).astype(np.float32)\n",
    "        near_ratio_ask = (ask_near / (ask_far + EPS)).astype(np.float32)\n",
    "\n",
    "        di_near = ((bid_near - ask_near) / (bid_near + ask_near + EPS)).astype(np.float32)\n",
    "        di_far = ((bid_far - ask_far) / (bid_far + ask_far + EPS)).astype(np.float32)\n",
    "\n",
    "        Xa = np.column_stack([\n",
    "            lr, spread,\n",
    "            log_buys, log_sells, ofi,\n",
    "            DI_15,\n",
    "            DI_L0_4[:, 0], DI_L0_4[:, 1], DI_L0_4[:, 2], DI_L0_4[:, 3], DI_L0_4[:, 4],\n",
    "            near_ratio_bid, near_ratio_ask,\n",
    "            di_near, di_far\n",
    "        ]).astype(np.float32)\n",
    "\n",
    "        feats.append(Xa)\n",
    "\n",
    "    X = np.stack(feats, axis=1).astype(np.float32)  # (T,N,F)\n",
    "    return X, feat_names\n",
    "\n",
    "\n",
    "X_node_raw, node_feat_names = build_node_tensor(df)\n",
    "edge_feat = np.nan_to_num(corr_array.astype(np.float32), nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "T = len(df)\n",
    "L = CFG[\"lookback\"]\n",
    "H = CFG[\"tb_horizon\"]\n",
    "\n",
    "# sample_t: чтобы можно было брать окно [t-L+1 ... t] и иметь будущий TB-exit без выхода за данные\n",
    "t_min = L - 1\n",
    "t_max = T - H - 2\n",
    "sample_t = np.arange(t_min, t_max + 1)\n",
    "n_samples = len(sample_t)\n",
    "\n",
    "print(\"X_node_raw:\", X_node_raw.shape, \"edge_feat:\", edge_feat.shape)\n",
    "print(\"node_feat_names:\", node_feat_names)\n",
    "print(\"n_samples:\", n_samples, \"t range:\", sample_t[0], sample_t[-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00eb5de4",
   "metadata": {},
   "source": [
    "## Train (folds) - Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d9bad799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holdout split:\n",
      "  n_samples total: 2597\n",
      "  n_samples CV   : 2337 (90.0%)\n",
      "  n_samples FINAL: 260 (10.0%)\n",
      "  CV range   : 0 2336\n",
      "  FINAL range: 2337 2596\n"
     ]
    }
   ],
   "source": [
    "# ЛОГИЧЕСКИЙ БЛОК: final holdout split (90% CV + 10% final test), time-ordered\n",
    "# ИСПОЛНЕНИЕ БЛОКА:\n",
    "\n",
    "def make_final_holdout_split(n_samples: int, final_test_frac: float):\n",
    "    if not (0.0 < final_test_frac < 0.5):\n",
    "        raise ValueError(\"final_test_frac should be in (0, 0.5)\")\n",
    "\n",
    "    n_final = max(1, int(round(final_test_frac * n_samples)))\n",
    "    n_cv = n_samples - n_final\n",
    "    if n_cv <= 10:\n",
    "        raise ValueError(\"Too few samples left for CV after holdout split.\")\n",
    "\n",
    "    idx_cv = np.arange(0, n_cv, dtype=np.int64)\n",
    "    idx_final = np.arange(n_cv, n_samples, dtype=np.int64)\n",
    "    return idx_cv, idx_final, n_cv, n_final\n",
    "\n",
    "idx_cv_all, idx_final_test, n_samples_cv, n_samples_final = make_final_holdout_split(\n",
    "    n_samples=n_samples,\n",
    "    final_test_frac=CFG[\"final_test_frac\"],\n",
    ")\n",
    "\n",
    "print(\"Holdout split:\")\n",
    "print(\"  n_samples total:\", n_samples)\n",
    "print(\"  n_samples CV   :\", n_samples_cv, f\"({100*(n_samples_cv/n_samples):.1f}%)\")\n",
    "print(\"  n_samples FINAL:\", n_samples_final, f\"({100*(n_samples_final/n_samples):.1f}%)\")\n",
    "print(\"  CV range   :\", idx_cv_all[0], idx_cv_all[-1])\n",
    "print(\"  FINAL range:\", idx_final_test[0], idx_final_test[-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd91f31",
   "metadata": {},
   "source": [
    "\n",
    "## 5. walk-forward splits (с глобальными окнами)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0efe2eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_folds: 4\n",
      " fold 1: train 1168 | val 233 | test 233\n",
      " fold 2: train 1401 | val 233 | test 233\n",
      " fold 3: train 1634 | val 233 | test 233\n",
      " fold 4: train 1867 | val 233 | test 233\n",
      "\n",
      "FINAL HOLDOUT:\n",
      " final_test size: 260\n"
     ]
    }
   ],
   "source": [
    "# ЛОГИЧЕСКИЙ БЛОК: walk-forward splits (expanding train + fixed val/test) on CV-part only\n",
    "# ИСПОЛНЕНИЕ БЛОКА:\n",
    "\n",
    "def make_walk_forward_splits(n_samples: int,\n",
    "                             train_min_frac: float,\n",
    "                             val_window_frac: float,\n",
    "                             test_window_frac: float,\n",
    "                             step_window_frac: float):\n",
    "    train_min = int(train_min_frac * n_samples)\n",
    "    val_w  = max(1, int(val_window_frac * n_samples))\n",
    "    test_w = max(1, int(test_window_frac * n_samples))\n",
    "    step_w = max(1, int(step_window_frac * n_samples))\n",
    "\n",
    "    splits = []\n",
    "    start = train_min\n",
    "    while True:\n",
    "        tr_end = start\n",
    "        va_end = tr_end + val_w\n",
    "        te_end = va_end + test_w\n",
    "        if te_end > n_samples:\n",
    "            break\n",
    "\n",
    "        idx_train = np.arange(0, tr_end, dtype=np.int64)\n",
    "        idx_val   = np.arange(tr_end, va_end, dtype=np.int64)\n",
    "        idx_test  = np.arange(va_end, te_end, dtype=np.int64)\n",
    "\n",
    "        splits.append((idx_train, idx_val, idx_test))\n",
    "        start += step_w\n",
    "\n",
    "    return splits\n",
    "\n",
    "# IMPORTANT: строим сплиты только на 90% (CV-part)\n",
    "walk_splits = make_walk_forward_splits(\n",
    "    n_samples=n_samples_cv,\n",
    "    train_min_frac=CFG[\"train_min_frac\"],\n",
    "    val_window_frac=CFG[\"val_window_frac\"],\n",
    "    test_window_frac=CFG[\"test_window_frac\"],\n",
    "    step_window_frac=CFG[\"step_window_frac\"],\n",
    ")\n",
    "\n",
    "print(\"n_folds:\", len(walk_splits))\n",
    "for i, (a, b, c) in enumerate(walk_splits):\n",
    "    print(f\" fold {i+1}: train {len(a)} | val {len(b)} | test {len(c)}\")\n",
    "\n",
    "print(\"\\nFINAL HOLDOUT:\")\n",
    "print(\" final_test size:\", len(idx_final_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb84cf3",
   "metadata": {},
   "source": [
    "## 6. Dataset + scaling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e7878f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SANITY batch shapes: torch.Size([64, 84, 3, 15]) torch.Size([64, 84, 3, 5]) torch.Size([64]) torch.Size([64]) torch.Size([64]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Step 8. Dataset + Scaling\n",
    "# =========================\n",
    "# ЛОГИЧЕСКИЙ БЛОК: подготовка y для sample_t + scaler fit on train only + Dataset/Dataloader\n",
    "# ИСПОЛНЕНИЕ БЛОКА:\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "def fit_robust_scaler_past_only(X_node_raw: np.ndarray, max_t_fit: int):\n",
    "    \"\"\"\n",
    "    Fit scaler on ALL nodes/features using only times [0..max_t_fit] (past only).\n",
    "    X_node_raw: (T,N,F)\n",
    "    \"\"\"\n",
    "    scaler = RobustScaler(with_centering=True, with_scaling=True, quantile_range=(25.0, 75.0))\n",
    "    X_fit = X_node_raw[:max_t_fit+1]  # (T_fit,N,F)\n",
    "    T_fit, N, F = X_fit.shape\n",
    "    scaler.fit(X_fit.reshape(-1, F))\n",
    "    return scaler\n",
    "\n",
    "def transform_and_clip(X_node_raw: np.ndarray, scaler: RobustScaler, max_abs_feat: float):\n",
    "    \"\"\"\n",
    "    Transform whole timeline with scaler fitted on past and clip.\n",
    "    \"\"\"\n",
    "    T, N, F = X_node_raw.shape\n",
    "    X = scaler.transform(X_node_raw.reshape(-1, F)).reshape(T, N, F).astype(np.float32)\n",
    "    if max_abs_feat is not None:\n",
    "        X = np.clip(X, -max_abs_feat, max_abs_feat)\n",
    "    return X\n",
    "\n",
    "@dataclass\n",
    "class Batch:\n",
    "    x: torch.Tensor        # (B,L,N,F)\n",
    "    e: torch.Tensor        # (B,L,E,W)\n",
    "    y_trade: torch.Tensor  # (B,)\n",
    "    y_dir: torch.Tensor    # (B,) valid only if y_trade==1\n",
    "    y_tb: torch.Tensor     # (B,) {0,1,2}\n",
    "    exit_ret: torch.Tensor # (B,)\n",
    "\n",
    "class WindowGraphDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 X_node: torch.Tensor,      # (T,N,F) scaled\n",
    "                 E_feat: torch.Tensor,      # (T,E,W)\n",
    "                 sample_t_list: np.ndarray, # (n_samples,)\n",
    "                 y_trade_t: np.ndarray,\n",
    "                 y_dir_t: np.ndarray,\n",
    "                 y_tb_t: np.ndarray,\n",
    "                 exit_ret_t: np.ndarray,\n",
    "                 lookback: int):\n",
    "        self.X_node = X_node\n",
    "        self.E_feat = E_feat\n",
    "        self.sample_t = sample_t_list.astype(np.int64)\n",
    "        self.y_trade = y_trade_t.astype(np.int64)\n",
    "        self.y_dir = y_dir_t.astype(np.int64)\n",
    "        self.y_tb = y_tb_t.astype(np.int64)\n",
    "        self.exit_ret = exit_ret_t.astype(np.float32)\n",
    "        self.L = int(lookback)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sample_t)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        t = int(self.sample_t[idx])\n",
    "        L = self.L\n",
    "        s = t - L + 1\n",
    "        x_win = self.X_node[s:t+1]   # (L,N,F)\n",
    "        e_win = self.E_feat[s:t+1]   # (L,E,W)\n",
    "\n",
    "        return (\n",
    "            x_win,\n",
    "            e_win,\n",
    "            torch.tensor(self.y_trade[idx], dtype=torch.long),\n",
    "            torch.tensor(self.y_dir[idx], dtype=torch.long),\n",
    "            torch.tensor(self.y_tb[idx], dtype=torch.long),\n",
    "            torch.tensor(self.exit_ret[idx], dtype=torch.float32),\n",
    "        )\n",
    "\n",
    "def make_loaders_for_fold(\n",
    "    X_node_raw: np.ndarray,\n",
    "    edge_feat: np.ndarray,\n",
    "    sample_t: np.ndarray,\n",
    "    y_tb: np.ndarray,\n",
    "    y_trade: np.ndarray,\n",
    "    y_dir: np.ndarray,\n",
    "    exit_ret: np.ndarray,\n",
    "    idx_train: np.ndarray,\n",
    "    idx_val: np.ndarray,\n",
    "    idx_test: np.ndarray,\n",
    "    batch_size: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    Важно: idx_* — индексы в пространстве CV-сэмплов (0..n_samples_cv-1).\n",
    "    Здесь sample_t передаём уже CV-часть (т.е. sample_t_cv).\n",
    "    \"\"\"\n",
    "    # fit scaler on past only (up to max train t)\n",
    "    max_train_t = int(sample_t[idx_train][-1])\n",
    "    scaler = fit_robust_scaler_past_only(X_node_raw, max_t_fit=max_train_t)\n",
    "    X_scaled = transform_and_clip(X_node_raw, scaler, CFG[\"max_abs_feat\"])\n",
    "\n",
    "    # torch tensors (cpu)\n",
    "    X_t = torch.from_numpy(X_scaled)                         # (T,N,F)\n",
    "    E_t = torch.from_numpy(edge_feat.astype(np.float32))     # (T,E,W)\n",
    "\n",
    "    # labels aligned to dataset index-space (same length as sample_t)\n",
    "    y_tb_t    = y_tb[sample_t]\n",
    "    y_trade_t = y_trade[sample_t]\n",
    "    y_dir_t   = y_dir[sample_t]\n",
    "    exit_t    = exit_ret[sample_t]\n",
    "\n",
    "    ds_train = WindowGraphDataset(X_t, E_t, sample_t[idx_train], y_trade_t[idx_train], y_dir_t[idx_train], y_tb_t[idx_train], exit_t[idx_train], CFG[\"lookback\"])\n",
    "    ds_val   = WindowGraphDataset(X_t, E_t, sample_t[idx_val],   y_trade_t[idx_val],   y_dir_t[idx_val],   y_tb_t[idx_val],   exit_t[idx_val],   CFG[\"lookback\"])\n",
    "    ds_test  = WindowGraphDataset(X_t, E_t, sample_t[idx_test],  y_trade_t[idx_test],  y_dir_t[idx_test],  y_tb_t[idx_test],  exit_t[idx_test],  CFG[\"lookback\"])\n",
    "\n",
    "    pin = (DEVICE.type == \"cuda\")\n",
    "    dl_train = DataLoader(ds_train, batch_size=batch_size, shuffle=True, drop_last=False, num_workers=0, pin_memory=pin)\n",
    "    dl_val   = DataLoader(ds_val,   batch_size=batch_size, shuffle=False, drop_last=False, num_workers=0, pin_memory=pin)\n",
    "    dl_test  = DataLoader(ds_test,  batch_size=batch_size, shuffle=False, drop_last=False, num_workers=0, pin_memory=pin)\n",
    "\n",
    "    return dl_train, dl_val, dl_test, scaler\n",
    "\n",
    "# --- quick sanity (не тренируем, просто проверяем что fold 1 loader собирается) ---\n",
    "sample_t_cv = sample_t[:n_samples_cv]  # CV-part only (time-ordered)\n",
    "(idx_train0, idx_val0, idx_test0) = walk_splits[0]\n",
    "dl_tr0, dl_va0, dl_te0, _ = make_loaders_for_fold(\n",
    "    X_node_raw=X_node_raw,\n",
    "    edge_feat=edge_feat,\n",
    "    sample_t=sample_t_cv,\n",
    "    y_tb=y_tb,\n",
    "    y_trade=y_trade,\n",
    "    y_dir=y_dir,\n",
    "    exit_ret=exit_ret,\n",
    "    idx_train=idx_train0,\n",
    "    idx_val=idx_val0,\n",
    "    idx_test=idx_test0,\n",
    "    batch_size=CFG[\"batch_size\"],\n",
    ")\n",
    "xb, eb, ytr, ydr, ytb_b, exb = next(iter(dl_tr0))\n",
    "print(\"SANITY batch shapes:\",\n",
    "      xb.shape, eb.shape, ytr.shape, ydr.shape, ytb_b.shape, exb.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2bcb07",
   "metadata": {},
   "source": [
    "## 7.Model (один класс, n_classes=2) + EdgeGatedMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4ffc3a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SANITY logits3: torch.Size([64, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/recbole_new_env/lib/python3.11/site-packages/torch/nn/modules/transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Step 9 (REPLACE). GNN + Temporal Attention + 3-class head\n",
    "# ============================================\n",
    "# ЛОГИЧЕСКИЙ БЛОК: spatial GNN + temporal attention + single 3-class logits\n",
    "# ИСПОЛНЕНИЕ БЛОКА:\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def _fix_heads(d_model: int, heads: int) -> int:\n",
    "    for h in range(heads, 0, -1):\n",
    "        if d_model % h == 0:\n",
    "            return h\n",
    "    return 1\n",
    "\n",
    "class EdgeGatedMPLayer(nn.Module):\n",
    "    def __init__(self, hidden: int, edge_dim: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.msg_lin = nn.Linear(hidden, hidden, bias=False)\n",
    "        self.gate_mlp = nn.Sequential(\n",
    "            nn.Linear(2 * hidden + edge_dim, hidden),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden, hidden),\n",
    "        )\n",
    "        self.ln1 = nn.LayerNorm(hidden)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(hidden, 4 * hidden),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(4 * hidden, hidden),\n",
    "        )\n",
    "        self.ln2 = nn.LayerNorm(hidden)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, h: torch.Tensor, edge_index: torch.Tensor, e: torch.Tensor):\n",
    "        # h: (BL,N,H), e: (BL,E,W), edge_index: (E,2) [src,dst]\n",
    "        BL, N, H = h.shape\n",
    "        src = edge_index[:, 0]\n",
    "        dst = edge_index[:, 1]\n",
    "\n",
    "        h_src = h[:, src, :]  # (BL,E,H)\n",
    "        h_dst = h[:, dst, :]  # (BL,E,H)\n",
    "\n",
    "        gate = torch.sigmoid(self.gate_mlp(torch.cat([h_src, h_dst, e], dim=-1)))  # (BL,E,H)\n",
    "        msg = self.msg_lin(h_src) * gate  # (BL,E,H)\n",
    "\n",
    "        agg = torch.zeros(BL, N, H, device=h.device, dtype=h.dtype)\n",
    "        agg.index_add_(1, dst, msg)\n",
    "\n",
    "        h2 = self.ln1(h + self.drop(agg))\n",
    "        h3 = self.ln2(h2 + self.drop(self.ff(h2)))\n",
    "        return h3\n",
    "\n",
    "class SpatialGNN(nn.Module):\n",
    "    def __init__(self, in_dim: int, hidden: int, edge_dim: int, layers: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.in_proj = nn.Linear(in_dim, hidden)\n",
    "        self.layers = nn.ModuleList([EdgeGatedMPLayer(hidden, edge_dim, dropout) for _ in range(layers)])\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor, e: torch.Tensor):\n",
    "        # x: (B,L,N,F), e: (B,L,E,W) -> h: (B,L,N,H)\n",
    "        B, L, N, _ = x.shape\n",
    "        h = self.drop(self.in_proj(x))  # (B,L,N,H)\n",
    "        H = h.shape[-1]\n",
    "\n",
    "        h = h.reshape(B * L, N, H)\n",
    "        e = e.reshape(B * L, e.shape[2], e.shape[3])\n",
    "\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, edge_index, e)\n",
    "\n",
    "        return h.reshape(B, L, N, H)\n",
    "\n",
    "class TemporalEncoderXformerCLS(nn.Module):\n",
    "    def __init__(self, d_model: int, n_layers: int, heads: int, dropout: float, ff_mult: int,\n",
    "                 use_pos_emb: bool, causal: bool, max_len: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.use_pos = use_pos_emb\n",
    "        self.causal = causal\n",
    "\n",
    "        self.cls = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "        self.pos = nn.Parameter(torch.zeros(1, max_len + 1, d_model)) if use_pos_emb else None  # +1 for CLS\n",
    "\n",
    "        heads = _fix_heads(d_model, heads)\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=heads,\n",
    "            dim_feedforward=ff_mult * d_model,\n",
    "            dropout=dropout,\n",
    "            activation=\"gelu\",\n",
    "            batch_first=True,\n",
    "            norm_first=True,\n",
    "        )\n",
    "        self.enc = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # x: (B,L,H) -> (B,H) via CLS\n",
    "        B, L, H = x.shape\n",
    "        cls = self.cls.expand(B, 1, H)\n",
    "        z = torch.cat([cls, x], dim=1)  # (B,L+1,H)\n",
    "\n",
    "        if self.pos is not None:\n",
    "            z = z + self.pos[:, :L + 1, :]\n",
    "\n",
    "        attn_mask = None\n",
    "        if self.causal:\n",
    "            sz = L + 1\n",
    "            attn_mask = torch.triu(torch.ones(sz, sz, device=z.device, dtype=torch.bool), diagonal=1)\n",
    "\n",
    "        z = self.enc(z, mask=attn_mask)\n",
    "        return z[:, 0, :]\n",
    "\n",
    "class TemporalEncoderAttnPool(nn.Module):\n",
    "    def __init__(self, d_model: int, heads: int, dropout: float, use_pos_emb: bool, max_len: int):\n",
    "        super().__init__()\n",
    "        self.use_pos = use_pos_emb\n",
    "        self.q = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "        self.pos = nn.Parameter(torch.zeros(1, max_len, d_model)) if use_pos_emb else None\n",
    "\n",
    "        heads = _fix_heads(d_model, heads)\n",
    "        self.mha = nn.MultiheadAttention(d_model, heads, dropout=dropout, batch_first=True)\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # x: (B,L,H) -> (B,H)\n",
    "        if self.pos is not None:\n",
    "            x = x + self.pos[:, :x.shape[1], :]\n",
    "        q = self.q.expand(x.shape[0], 1, x.shape[2])\n",
    "        out, _ = self.mha(q, x, x, need_weights=False)\n",
    "        return self.ln(out.squeeze(1))\n",
    "\n",
    "class GNNTemporal3Class(nn.Module):\n",
    "    def __init__(self, node_in: int, edge_in: int):\n",
    "        super().__init__()\n",
    "        dropout = CFG[\"dropout\"] if CFG[\"attn_dropout\"] is None else CFG[\"attn_dropout\"]\n",
    "        hidden = CFG[\"hidden\"]\n",
    "        d_model = CFG[\"Attn_hidden\"]\n",
    "        assert hidden == d_model, \"Ожидаю CFG['hidden'] == CFG['Attn_hidden']\"\n",
    "\n",
    "        self.gnn = SpatialGNN(node_in, hidden, edge_in, CFG[\"gnn_layers\"], CFG[\"dropout\"])\n",
    "\n",
    "        max_len = CFG[\"lookback\"]\n",
    "        if CFG[\"temporal_mode\"] == \"xformer_cls\":\n",
    "            self.temporal = TemporalEncoderXformerCLS(\n",
    "                d_model=d_model,\n",
    "                n_layers=CFG[\"Attn_layers\"],\n",
    "                heads=CFG[\"attn_heads\"],\n",
    "                dropout=dropout,\n",
    "                ff_mult=CFG[\"attn_ff_mult\"],\n",
    "                use_pos_emb=CFG[\"attn_use_pos_emb\"],\n",
    "                causal=CFG[\"attn_causal\"],\n",
    "                max_len=max_len,\n",
    "            )\n",
    "        elif CFG[\"temporal_mode\"] == \"attn_pool\":\n",
    "            self.temporal = TemporalEncoderAttnPool(\n",
    "                d_model=d_model,\n",
    "                heads=CFG[\"attn_heads\"],\n",
    "                dropout=dropout,\n",
    "                use_pos_emb=CFG[\"attn_use_pos_emb\"],\n",
    "                max_len=max_len,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown temporal_mode={CFG['temporal_mode']}\")\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Dropout(CFG[\"dropout\"]),\n",
    "            nn.Linear(d_model, 3),  # down / flat / up\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, e: torch.Tensor, edge_index: torch.Tensor, target_node: int):\n",
    "        # x: (B,L,N,F), e: (B,L,E,W) -> logits3: (B,3)\n",
    "        h = self.gnn(x, edge_index, e)              # (B,L,N,H)\n",
    "        h_tgt = h[:, :, target_node, :]             # (B,L,H)\n",
    "        z = self.temporal(h_tgt)                    # (B,H)\n",
    "        return self.head(z)\n",
    "\n",
    "# --- sanity forward ---\n",
    "node_in_dim = X_node_raw.shape[-1]\n",
    "edge_in_dim = edge_feat.shape[-1]\n",
    "model = GNNTemporal3Class(node_in=node_in_dim, edge_in=edge_in_dim).to(DEVICE)\n",
    "\n",
    "xb, eb, ytr, ydr, ytb_b, exb = next(iter(dl_tr0))\n",
    "with torch.no_grad():\n",
    "    logits3 = model(xb.to(DEVICE), eb.to(DEVICE), EDGE_INDEX.to(DEVICE), TARGET_NODE)\n",
    "print(\"SANITY logits3:\", logits3.shape)  # (B,3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8be8053",
   "metadata": {},
   "source": [
    "## 8. Training/Eval: Stage A (trade) и Stage B (direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d26195c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK: metrics+pnl v2 ready.\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# Step 10 (REPLACE v2). Metrics + PnL stats for 3-class head\n",
    "# ==========================\n",
    "# ЛОГИЧЕСКИЙ БЛОК: f1 (3-class), AUC_trade, AUC_OVR_3c, PnL grid + mean/std/sharpe-proxy\n",
    "# ИСПОЛНЕНИЕ БЛОКА:\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "EPS = 1e-9\n",
    "\n",
    "def safe_auc_binary(y_true: np.ndarray, y_score: np.ndarray):\n",
    "    try:\n",
    "        if len(np.unique(y_true)) < 2:\n",
    "            return float(\"nan\")\n",
    "        return float(roc_auc_score(y_true, y_score))\n",
    "    except Exception:\n",
    "        return float(\"nan\")\n",
    "\n",
    "def safe_auc_ovr_multiclass(y_true: np.ndarray, y_prob_3: np.ndarray):\n",
    "    try:\n",
    "        classes = np.unique(y_true)\n",
    "        if len(classes) < 2:\n",
    "            return float(\"nan\")\n",
    "        return float(roc_auc_score(y_true, y_prob_3, multi_class=\"ovr\", average=\"macro\"))\n",
    "    except Exception:\n",
    "        return float(\"nan\")\n",
    "\n",
    "def probs_from_logits3(logits3: torch.Tensor):\n",
    "    return torch.softmax(logits3, dim=-1)\n",
    "\n",
    "def derive_p_trade_and_p_up_cond(p3: np.ndarray):\n",
    "    p_down = p3[:, 0]\n",
    "    p_flat = p3[:, 1]\n",
    "    p_up   = p3[:, 2]\n",
    "\n",
    "    p_trade = 1.0 - p_flat\n",
    "    denom = p_up + p_down\n",
    "    p_up_cond = np.where(denom > EPS, p_up / denom, 0.5)\n",
    "    return p_trade, p_up_cond\n",
    "\n",
    "def derive_pred_tb_from_two_thresholds(p3: np.ndarray, thr_trade: float=0.5, thr_dir: float=0.5):\n",
    "    p_trade, p_up_cond = derive_p_trade_and_p_up_cond(p3)\n",
    "    conf_dir = np.maximum(p_up_cond, 1.0 - p_up_cond)\n",
    "\n",
    "    pred = np.ones(len(p_trade), dtype=np.int64)  # flat\n",
    "    take = (p_trade >= thr_trade) & (conf_dir >= thr_dir)\n",
    "    pred[take & (p_up_cond >= 0.5)] = 2\n",
    "    pred[take & (p_up_cond < 0.5)]  = 0\n",
    "    return pred\n",
    "\n",
    "def pnl_from_preds(pred_tb: np.ndarray, exit_ret: np.ndarray, cost_bps: float):\n",
    "    pos = np.zeros_like(exit_ret, dtype=np.float32)\n",
    "    pos[pred_tb == 2] = 1.0\n",
    "    pos[pred_tb == 0] = -1.0\n",
    "    gross = pos * exit_ret\n",
    "\n",
    "    trade = (pred_tb != 1).astype(np.float32)\n",
    "    cost = (cost_bps / 1e4) * trade\n",
    "    return gross - cost  # net pnl per sample\n",
    "\n",
    "def pnl_stats(net: np.ndarray, pred_tb: np.ndarray):\n",
    "    \"\"\"\n",
    "    Stats on executed trades only.\n",
    "    sharpe-proxy = mean/std * sqrt(n_trades)\n",
    "    \"\"\"\n",
    "    trade_mask = (pred_tb != 1)\n",
    "    n_trades = int(trade_mask.sum())\n",
    "    if n_trades == 0:\n",
    "        return {\"trades\": 0, \"sum\": 0.0, \"mean\": float(\"nan\"), \"std\": float(\"nan\"), \"sharpe\": float(\"nan\")}\n",
    "    x = net[trade_mask].astype(np.float64)\n",
    "    m = float(x.mean())\n",
    "    s = float(x.std(ddof=1)) if n_trades > 1 else float(\"nan\")\n",
    "    sh = float(\"nan\") if (not np.isfinite(s) or s <= 0) else float(m / s * math.sqrt(n_trades))\n",
    "    return {\"trades\": n_trades, \"sum\": float(x.sum()), \"mean\": m, \"std\": s, \"sharpe\": sh}\n",
    "\n",
    "def pnl_grid_search_from_p3(p3: np.ndarray, y_tb_true: np.ndarray, exit_ret: np.ndarray,\n",
    "                            cost_bps: float, thr_trade_grid, thr_dir_grid, min_trades: int=50):\n",
    "    best = {\n",
    "        \"pnl_sum\": -1e18,\n",
    "        \"thr_trade\": None,\n",
    "        \"thr_dir\": None,\n",
    "        \"trades\": 0,\n",
    "        \"pnl_mean\": float(\"nan\"),\n",
    "        \"pnl_std\": float(\"nan\"),\n",
    "        \"pnl_sharpe\": float(\"nan\"),\n",
    "        \"f1m\": float(\"nan\"),\n",
    "    }\n",
    "    for tt in thr_trade_grid:\n",
    "        for td in thr_dir_grid:\n",
    "            pred = derive_pred_tb_from_two_thresholds(p3, tt, td)\n",
    "            trades = int(np.sum(pred != 1))\n",
    "            if trades < min_trades:\n",
    "                continue\n",
    "\n",
    "            net = pnl_from_preds(pred, exit_ret, cost_bps)\n",
    "            st = pnl_stats(net, pred)\n",
    "\n",
    "            pnl_sum = st[\"sum\"]\n",
    "            f1m = float(f1_score(y_tb_true, pred, average=\"macro\"))\n",
    "\n",
    "            if pnl_sum > best[\"pnl_sum\"]:\n",
    "                best = {\n",
    "                    \"pnl_sum\": pnl_sum,\n",
    "                    \"thr_trade\": float(tt),\n",
    "                    \"thr_dir\": float(td),\n",
    "                    \"trades\": trades,\n",
    "                    \"pnl_mean\": st[\"mean\"],\n",
    "                    \"pnl_std\": st[\"std\"],\n",
    "                    \"pnl_sharpe\": st[\"sharpe\"],\n",
    "                    \"f1m\": f1m,\n",
    "                }\n",
    "    return best\n",
    "\n",
    "print(\"OK: metrics+pnl v2 ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8d8c16",
   "metadata": {},
   "source": [
    "## 9. Two-stage PnL by confidence thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5c80174b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK: 3-class train/eval v2 (best by PnL) ready.\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# Step 11 (REPLACE v2). Train / Eval for 3-class head (select best epoch by PnL)\n",
    "# ==========================\n",
    "# ЛОГИЧЕСКИЙ БЛОК: CE(3-class) + лог tr_loss, va_loss, f1, va_auc, pnl_sum/mean/std/sharpe\n",
    "# ИСПОЛНЕНИЕ БЛОКА:\n",
    "\n",
    "def train_one_epoch_3c(model, dl, optimizer, edge_index, target_node):\n",
    "    model.train()\n",
    "    total_loss, n = 0.0, 0\n",
    "\n",
    "    use_amp = bool(CFG[\"use_amp\"] and DEVICE.type == \"cuda\")\n",
    "    gscaler = torch.amp.GradScaler('cuda', enabled=use_amp)\n",
    "\n",
    "    for xb, eb, ytr, ydr, ytb_b, exb in dl:\n",
    "        xb = xb.to(DEVICE)\n",
    "        eb = eb.to(DEVICE)\n",
    "        ytb = ytb_b.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.amp.autocast('cuda', enabled=use_amp):\n",
    "            logits3 = model(xb, eb, edge_index, target_node)\n",
    "            loss = F.cross_entropy(logits3, ytb)\n",
    "\n",
    "        gscaler.scale(loss).backward()\n",
    "        if CFG[\"grad_clip\"] is not None:\n",
    "            gscaler.unscale_(optimizer)\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), CFG[\"grad_clip\"])\n",
    "        gscaler.step(optimizer)\n",
    "        gscaler.update()\n",
    "\n",
    "        bs = xb.size(0)\n",
    "        total_loss += float(loss.item()) * bs\n",
    "        n += bs\n",
    "\n",
    "    return total_loss / max(1, n)\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_one_epoch_3c(model, dl, edge_index, target_node):\n",
    "    model.eval()\n",
    "    total_loss, n = 0.0, 0\n",
    "\n",
    "    all_logits, all_y, all_ex = [], [], []\n",
    "    for xb, eb, ytr, ydr, ytb_b, exb in dl:\n",
    "        xb = xb.to(DEVICE)\n",
    "        eb = eb.to(DEVICE)\n",
    "        ytb = ytb_b.to(DEVICE)\n",
    "\n",
    "        logits3 = model(xb, eb, edge_index, target_node)\n",
    "        loss = F.cross_entropy(logits3, ytb)\n",
    "\n",
    "        bs = xb.size(0)\n",
    "        total_loss += float(loss.item()) * bs\n",
    "        n += bs\n",
    "\n",
    "        all_logits.append(logits3.detach().cpu())\n",
    "        all_y.append(ytb_b.numpy())\n",
    "        all_ex.append(exb.numpy())\n",
    "\n",
    "    va_loss = total_loss / max(1, n)\n",
    "\n",
    "    logits = torch.cat(all_logits, dim=0)             # (N,3)\n",
    "    p3 = probs_from_logits3(logits).numpy()           # (N,3)\n",
    "    y = np.concatenate(all_y).astype(np.int64)        # (N,)\n",
    "    ex = np.concatenate(all_ex).astype(np.float32)    # (N,)\n",
    "\n",
    "    # классификация \"как есть\" по argmax\n",
    "    pred_argmax = np.argmax(p3, axis=1)\n",
    "    f1m = float(f1_score(y, pred_argmax, average=\"macro\"))\n",
    "\n",
    "    # AUC trade (flat vs trade): p_trade = 1 - p_flat\n",
    "    y_trade_true = (y != 1).astype(np.int64)\n",
    "    p_trade, _ = derive_p_trade_and_p_up_cond(p3)\n",
    "    auc_trade = safe_auc_binary(y_trade_true, p_trade)\n",
    "\n",
    "    # optional: multiclass ovr auc\n",
    "    auc_ovr_3c = safe_auc_ovr_multiclass(y, p3)\n",
    "\n",
    "    # best PnL by thresholds\n",
    "    thr_trade_grid = CFG[\"thr_trade_grid\"] if CFG[\"proxy_thr_trade_grid\"] is None else CFG[\"proxy_thr_trade_grid\"]\n",
    "    thr_dir_grid   = CFG[\"thr_dir_grid\"]   if CFG[\"proxy_thr_dir_grid\"]   is None else CFG[\"proxy_thr_dir_grid\"]\n",
    "\n",
    "    best = pnl_grid_search_from_p3(\n",
    "        p3, y, ex,\n",
    "        cost_bps=CFG[\"cost_bps\"],\n",
    "        thr_trade_grid=thr_trade_grid,\n",
    "        thr_dir_grid=thr_dir_grid,\n",
    "        min_trades=CFG[\"proxy_min_trades\"],\n",
    "    )\n",
    "\n",
    "    if best[\"thr_trade\"] is not None:\n",
    "        pred_best = derive_pred_tb_from_two_thresholds(p3, best[\"thr_trade\"], best[\"thr_dir\"])\n",
    "        net = pnl_from_preds(pred_best, ex, CFG[\"cost_bps\"])\n",
    "        st = pnl_stats(net, pred_best)\n",
    "    else:\n",
    "        st = {\"sum\": float(\"nan\"), \"mean\": float(\"nan\"), \"std\": float(\"nan\"), \"sharpe\": float(\"nan\"), \"trades\": 0}\n",
    "\n",
    "    return {\n",
    "        \"va_loss\": va_loss,\n",
    "        \"f1m_3c\": f1m,\n",
    "        \"va_auc\": auc_trade,\n",
    "        \"auc_ovr_3c\": auc_ovr_3c,\n",
    "\n",
    "        \"pnl_best_sum\": st[\"sum\"],\n",
    "        \"pnl_best_mean\": st[\"mean\"],\n",
    "        \"pnl_best_std\": st[\"std\"],\n",
    "        \"pnl_best_sharpe\": st[\"sharpe\"],\n",
    "        \"pnl_best_trades\": st[\"trades\"],\n",
    "\n",
    "        \"best_thr_trade\": best[\"thr_trade\"],\n",
    "        \"best_thr_dir\": best[\"thr_dir\"],\n",
    "    }\n",
    "\n",
    "def run_fold_training_3c(fold_id: int, dl_train, dl_val, dl_test):\n",
    "    model = GNNTemporal3Class(node_in=node_in_dim, edge_in=edge_in_dim).to(DEVICE)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=CFG[\"lr\"], weight_decay=CFG[\"weight_decay\"])\n",
    "    edge_index = EDGE_INDEX.to(DEVICE)\n",
    "\n",
    "    # выбираем лучшую эпоху по PnL на val\n",
    "    best_key = \"pnl_best_sum\"\n",
    "    best_val = -1e18\n",
    "    best_state = None\n",
    "\n",
    "    for ep in range(1, CFG[\"epochs\"] + 1):\n",
    "        tr_loss = train_one_epoch_3c(model, dl_train, opt, edge_index, TARGET_NODE)\n",
    "        va = eval_one_epoch_3c(model, dl_val, edge_index, TARGET_NODE)\n",
    "\n",
    "        key = va[best_key]\n",
    "        if key == key and key > best_val:  # not nan\n",
    "            best_val = key\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "        print(f\"[FOLD {fold_id}] ep {ep:02d} lr={CFG['lr']:.2e} \"\n",
    "              f\"tr_loss={tr_loss:.4f} va_loss={va['va_loss']:.4f} \"\n",
    "              f\"f1={va['f1m_3c']:.3f} va_auc={va['va_auc']:.3f} \"\n",
    "              f\"pnl_sum={va['pnl_best_sum']:.4f} pnl_mean={va['pnl_best_mean']:.6f} \"\n",
    "              f\"pnl_std={va['pnl_best_std']:.6f} sharpe~={va['pnl_best_sharpe']:.3f} \"\n",
    "              f\"trades={va['pnl_best_trades']} thr=({va['best_thr_trade']},{va['best_thr_dir']}) \"\n",
    "              f\"auc_ovr3c={va['auc_ovr_3c']:.3f}\")\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    te = eval_one_epoch_3c(model, dl_test, edge_index, TARGET_NODE)\n",
    "    print(f\"[FOLD {fold_id}] TEST: loss={te['va_loss']:.4f} f1={te['f1m_3c']:.3f} \"\n",
    "          f\"va_auc={te['va_auc']:.3f} pnl_sum={te['pnl_best_sum']:.4f} \"\n",
    "          f\"pnl_mean={te['pnl_best_mean']:.6f} pnl_std={te['pnl_best_std']:.6f} \"\n",
    "          f\"sharpe~={te['pnl_best_sharpe']:.3f} trades={te['pnl_best_trades']} \"\n",
    "          f\"thr=({te['best_thr_trade']},{te['best_thr_dir']}) auc_ovr3c={te['auc_ovr_3c']:.3f}\")\n",
    "\n",
    "    return {\"test\": te}\n",
    "\n",
    "print(\"OK: 3-class train/eval v2 (best by PnL) ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15582194",
   "metadata": {},
   "source": [
    "## 10. Run folds: scale once → train trade → filter trades → train dir → PnL sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4d955868",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/recbole_new_env/lib/python3.11/site-packages/torch/nn/modules/transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FOLD 1] ep 01 lr=2.00e-04 tr_loss=0.9883 va_loss=0.9627 f1=0.254 va_auc=0.379 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.400\n",
      "[FOLD 1] ep 02 lr=2.00e-04 tr_loss=0.9646 va_loss=0.9556 f1=0.254 va_auc=0.359 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.403\n",
      "[FOLD 1] ep 03 lr=2.00e-04 tr_loss=0.9535 va_loss=0.9459 f1=0.254 va_auc=0.375 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.420\n",
      "[FOLD 1] ep 04 lr=2.00e-04 tr_loss=0.9366 va_loss=0.9447 f1=0.254 va_auc=0.422 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.462\n",
      "[FOLD 1] ep 05 lr=2.00e-04 tr_loss=0.9327 va_loss=0.9429 f1=0.254 va_auc=0.438 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.501\n",
      "[FOLD 1] ep 06 lr=2.00e-04 tr_loss=0.9187 va_loss=0.9640 f1=0.254 va_auc=0.387 pnl_sum=-0.0187 pnl_mean=-0.000415 pnl_std=0.005227 sharpe~=-0.533 trades=45 thr=(0.5,0.55) auc_ovr3c=0.487\n",
      "[FOLD 1] ep 07 lr=2.00e-04 tr_loss=0.9255 va_loss=0.9393 f1=0.254 va_auc=0.380 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.497\n",
      "[FOLD 1] ep 08 lr=2.00e-04 tr_loss=0.8928 va_loss=0.9438 f1=0.254 va_auc=0.357 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.511\n",
      "[FOLD 1] ep 09 lr=2.00e-04 tr_loss=0.8925 va_loss=0.9888 f1=0.254 va_auc=0.362 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.491\n",
      "[FOLD 1] ep 10 lr=2.00e-04 tr_loss=0.8869 va_loss=0.9552 f1=0.254 va_auc=0.357 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.496\n",
      "[FOLD 1] ep 11 lr=2.00e-04 tr_loss=0.8510 va_loss=1.0008 f1=0.253 va_auc=0.377 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.486\n",
      "[FOLD 1] ep 12 lr=2.00e-04 tr_loss=0.8744 va_loss=0.9979 f1=0.254 va_auc=0.377 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.479\n",
      "[FOLD 1] ep 13 lr=2.00e-04 tr_loss=0.8612 va_loss=0.9925 f1=0.254 va_auc=0.350 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.477\n",
      "[FOLD 1] ep 14 lr=2.00e-04 tr_loss=0.8502 va_loss=1.0309 f1=0.266 va_auc=0.336 pnl_sum=-0.0432 pnl_mean=-0.001442 pnl_std=0.005003 sharpe~=-1.578 trades=30 thr=(0.5,0.7) auc_ovr3c=0.448\n",
      "[FOLD 1] ep 15 lr=2.00e-04 tr_loss=0.8582 va_loss=0.9959 f1=0.262 va_auc=0.400 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.493\n",
      "[FOLD 1] ep 16 lr=2.00e-04 tr_loss=0.8412 va_loss=1.0203 f1=0.254 va_auc=0.372 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.499\n",
      "[FOLD 1] ep 17 lr=2.00e-04 tr_loss=0.8365 va_loss=0.9714 f1=0.296 va_auc=0.356 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.478\n",
      "[FOLD 1] ep 18 lr=2.00e-04 tr_loss=0.8265 va_loss=1.0094 f1=0.267 va_auc=0.407 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.510\n",
      "[FOLD 1] ep 19 lr=2.00e-04 tr_loss=0.8321 va_loss=0.9676 f1=0.265 va_auc=0.359 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.487\n",
      "[FOLD 1] ep 20 lr=2.00e-04 tr_loss=0.8374 va_loss=0.9666 f1=0.313 va_auc=0.366 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.495\n",
      "[FOLD 1] ep 21 lr=2.00e-04 tr_loss=0.8278 va_loss=0.9816 f1=0.309 va_auc=0.388 pnl_sum=0.0590 pnl_mean=0.001736 pnl_std=0.005488 sharpe~=1.844 trades=34 thr=(0.55,0.6) auc_ovr3c=0.515\n",
      "[FOLD 1] ep 22 lr=2.00e-04 tr_loss=0.8475 va_loss=0.9817 f1=0.290 va_auc=0.366 pnl_sum=0.0638 pnl_mean=0.001723 pnl_std=0.005586 sharpe~=1.876 trades=37 thr=(0.6,0.5) auc_ovr3c=0.495\n",
      "[FOLD 1] ep 23 lr=2.00e-04 tr_loss=0.8054 va_loss=0.9646 f1=0.309 va_auc=0.346 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.485\n",
      "[FOLD 1] ep 24 lr=2.00e-04 tr_loss=0.8020 va_loss=1.1009 f1=0.254 va_auc=0.355 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.501\n",
      "[FOLD 1] ep 25 lr=2.00e-04 tr_loss=0.8261 va_loss=1.0983 f1=0.254 va_auc=0.386 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.516\n",
      "[FOLD 1] ep 26 lr=2.00e-04 tr_loss=0.8052 va_loss=0.9885 f1=0.254 va_auc=0.371 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.522\n",
      "[FOLD 1] ep 27 lr=2.00e-04 tr_loss=0.7761 va_loss=1.0386 f1=0.253 va_auc=0.381 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.502\n",
      "[FOLD 1] ep 28 lr=2.00e-04 tr_loss=0.7826 va_loss=1.0029 f1=0.253 va_auc=0.405 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.525\n",
      "[FOLD 1] ep 29 lr=2.00e-04 tr_loss=0.7958 va_loss=1.0704 f1=0.254 va_auc=0.387 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.501\n",
      "[FOLD 1] ep 30 lr=2.00e-04 tr_loss=0.7927 va_loss=1.0503 f1=0.254 va_auc=0.398 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.516\n",
      "[FOLD 1] TEST: loss=1.0082 f1=0.236 va_auc=0.470 pnl_sum=-0.1325 pnl_mean=-0.001720 pnl_std=0.003106 sharpe~=-4.861 trades=77 thr=(0.5,0.5) auc_ovr3c=0.458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/recbole_new_env/lib/python3.11/site-packages/torch/nn/modules/transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FOLD 2] ep 01 lr=2.00e-04 tr_loss=1.0513 va_loss=0.8456 f1=0.286 va_auc=0.371 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.334\n",
      "[FOLD 2] ep 02 lr=2.00e-04 tr_loss=0.9576 va_loss=0.7732 f1=0.286 va_auc=0.331 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.310\n",
      "[FOLD 2] ep 03 lr=2.00e-04 tr_loss=0.9466 va_loss=0.7695 f1=0.286 va_auc=0.318 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.294\n",
      "[FOLD 2] ep 04 lr=2.00e-04 tr_loss=0.9342 va_loss=0.8165 f1=0.286 va_auc=0.306 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.281\n",
      "[FOLD 2] ep 05 lr=2.00e-04 tr_loss=0.9249 va_loss=0.8005 f1=0.286 va_auc=0.396 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.346\n",
      "[FOLD 2] ep 06 lr=2.00e-04 tr_loss=0.9087 va_loss=0.8760 f1=0.286 va_auc=0.435 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.384\n",
      "[FOLD 2] ep 07 lr=2.00e-04 tr_loss=0.9020 va_loss=0.9825 f1=0.286 va_auc=0.430 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.394\n",
      "[FOLD 2] ep 08 lr=2.00e-04 tr_loss=0.8742 va_loss=1.1153 f1=0.278 va_auc=0.416 pnl_sum=-0.2108 pnl_mean=-0.003294 pnl_std=0.003007 sharpe~=-8.763 trades=64 thr=(0.5,0.5) auc_ovr3c=0.378\n",
      "[FOLD 2] ep 09 lr=2.00e-04 tr_loss=0.8844 va_loss=1.0126 f1=0.286 va_auc=0.372 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.320\n",
      "[FOLD 2] ep 10 lr=2.00e-04 tr_loss=0.8802 va_loss=1.0963 f1=0.286 va_auc=0.389 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.340\n",
      "[FOLD 2] ep 11 lr=2.00e-04 tr_loss=0.8552 va_loss=0.9897 f1=0.286 va_auc=0.429 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.375\n",
      "[FOLD 2] ep 12 lr=2.00e-04 tr_loss=0.8511 va_loss=1.0518 f1=0.286 va_auc=0.397 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.339\n",
      "[FOLD 2] ep 13 lr=2.00e-04 tr_loss=0.8466 va_loss=1.1700 f1=0.281 va_auc=0.358 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.329\n",
      "[FOLD 2] ep 14 lr=2.00e-04 tr_loss=0.8543 va_loss=1.0924 f1=0.286 va_auc=0.346 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.308\n",
      "[FOLD 2] ep 15 lr=2.00e-04 tr_loss=0.8279 va_loss=1.0483 f1=0.286 va_auc=0.347 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.314\n",
      "[FOLD 2] ep 16 lr=2.00e-04 tr_loss=0.8391 va_loss=1.1154 f1=0.286 va_auc=0.379 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.356\n",
      "[FOLD 2] ep 17 lr=2.00e-04 tr_loss=0.8249 va_loss=1.0595 f1=0.286 va_auc=0.374 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.341\n",
      "[FOLD 2] ep 18 lr=2.00e-04 tr_loss=0.7980 va_loss=1.2216 f1=0.278 va_auc=0.389 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.348\n",
      "[FOLD 2] ep 19 lr=2.00e-04 tr_loss=0.7983 va_loss=1.1131 f1=0.286 va_auc=0.383 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.318\n",
      "[FOLD 2] ep 20 lr=2.00e-04 tr_loss=0.8014 va_loss=1.1097 f1=0.286 va_auc=0.394 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.328\n",
      "[FOLD 2] ep 21 lr=2.00e-04 tr_loss=0.8232 va_loss=1.0667 f1=0.286 va_auc=0.385 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.345\n",
      "[FOLD 2] ep 22 lr=2.00e-04 tr_loss=0.8086 va_loss=1.0321 f1=0.286 va_auc=0.353 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.326\n",
      "[FOLD 2] ep 23 lr=2.00e-04 tr_loss=0.7943 va_loss=1.1102 f1=0.286 va_auc=0.390 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.360\n",
      "[FOLD 2] ep 24 lr=2.00e-04 tr_loss=0.8007 va_loss=1.0456 f1=0.286 va_auc=0.373 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.345\n",
      "[FOLD 2] ep 25 lr=2.00e-04 tr_loss=0.7560 va_loss=1.1024 f1=0.286 va_auc=0.395 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.356\n",
      "[FOLD 2] ep 26 lr=2.00e-04 tr_loss=0.7633 va_loss=1.1047 f1=0.286 va_auc=0.383 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.353\n",
      "[FOLD 2] ep 27 lr=2.00e-04 tr_loss=0.7787 va_loss=1.1190 f1=0.286 va_auc=0.344 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.310\n",
      "[FOLD 2] ep 28 lr=2.00e-04 tr_loss=0.7733 va_loss=1.1351 f1=0.286 va_auc=0.380 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.339\n",
      "[FOLD 2] ep 29 lr=2.00e-04 tr_loss=0.7734 va_loss=1.0799 f1=0.286 va_auc=0.413 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.378\n",
      "[FOLD 2] ep 30 lr=2.00e-04 tr_loss=0.7781 va_loss=1.1245 f1=0.286 va_auc=0.418 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.375\n",
      "[FOLD 2] TEST: loss=1.3803 f1=0.210 va_auc=0.562 pnl_sum=-0.0496 pnl_mean=-0.000774 pnl_std=0.008076 sharpe~=-0.767 trades=64 thr=(0.5,0.5) auc_ovr3c=0.391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/recbole_new_env/lib/python3.11/site-packages/torch/nn/modules/transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FOLD 3] ep 01 lr=2.00e-04 tr_loss=0.9938 va_loss=1.4582 f1=0.156 va_auc=0.262 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.398\n",
      "[FOLD 3] ep 02 lr=2.00e-04 tr_loss=0.9482 va_loss=1.4432 f1=0.156 va_auc=0.279 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.351\n",
      "[FOLD 3] ep 03 lr=2.00e-04 tr_loss=0.9236 va_loss=1.3967 f1=0.156 va_auc=0.279 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.356\n",
      "[FOLD 3] ep 04 lr=2.00e-04 tr_loss=0.9155 va_loss=1.4742 f1=0.156 va_auc=0.322 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.343\n",
      "[FOLD 3] ep 05 lr=2.00e-04 tr_loss=0.9102 va_loss=1.6462 f1=0.156 va_auc=0.315 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.331\n",
      "[FOLD 3] ep 06 lr=2.00e-04 tr_loss=0.9019 va_loss=1.7912 f1=0.156 va_auc=0.316 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.332\n",
      "[FOLD 3] ep 07 lr=2.00e-04 tr_loss=0.8808 va_loss=1.9586 f1=0.156 va_auc=0.310 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.347\n",
      "[FOLD 3] ep 08 lr=2.00e-04 tr_loss=0.8670 va_loss=1.9444 f1=0.156 va_auc=0.283 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.334\n",
      "[FOLD 3] ep 09 lr=2.00e-04 tr_loss=0.8650 va_loss=2.1355 f1=0.156 va_auc=0.335 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.373\n",
      "[FOLD 3] ep 10 lr=2.00e-04 tr_loss=0.8587 va_loss=1.9535 f1=0.156 va_auc=0.320 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.373\n",
      "[FOLD 3] ep 11 lr=2.00e-04 tr_loss=0.8480 va_loss=2.0506 f1=0.156 va_auc=0.334 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.392\n",
      "[FOLD 3] ep 12 lr=2.00e-04 tr_loss=0.8310 va_loss=2.0055 f1=0.156 va_auc=0.299 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.381\n",
      "[FOLD 3] ep 13 lr=2.00e-04 tr_loss=0.8193 va_loss=2.0094 f1=0.156 va_auc=0.271 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.377\n",
      "[FOLD 3] ep 14 lr=2.00e-04 tr_loss=0.8188 va_loss=2.0748 f1=0.156 va_auc=0.295 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.394\n",
      "[FOLD 3] ep 15 lr=2.00e-04 tr_loss=0.8305 va_loss=1.9874 f1=0.156 va_auc=0.335 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.388\n",
      "[FOLD 3] ep 16 lr=2.00e-04 tr_loss=0.8104 va_loss=2.0260 f1=0.156 va_auc=0.376 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.449\n",
      "[FOLD 3] ep 17 lr=2.00e-04 tr_loss=0.8054 va_loss=1.9901 f1=0.156 va_auc=0.322 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.383\n",
      "[FOLD 3] ep 18 lr=2.00e-04 tr_loss=0.8028 va_loss=2.2235 f1=0.156 va_auc=0.300 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.405\n",
      "[FOLD 3] ep 19 lr=2.00e-04 tr_loss=0.8087 va_loss=2.1285 f1=0.156 va_auc=0.285 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.409\n",
      "[FOLD 3] ep 20 lr=2.00e-04 tr_loss=0.7943 va_loss=2.1102 f1=0.156 va_auc=0.275 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.409\n",
      "[FOLD 3] ep 21 lr=2.00e-04 tr_loss=0.7860 va_loss=2.2427 f1=0.156 va_auc=0.243 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.400\n",
      "[FOLD 3] ep 22 lr=2.00e-04 tr_loss=0.7815 va_loss=2.3300 f1=0.156 va_auc=0.336 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.464\n",
      "[FOLD 3] ep 23 lr=2.00e-04 tr_loss=0.7853 va_loss=2.2030 f1=0.156 va_auc=0.336 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.445\n",
      "[FOLD 3] ep 24 lr=2.00e-04 tr_loss=0.7799 va_loss=2.2889 f1=0.156 va_auc=0.369 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.455\n",
      "[FOLD 3] ep 25 lr=2.00e-04 tr_loss=0.7527 va_loss=2.4245 f1=0.156 va_auc=0.323 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.459\n",
      "[FOLD 3] ep 26 lr=2.00e-04 tr_loss=0.7691 va_loss=2.1613 f1=0.156 va_auc=0.385 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.463\n",
      "[FOLD 3] ep 27 lr=2.00e-04 tr_loss=0.7885 va_loss=2.2724 f1=0.156 va_auc=0.329 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.437\n",
      "[FOLD 3] ep 28 lr=2.00e-04 tr_loss=0.7683 va_loss=2.1789 f1=0.156 va_auc=0.408 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.462\n",
      "[FOLD 3] ep 29 lr=2.00e-04 tr_loss=0.7647 va_loss=2.1933 f1=0.156 va_auc=0.398 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.466\n",
      "[FOLD 3] ep 30 lr=2.00e-04 tr_loss=0.7494 va_loss=2.1897 f1=0.156 va_auc=0.396 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.465\n",
      "[FOLD 3] TEST: loss=nan f1=0.148 va_auc=nan pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/recbole_new_env/lib/python3.11/site-packages/torch/nn/modules/transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FOLD 4] ep 01 lr=2.00e-04 tr_loss=1.0079 va_loss=nan f1=0.148 va_auc=nan pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=nan\n",
      "[FOLD 4] ep 02 lr=2.00e-04 tr_loss=0.9835 va_loss=nan f1=0.148 va_auc=nan pnl_sum=0.1393 pnl_mean=0.002247 pnl_std=0.008850 sharpe~=1.999 trades=62 thr=(0.5,0.5) auc_ovr3c=nan\n",
      "[FOLD 4] ep 03 lr=2.00e-04 tr_loss=0.9711 va_loss=nan f1=0.148 va_auc=nan pnl_sum=0.0743 pnl_mean=0.000722 pnl_std=0.009255 sharpe~=0.791 trades=103 thr=(0.55,0.5) auc_ovr3c=nan\n",
      "[FOLD 4] ep 04 lr=2.00e-04 tr_loss=0.9552 va_loss=nan f1=0.322 va_auc=nan pnl_sum=-0.0558 pnl_mean=-0.000473 pnl_std=0.009210 sharpe~=-0.557 trades=118 thr=(0.65,0.5) auc_ovr3c=nan\n",
      "[FOLD 4] ep 05 lr=2.00e-04 tr_loss=0.9426 va_loss=nan f1=0.288 va_auc=nan pnl_sum=0.2146 pnl_mean=0.004470 pnl_std=0.007295 sharpe~=4.245 trades=48 thr=(0.6,0.5) auc_ovr3c=nan\n",
      "[FOLD 4] ep 06 lr=2.00e-04 tr_loss=0.9188 va_loss=nan f1=0.214 va_auc=nan pnl_sum=0.2193 pnl_mean=0.004872 pnl_std=0.007145 sharpe~=4.575 trades=45 thr=(0.55,0.5) auc_ovr3c=nan\n",
      "[FOLD 4] ep 07 lr=2.00e-04 tr_loss=0.9255 va_loss=nan f1=0.293 va_auc=nan pnl_sum=0.2403 pnl_mean=0.004713 pnl_std=0.006790 sharpe~=4.957 trades=51 thr=(0.7,0.5) auc_ovr3c=nan\n",
      "[FOLD 4] ep 08 lr=2.00e-04 tr_loss=0.9231 va_loss=nan f1=0.300 va_auc=nan pnl_sum=0.0399 pnl_mean=0.001108 pnl_std=0.007462 sharpe~=0.891 trades=36 thr=(0.5,0.55) auc_ovr3c=nan\n",
      "[FOLD 4] ep 09 lr=2.00e-04 tr_loss=0.8935 va_loss=nan f1=0.226 va_auc=nan pnl_sum=0.1874 pnl_mean=0.003675 pnl_std=0.008171 sharpe~=3.212 trades=51 thr=(0.55,0.55) auc_ovr3c=nan\n",
      "[FOLD 4] ep 10 lr=2.00e-04 tr_loss=0.8952 va_loss=nan f1=0.245 va_auc=nan pnl_sum=0.2084 pnl_mean=0.003532 pnl_std=0.007809 sharpe~=3.474 trades=59 thr=(0.5,0.5) auc_ovr3c=nan\n",
      "[FOLD 4] ep 11 lr=2.00e-04 tr_loss=0.8839 va_loss=nan f1=0.295 va_auc=nan pnl_sum=0.0865 pnl_mean=0.000646 pnl_std=0.009017 sharpe~=0.829 trades=134 thr=(0.5,0.5) auc_ovr3c=nan\n",
      "[FOLD 4] ep 12 lr=2.00e-04 tr_loss=0.8773 va_loss=nan f1=0.365 va_auc=nan pnl_sum=0.2163 pnl_mean=0.003932 pnl_std=0.008253 sharpe~=3.533 trades=55 thr=(0.7,0.55) auc_ovr3c=nan\n",
      "[FOLD 4] ep 13 lr=2.00e-04 tr_loss=0.8686 va_loss=nan f1=0.268 va_auc=nan pnl_sum=0.1504 pnl_mean=0.004700 pnl_std=0.007658 sharpe~=3.471 trades=32 thr=(0.7,0.5) auc_ovr3c=nan\n",
      "[FOLD 4] ep 14 lr=2.00e-04 tr_loss=0.8700 va_loss=nan f1=0.270 va_auc=nan pnl_sum=0.1084 pnl_mean=0.002126 pnl_std=0.008608 sharpe~=1.764 trades=51 thr=(0.65,0.55) auc_ovr3c=nan\n",
      "[FOLD 4] ep 15 lr=2.00e-04 tr_loss=0.8616 va_loss=nan f1=0.262 va_auc=nan pnl_sum=0.0983 pnl_mean=0.002730 pnl_std=0.008710 sharpe~=1.881 trades=36 thr=(0.65,0.65) auc_ovr3c=nan\n",
      "[FOLD 4] ep 16 lr=2.00e-04 tr_loss=0.8548 va_loss=nan f1=0.269 va_auc=nan pnl_sum=0.0248 pnl_mean=0.000187 pnl_std=0.009083 sharpe~=0.237 trades=133 thr=(0.65,0.5) auc_ovr3c=nan\n",
      "[FOLD 4] ep 17 lr=2.00e-04 tr_loss=0.8340 va_loss=nan f1=0.349 va_auc=nan pnl_sum=0.0735 pnl_mean=0.001290 pnl_std=0.009234 sharpe~=1.055 trades=57 thr=(0.7,0.5) auc_ovr3c=nan\n",
      "[FOLD 4] ep 18 lr=2.00e-04 tr_loss=0.8453 va_loss=nan f1=0.290 va_auc=nan pnl_sum=0.0575 pnl_mean=0.000432 pnl_std=0.009069 sharpe~=0.550 trades=133 thr=(0.5,0.5) auc_ovr3c=nan\n",
      "[FOLD 4] ep 19 lr=2.00e-04 tr_loss=0.8417 va_loss=nan f1=0.349 va_auc=nan pnl_sum=0.0335 pnl_mean=0.000390 pnl_std=0.009315 sharpe~=0.388 trades=86 thr=(0.7,0.5) auc_ovr3c=nan\n",
      "[FOLD 4] ep 20 lr=2.00e-04 tr_loss=0.8298 va_loss=nan f1=0.306 va_auc=nan pnl_sum=0.1765 pnl_mean=0.001317 pnl_std=0.008928 sharpe~=1.708 trades=134 thr=(0.5,0.5) auc_ovr3c=nan\n",
      "[FOLD 4] ep 21 lr=2.00e-04 tr_loss=0.8257 va_loss=nan f1=0.260 va_auc=nan pnl_sum=0.0763 pnl_mean=0.001624 pnl_std=0.009054 sharpe~=1.230 trades=47 thr=(0.7,0.5) auc_ovr3c=nan\n",
      "[FOLD 4] ep 22 lr=2.00e-04 tr_loss=0.8221 va_loss=nan f1=0.292 va_auc=nan pnl_sum=0.1575 pnl_mean=0.001323 pnl_std=0.008827 sharpe~=1.636 trades=119 thr=(0.65,0.5) auc_ovr3c=nan\n",
      "[FOLD 4] ep 23 lr=2.00e-04 tr_loss=0.8282 va_loss=nan f1=0.267 va_auc=nan pnl_sum=0.1117 pnl_mean=0.000834 pnl_std=0.008998 sharpe~=1.073 trades=134 thr=(0.5,0.5) auc_ovr3c=nan\n",
      "[FOLD 4] ep 24 lr=2.00e-04 tr_loss=0.8141 va_loss=nan f1=0.239 va_auc=nan pnl_sum=0.1170 pnl_mean=0.003774 pnl_std=0.008290 sharpe~=2.534 trades=31 thr=(0.6,0.5) auc_ovr3c=nan\n",
      "[FOLD 4] ep 25 lr=2.00e-04 tr_loss=0.8090 va_loss=nan f1=0.227 va_auc=nan pnl_sum=0.0470 pnl_mean=0.000484 pnl_std=0.008816 sharpe~=0.541 trades=97 thr=(0.5,0.5) auc_ovr3c=nan\n",
      "[FOLD 4] ep 26 lr=2.00e-04 tr_loss=0.8114 va_loss=nan f1=0.251 va_auc=nan pnl_sum=0.1308 pnl_mean=0.001258 pnl_std=0.008975 sharpe~=1.430 trades=104 thr=(0.55,0.65) auc_ovr3c=nan\n",
      "[FOLD 4] ep 27 lr=2.00e-04 tr_loss=0.8144 va_loss=nan f1=0.303 va_auc=nan pnl_sum=0.1780 pnl_mean=0.001413 pnl_std=0.008866 sharpe~=1.788 trades=126 thr=(0.65,0.5) auc_ovr3c=nan\n",
      "[FOLD 4] ep 28 lr=2.00e-04 tr_loss=0.8039 va_loss=nan f1=0.243 va_auc=nan pnl_sum=0.0548 pnl_mean=0.000559 pnl_std=0.008906 sharpe~=0.621 trades=98 thr=(0.6,0.6) auc_ovr3c=nan\n",
      "[FOLD 4] ep 29 lr=2.00e-04 tr_loss=0.7947 va_loss=nan f1=0.259 va_auc=nan pnl_sum=0.1869 pnl_mean=0.001507 pnl_std=0.008842 sharpe~=1.898 trades=124 thr=(0.5,0.5) auc_ovr3c=nan\n",
      "[FOLD 4] ep 30 lr=2.00e-04 tr_loss=0.7990 va_loss=nan f1=0.288 va_auc=nan pnl_sum=0.1582 pnl_mean=0.001208 pnl_std=0.008902 sharpe~=1.553 trades=131 thr=(0.5,0.5) auc_ovr3c=nan\n",
      "[FOLD 4] TEST: loss=1.1685 f1=0.186 va_auc=0.440 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0 thr=(None,None) auc_ovr3c=0.535\n",
      "\n",
      "=== SUMMARY (per fold test) ===\n",
      "fold 1: f1=0.236 va_auc=0.470 auc_ovr3c=0.458 pnl_sum=-0.1325 pnl_mean=-0.001720 pnl_std=0.003106 sharpe~=-4.861 trades=77\n",
      "fold 2: f1=0.210 va_auc=0.562 auc_ovr3c=0.391 pnl_sum=-0.0496 pnl_mean=-0.000774 pnl_std=0.008076 sharpe~=-0.767 trades=64\n",
      "fold 3: f1=0.148 va_auc=nan auc_ovr3c=nan pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0\n",
      "fold 4: f1=0.186 va_auc=0.440 auc_ovr3c=0.535 pnl_sum=nan pnl_mean=nan pnl_std=nan sharpe~=nan trades=0\n"
     ]
    }
   ],
   "source": [
    "# ==================================\n",
    "# Step 12 (REPLACE v2). Walk-forward CV training (3-class) + extended summary\n",
    "# ==================================\n",
    "# ЛОГИЧЕСКИЙ БЛОК: прогон по фолдам + summary\n",
    "# ИСПОЛНЕНИЕ БЛОКА:\n",
    "\n",
    "results = []\n",
    "for fi, (idx_tr, idx_va, idx_te) in enumerate(walk_splits, start=1):\n",
    "    dl_tr, dl_va, dl_te, _ = make_loaders_for_fold(\n",
    "        X_node_raw=X_node_raw,\n",
    "        edge_feat=edge_feat,\n",
    "        sample_t=sample_t_cv,\n",
    "        y_tb=y_tb,\n",
    "        y_trade=y_trade,\n",
    "        y_dir=y_dir,\n",
    "        exit_ret=exit_ret,\n",
    "        idx_train=idx_tr,\n",
    "        idx_val=idx_va,\n",
    "        idx_test=idx_te,\n",
    "        batch_size=CFG[\"batch_size\"],\n",
    "    )\n",
    "    out = run_fold_training_3c(fi, dl_tr, dl_va, dl_te)\n",
    "    results.append(out)\n",
    "\n",
    "print(\"\\n=== SUMMARY (per fold test) ===\")\n",
    "for i, r in enumerate(results, start=1):\n",
    "    te = r[\"test\"]\n",
    "    print(f\"fold {i}: f1={te['f1m_3c']:.3f} va_auc={te['va_auc']:.3f} auc_ovr3c={te['auc_ovr_3c']:.3f} \"\n",
    "          f\"pnl_sum={te['pnl_best_sum']:.4f} pnl_mean={te['pnl_best_mean']:.6f} \"\n",
    "          f\"pnl_std={te['pnl_best_std']:.6f} sharpe~={te['pnl_best_sharpe']:.3f} \"\n",
    "          f\"trades={te['pnl_best_trades']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recbole_new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
