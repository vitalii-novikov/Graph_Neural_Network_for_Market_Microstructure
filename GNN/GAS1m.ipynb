{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Two-stage LOB GNN (SGA-TCN) — VS Code / Jupyter compatible (.py)\n",
    "\n",
    " Формат: логические шаги через `# %%`.\n",
    "\n",
    " Важные принципы:\n",
    "\n",
    " - Scaling только по train (time-ordered, без leakage)\n",
    "\n",
    " - Stage A: trade/no-trade (AUC)\n",
    "\n",
    " - Stage B: direction на trade-only (AUC на trade-only)\n",
    "\n",
    " - Пороги (thr_trade, thr_dir) выбираем ТОЛЬКО на val, на holdout НЕ подбираем\n",
    "\n",
    "\n",
    "\n",
    " Рекомендовано для Mac M2 (CPU): batch_size=64..128, hidden=64..128, epochs=15..25, AMP отключён на CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Step 0 — Imports + reproducibility + config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cpu\n",
      "EDGE_LIST: ['ADA->BTC', 'ADA->ETH', 'BTC->ADA', 'BTC->ETH', 'ETH->ADA', 'ETH->BTC', 'ADA->ADA', 'BTC->BTC', 'ETH->ETH']\n",
      "EDGE_INDEX: [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1], [0, 0], [1, 1], [2, 2]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, roc_auc_score\n",
    "\n",
    "\n",
    "def seed_everything(seed: int = 1234) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "seed_everything(100)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "\n",
    "torch.set_num_threads(max(1, os.cpu_count() or 4))\n",
    "\n",
    "CFG: Dict = {\n",
    "    # data\n",
    "    \"freq\": \"1min\",\n",
    "    \"data_dir\": Path(\"../dataset\"),\n",
    "    \"final_test_frac\": 0.10,\n",
    "\n",
    "    # order book\n",
    "    \"book_levels\": 15,\n",
    "    \"top_levels\": 5,\n",
    "    \"near_levels\": 5,\n",
    "\n",
    "    # walk-forward windows (in sample-space)\n",
    "    \"train_min_frac\": 0.50,\n",
    "    \"val_window_frac\": 0.10,\n",
    "    \"test_window_frac\": 0.10,\n",
    "    \"step_window_frac\": 0.10,\n",
    "\n",
    "    # scaling\n",
    "    \"max_abs_feat\": 10.0,\n",
    "    \"max_abs_edge\": 6.0,          # <-- новое: клип для edge after scaling\n",
    "\n",
    "    # correlations / graph\n",
    "    \"corr_windows\": [6 * 5, 12 * 5, 24 * 5, 48 * 5, 84 * 5],  # 30m,1h,2h,4h,7h\n",
    "    \"corr_lags\": [0, 1, 2, 5],     # <-- новое: lead-lag (без leakage)\n",
    "    \"edges_mode\": \"all_pairs\",     # \"manual\" | \"all_pairs\"\n",
    "    \"edges\": [(\"ADA\", \"BTC\"), (\"ADA\", \"ETH\"), (\"ETH\", \"BTC\")],  # используется если edges_mode=\"manual\"\n",
    "    \"add_self_loops\": True,        # <-- новое\n",
    "    \"edge_transform\": \"fisher\",    # \"none\" | \"fisher\"\n",
    "    \"edge_scale\": True,            # <-- новое: robust-scale edge features per fold\n",
    "    \"edge_dropout\": 0.10,          # <-- новое: regularization\n",
    "\n",
    "    # triple-barrier\n",
    "    \"tb_horizon\": 1 * 15,\n",
    "    \"lookback\": 2 * 12 * 5,\n",
    "    \"tb_pt_mult\": 1.2,\n",
    "    \"tb_sl_mult\": 1.1,\n",
    "    \"tb_min_barrier\": 0.001,\n",
    "    \"tb_max_barrier\": 0.006,\n",
    "\n",
    "    # training\n",
    "    \"batch_size\": 64,\n",
    "    \"epochs\": 20,\n",
    "    \"lr\": 3e-4,                    # <-- чуть выше часто лучше с OneCycle\n",
    "    \"weight_decay\": 5e-4,\n",
    "    \"grad_clip\": 1.0,\n",
    "    \"dropout\": 0.20,\n",
    "\n",
    "    # NEW: stability tricks\n",
    "    \"label_smoothing\": 0.02,\n",
    "    \"use_weighted_sampler\": True,\n",
    "    \"use_onecycle\": True,\n",
    "\n",
    "    # model\n",
    "    \"hidden\": 96,\n",
    "    \"gnn_layers\": 3,\n",
    "\n",
    "    # --- SGA (spatial)\n",
    "    \"gat_heads\": 2,\n",
    "\n",
    "    # --- TCN (temporal)\n",
    "    \"tcn_channels\": 96,\n",
    "    \"tcn_layers\": 3,\n",
    "    \"tcn_kernel\": 2,\n",
    "    \"tcn_dropout\": 0.20,\n",
    "    \"tcn_causal\": True,\n",
    "    \"tcn_pool\": \"last\",\n",
    "\n",
    "    # trading eval\n",
    "    \"cost_bps\": 1.0,\n",
    "\n",
    "    # threshold sweep grids (val only)\n",
    "    \"thr_trade_grid\": [0.35, 0.40, 0.45, 0.50, 0.55, 0.60, 0.65, 0.70, 0.75],\n",
    "    \"thr_dir_grid\":   [0.50, 0.55, 0.60, 0.65, 0.70],\n",
    "\n",
    "    # min trades constraints\n",
    "    \"eval_min_trades\": 50,\n",
    "\n",
    "    # NEW: anti-overtrading threshold selection\n",
    "    \"max_trade_rate_val\": 0.65,        # <-- ключевая правка против fold2/holdout 0.9+\n",
    "    \"trade_rate_penalty\": 0.20,        # штраф за отклонение от target_trade_rate\n",
    "    \"thr_objective\": \"pnl_sum\",        # \"pnl_sum\" | \"pnl_sharpe\" | \"pnl_per_trade\"\n",
    "\n",
    "    # динамические квантильные пороги для thr_trade\n",
    "    \"proxy_target_trades\": [50, 100, 200],\n",
    "}\n",
    "\n",
    "ASSETS = [\"ADA\", \"BTC\", \"ETH\"]\n",
    "ASSET2IDX = {a: i for i, a in enumerate(ASSETS)}\n",
    "TARGET_ASSET = \"ETH\"\n",
    "TARGET_NODE = ASSET2IDX[TARGET_ASSET]\n",
    "\n",
    "\n",
    "def build_edge_list(cfg: Dict, assets: List[str]) -> List[Tuple[str, str]]:\n",
    "    mode = str(cfg.get(\"edges_mode\", \"manual\"))\n",
    "    if mode == \"manual\":\n",
    "        edges = list(cfg[\"edges\"])\n",
    "    elif mode == \"all_pairs\":\n",
    "        edges = [(s, t) for s in assets for t in assets if s != t]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown edges_mode={mode}\")\n",
    "\n",
    "    if bool(cfg.get(\"add_self_loops\", True)):\n",
    "        edges = edges + [(a, a) for a in assets]\n",
    "    return edges\n",
    "\n",
    "\n",
    "EDGE_LIST = build_edge_list(CFG, ASSETS)\n",
    "EDGE_NAMES = [f\"{s}->{t}\" for s, t in EDGE_LIST]\n",
    "EDGE_INDEX = torch.tensor([[ASSET2IDX[s], ASSET2IDX[t]] for (s, t) in EDGE_LIST], dtype=torch.long)\n",
    "\n",
    "print(\"EDGE_LIST:\", EDGE_NAMES)\n",
    "print(\"EDGE_INDEX:\", EDGE_INDEX.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Step 1 — Load data + log returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded df: (13687, 106)\n",
      "Columns example: ['timestamp', 'ADA', 'spread_ADA', 'buys_ADA', 'sells_ADA', 'bids_vol_ADA_0', 'bids_vol_ADA_1', 'bids_vol_ADA_2', 'bids_vol_ADA_3', 'bids_vol_ADA_4', 'bids_vol_ADA_5', 'bids_vol_ADA_6', 'bids_vol_ADA_7', 'bids_vol_ADA_8', 'bids_vol_ADA_9', 'bids_vol_ADA_10', 'bids_vol_ADA_11', 'bids_vol_ADA_12', 'bids_vol_ADA_13', 'bids_vol_ADA_14']\n",
      "Time range: 2021-04-07 11:34:00+00:00 -> 2021-04-17 00:34:00+00:00\n",
      "                  timestamp      ADA  spread_ADA      buys_ADA      sells_ADA  \\\n",
      "0 2021-04-07 11:34:00+00:00  1.16205      0.0001  56936.467913  258248.957367   \n",
      "1 2021-04-07 11:35:00+00:00  1.16800      0.0022  56491.336799   78665.286640   \n",
      "\n",
      "   bids_vol_ADA_0  bids_vol_ADA_1  bids_vol_ADA_2  bids_vol_ADA_3  \\\n",
      "0      876.869995     5984.169922        5.810000       18.240000   \n",
      "1    33769.671875    23137.169922      550.299988      550.299988   \n",
      "\n",
      "   bids_vol_ADA_4  ...  asks_vol_ETH_8  asks_vol_ETH_9  asks_vol_ETH_10  \\\n",
      "0    19844.640625  ...      373.700012      196.699997      2059.709961   \n",
      "1    19012.320312  ...     3873.709961     1954.630005       197.039993   \n",
      "\n",
      "   asks_vol_ETH_11  asks_vol_ETH_12  asks_vol_ETH_13  asks_vol_ETH_14  \\\n",
      "0      3874.989990      5901.209961       178.289993     28512.160156   \n",
      "1     12661.990234     20006.970703     28562.310547      3874.379883   \n",
      "\n",
      "     lr_ADA    lr_BTC    lr_ETH  \n",
      "0  0.000000  0.000000  0.000000  \n",
      "1  0.005107  0.000937  0.001931  \n",
      "\n",
      "[2 rows x 106 columns]\n"
     ]
    }
   ],
   "source": [
    "def load_asset(asset: str, freq: str, data_dir: Path, book_levels: int, part: Tuple[int, int] = (0, 80)) -> pd.DataFrame:\n",
    "    path = data_dir / f\"{asset}_{freq}.csv\"\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.iloc[int(len(df) * part[0] / 100): int(len(df) * part[1] / 100)]\n",
    "\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"system_time\"]).dt.round(\"min\")\n",
    "    df = df.sort_values(\"timestamp\").set_index(\"timestamp\")\n",
    "\n",
    "    bid_cols = [f\"bids_notional_{i}\" for i in range(book_levels)]\n",
    "    ask_cols = [f\"asks_notional_{i}\" for i in range(book_levels)]\n",
    "\n",
    "    needed = [\"midpoint\", \"spread\", \"buys\", \"sells\"] + bid_cols + ask_cols\n",
    "    missing = [c for c in needed if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"{asset}: missing columns in CSV: {missing[:10]}{'...' if len(missing) > 10 else ''}\")\n",
    "\n",
    "    return df[needed]\n",
    "\n",
    "\n",
    "def load_all_assets() -> pd.DataFrame:\n",
    "    freq = CFG[\"freq\"]\n",
    "    data_dir = CFG[\"data_dir\"]\n",
    "    book_levels = CFG[\"book_levels\"]\n",
    "\n",
    "    def rename_cols(df_one: pd.DataFrame, asset: str) -> pd.DataFrame:\n",
    "        rename_map = {\n",
    "            \"midpoint\": asset,\n",
    "            \"buys\": f\"buys_{asset}\",\n",
    "            \"sells\": f\"sells_{asset}\",\n",
    "            \"spread\": f\"spread_{asset}\",\n",
    "        }\n",
    "        for i in range(book_levels):\n",
    "            rename_map[f\"bids_notional_{i}\"] = f\"bids_vol_{asset}_{i}\"\n",
    "            rename_map[f\"asks_notional_{i}\"] = f\"asks_vol_{asset}_{i}\"\n",
    "        return df_one.rename(columns=rename_map)\n",
    "\n",
    "    df_ada = rename_cols(load_asset(\"ADA\", freq, data_dir, book_levels, part=(0, 80)), \"ADA\")\n",
    "    df_btc = rename_cols(load_asset(\"BTC\", freq, data_dir, book_levels, part=(0, 80)), \"BTC\")\n",
    "    df_eth = rename_cols(load_asset(\"ETH\", freq, data_dir, book_levels, part=(0, 80)), \"ETH\")\n",
    "\n",
    "    df = df_ada.join(df_btc).join(df_eth).reset_index()\n",
    "    return df\n",
    "\n",
    "\n",
    "df = load_all_assets()\n",
    "for a in ASSETS:\n",
    "    df[f\"lr_{a}\"] = np.log(df[a]).diff().fillna(0.0)\n",
    "\n",
    "print(\"Loaded df:\", df.shape)\n",
    "print(\"Columns example:\", df.columns[:20].tolist())\n",
    "print(\"Time range:\", df[\"timestamp\"].min(), \"->\", df[\"timestamp\"].max())\n",
    "print(df.head(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Step 2 — Multi-window correlations → edge features (T,E,W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edge_feat shape: (13687, 9, 20) (T,E,edge_dim)\n",
      "edge_dim = 20  = windows * lags = 20\n",
      "Edge names: ['ADA->BTC', 'ADA->ETH', 'BTC->ADA', 'BTC->ETH', 'ETH->ADA', 'ETH->BTC', 'ADA->ADA', 'BTC->BTC', 'ETH->ETH']\n",
      "edge_feat sample [t=100, first 3 edges]:\n",
      " [[ 6.7925054e-01  7.8185719e-01  8.3443433e-01  8.3443433e-01\n",
      "   8.3443433e-01  3.4026209e-01  1.4996846e-01  1.6863135e-01\n",
      "   1.6863135e-01  1.6863135e-01 -2.9266590e-02 -1.5999632e-01\n",
      "  -2.5908518e-01 -2.5908518e-01 -2.5908518e-01  1.4277337e-01\n",
      "  -1.0870378e-02  5.0888385e-04  5.0888385e-04  5.0888385e-04]\n",
      " [ 6.3383293e-01  6.9067067e-01  8.7368768e-01  8.7368768e-01\n",
      "   8.7368768e-01  3.4575835e-01  1.5627505e-01  2.0534241e-01\n",
      "   2.0534241e-01  2.0534241e-01  6.9384493e-02 -1.1163518e-01\n",
      "  -1.7551416e-01 -1.7551416e-01 -1.7551416e-01 -1.6881377e-01\n",
      "  -1.0781832e-01 -6.3380465e-02 -6.3380465e-02 -6.3380465e-02]\n",
      " [ 6.7925054e-01  7.8185719e-01  8.3443433e-01  8.3443433e-01\n",
      "   8.3443433e-01 -4.8168253e-02 -1.6241662e-01 -1.6193953e-01\n",
      "  -1.6193953e-01 -1.6193953e-01 -2.1278685e-01  4.6374347e-02\n",
      "  -1.7361922e-02 -1.7361922e-02 -1.7361922e-02  1.3684873e-01\n",
      "   3.6141057e-02  6.9459870e-02  6.9459870e-02  6.9459870e-02]]\n",
      "edge_feat stats: mean= 0.4536975026130676 std= 0.5007497668266296\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "def _fisher_z(x: np.ndarray, eps: float = 1e-6) -> np.ndarray:\n",
    "    # clip to avoid inf\n",
    "    x = np.clip(x, -0.999, 0.999)\n",
    "    return 0.5 * np.log((1.0 + x + eps) / (1.0 - x + eps))\n",
    "\n",
    "\n",
    "def build_corr_array(\n",
    "    df_: pd.DataFrame,\n",
    "    corr_windows: List[int],\n",
    "    edges: List[Tuple[str, str]],\n",
    "    lags: List[int],\n",
    "    transform: str = \"fisher\",\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    edge features per time:\n",
    "      for edge s->t:\n",
    "        for lag in lags:\n",
    "          corr( lr_s.shift(lag), lr_t ) over rolling window\n",
    "    No leakage: shift(lag>0) uses PAST of source.\n",
    "    Self-loop edges a->a: set constant 1.0 (model can learn its meaning).\n",
    "    \"\"\"\n",
    "    T_ = len(df_)\n",
    "    E_ = len(edges)\n",
    "    W_ = len(corr_windows)\n",
    "    Lg = len(lags)\n",
    "    out = np.zeros((T_, E_, W_ * Lg), dtype=np.float32)\n",
    "\n",
    "    lr_map = {a: df_[f\"lr_{a}\"].astype(float) for a in ASSETS}\n",
    "\n",
    "    for ei, (s, t) in enumerate(edges):\n",
    "        if s == t:\n",
    "            out[:, ei, :] = 1.0\n",
    "            continue\n",
    "\n",
    "        src0 = lr_map[s]\n",
    "        dst0 = lr_map[t]\n",
    "\n",
    "        feat_idx = 0\n",
    "        for lag in lags:\n",
    "            src = src0.shift(int(lag)) if int(lag) > 0 else src0\n",
    "\n",
    "            for w in corr_windows:\n",
    "                r = src.rolling(int(w), min_periods=1).corr(dst0)\n",
    "                r = np.nan_to_num(r.to_numpy(dtype=np.float32), nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "                if transform == \"fisher\":\n",
    "                    r = _fisher_z(r).astype(np.float32)\n",
    "\n",
    "                out[:, ei, feat_idx] = r\n",
    "                feat_idx += 1\n",
    "\n",
    "    return out.astype(np.float32)\n",
    "\n",
    "\n",
    "edge_feat = build_corr_array(\n",
    "    df,\n",
    "    CFG[\"corr_windows\"],\n",
    "    EDGE_LIST,\n",
    "    CFG[\"corr_lags\"],\n",
    "    transform=str(CFG.get(\"edge_transform\", \"fisher\")),\n",
    ")\n",
    "\n",
    "print(\"edge_feat shape:\", edge_feat.shape, \"(T,E,edge_dim)\")\n",
    "print(\"edge_dim =\", edge_feat.shape[-1], \" = windows * lags =\", len(CFG[\"corr_windows\"]) * len(CFG[\"corr_lags\"]))\n",
    "print(\"Edge names:\", EDGE_NAMES)\n",
    "print(\"edge_feat sample [t=100, first 3 edges]:\\n\", edge_feat[100, :3, :])\n",
    "print(\"edge_feat stats: mean=\", float(edge_feat.mean()), \"std=\", float(edge_feat.std()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Step 3 — Triple-barrier labels → two-stage labels + exit_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TB dist [down,flat,up]: [2684 8683 2320]\n",
      "Trade ratio (true): 0.36560239643457293\n"
     ]
    }
   ],
   "source": [
    "def triple_barrier_labels_from_lr(\n",
    "    lr: pd.Series,\n",
    "    horizon: int,\n",
    "    vol_window: int,\n",
    "    pt_mult: float,\n",
    "    sl_mult: float,\n",
    "    min_barrier: float,\n",
    "    max_barrier: float,\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      y_tb: {0=down, 1=flat/no-trade, 2=up}\n",
    "      exit_ret: realized log-return to exit (tp/sl/timeout)\n",
    "      exit_t: exit index\n",
    "      thr: barrier per t (float array, len T)\n",
    "    No leakage: vol is shift(1).\n",
    "    \"\"\"\n",
    "    lr = lr.astype(float).copy()\n",
    "    T = len(lr)\n",
    "\n",
    "    vol = lr.rolling(vol_window, min_periods=max(10, vol_window // 10)).std().shift(1)\n",
    "    thr = (vol * np.sqrt(horizon)).clip(lower=min_barrier, upper=max_barrier)\n",
    "\n",
    "    y = np.ones(T, dtype=np.int64)\n",
    "    exit_ret = np.zeros(T, dtype=np.float32)\n",
    "    exit_t = np.arange(T, dtype=np.int64)\n",
    "\n",
    "    lr_np = lr.fillna(0.0).to_numpy(dtype=np.float64)\n",
    "    thr_np = thr.fillna(min_barrier).to_numpy(dtype=np.float64)\n",
    "\n",
    "    for t in range(T - horizon - 1):\n",
    "        up = pt_mult * thr_np[t]\n",
    "        dn = -sl_mult * thr_np[t]\n",
    "\n",
    "        cum = 0.0\n",
    "        hit = 1\n",
    "        et = t + horizon\n",
    "        er = 0.0\n",
    "\n",
    "        for dt in range(1, horizon + 1):\n",
    "            cum += lr_np[t + dt]\n",
    "            if cum >= up:\n",
    "                hit, et, er = 2, t + dt, cum\n",
    "                break\n",
    "            if cum <= dn:\n",
    "                hit, et, er = 0, t + dt, cum\n",
    "                break\n",
    "\n",
    "        if hit == 1:\n",
    "            er = float(np.sum(lr_np[t + 1: t + horizon + 1]))\n",
    "            et = t + horizon\n",
    "\n",
    "        y[t] = hit\n",
    "        exit_ret[t] = er\n",
    "        exit_t[t] = et\n",
    "\n",
    "    return y, exit_ret, exit_t, thr_np\n",
    "\n",
    "\n",
    "y_tb, exit_ret, exit_t, tb_thr = triple_barrier_labels_from_lr(\n",
    "    df[\"lr_ETH\"],\n",
    "    horizon=CFG[\"tb_horizon\"],\n",
    "    vol_window=CFG[\"lookback\"],\n",
    "    pt_mult=CFG[\"tb_pt_mult\"],\n",
    "    sl_mult=CFG[\"tb_sl_mult\"],\n",
    "    min_barrier=CFG[\"tb_min_barrier\"],\n",
    "    max_barrier=CFG[\"tb_max_barrier\"],\n",
    ")\n",
    "\n",
    "# two-stage labels\n",
    "y_trade = (y_tb != 1).astype(np.int64)   # 1=trade, 0=no-trade\n",
    "y_dir = (y_tb == 2).astype(np.int64)     # 1=up, 0=down (meaningful only when y_trade==1)\n",
    "\n",
    "dist = np.bincount(y_tb, minlength=3)\n",
    "print(\"TB dist [down,flat,up]:\", dist)\n",
    "print(\"Trade ratio (true):\", float(y_trade.mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Step 4 — Build node tensor (T,N,F) + sample_t (valid indices in sample-space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_node_raw: (13687, 3, 15) edge_feat: (13687, 9, 20)\n",
      "node_feat_names: ['lr', 'spread', 'log_buys', 'log_sells', 'ofi', 'DI_15', 'DI_L0', 'DI_L1', 'DI_L2', 'DI_L3', 'DI_L4', 'near_ratio_bid', 'near_ratio_ask', 'di_near', 'di_far']\n",
      "n_samples: 13552 | t range: 119 -> 13670\n",
      "Feature stats (TARGET asset, lr): mean= 1.6185271306312643e-05 std= 0.0010841602925211191\n"
     ]
    }
   ],
   "source": [
    "EPS = 1e-6\n",
    "\n",
    "\n",
    "def safe_log1p(x: np.ndarray) -> np.ndarray:\n",
    "    return np.log1p(np.maximum(x, 0.0))\n",
    "\n",
    "\n",
    "def build_node_tensor(df_: pd.DataFrame) -> Tuple[np.ndarray, List[str]]:\n",
    "    \"\"\"\n",
    "    Features per asset:\n",
    "      lr, spread,\n",
    "      log_buys, log_sells, ofi,\n",
    "      DI_15,\n",
    "      DI_L0..DI_L4,\n",
    "      near_ratio_bid, near_ratio_ask,\n",
    "      di_near, di_far\n",
    "    \"\"\"\n",
    "    book_levels = CFG[\"book_levels\"]\n",
    "    top_k = CFG[\"top_levels\"]\n",
    "    near_k = CFG[\"near_levels\"]\n",
    "\n",
    "    if near_k >= book_levels:\n",
    "        raise ValueError(\"CFG['near_levels'] must be < CFG['book_levels']\")\n",
    "\n",
    "    feat_names = [\n",
    "        \"lr\", \"spread\",\n",
    "        \"log_buys\", \"log_sells\", \"ofi\",\n",
    "        \"DI_15\",\n",
    "        \"DI_L0\", \"DI_L1\", \"DI_L2\", \"DI_L3\", \"DI_L4\",\n",
    "        \"near_ratio_bid\", \"near_ratio_ask\",\n",
    "        \"di_near\", \"di_far\",\n",
    "    ]\n",
    "\n",
    "    feats_all = []\n",
    "    for a in ASSETS:\n",
    "        lr = df_[f\"lr_{a}\"].values.astype(np.float32)\n",
    "        spread = df_[f\"spread_{a}\"].values.astype(np.float32)\n",
    "\n",
    "        buys = df_[f\"buys_{a}\"].values.astype(np.float32)\n",
    "        sells = df_[f\"sells_{a}\"].values.astype(np.float32)\n",
    "\n",
    "        log_buys = safe_log1p(buys).astype(np.float32)\n",
    "        log_sells = safe_log1p(sells).astype(np.float32)\n",
    "\n",
    "        ofi = ((buys - sells) / (buys + sells + EPS)).astype(np.float32)\n",
    "\n",
    "        bids_lvls = np.stack([df_[f\"bids_vol_{a}_{i}\"].values.astype(np.float32) for i in range(book_levels)], axis=1)\n",
    "        asks_lvls = np.stack([df_[f\"asks_vol_{a}_{i}\"].values.astype(np.float32) for i in range(book_levels)], axis=1)\n",
    "\n",
    "        bid_sum = bids_lvls.sum(axis=1)\n",
    "        ask_sum = asks_lvls.sum(axis=1)\n",
    "        di_15 = ((bid_sum - ask_sum) / (bid_sum + ask_sum + EPS)).astype(np.float32)\n",
    "\n",
    "        di_levels = []\n",
    "        for i in range(top_k):\n",
    "            b = bids_lvls[:, i]\n",
    "            s = asks_lvls[:, i]\n",
    "            di_levels.append(((b - s) / (b + s + EPS)).astype(np.float32))\n",
    "        di_l0_4 = np.stack(di_levels, axis=1)  # (T,5)\n",
    "\n",
    "        bid_near = bids_lvls[:, :near_k].sum(axis=1)\n",
    "        ask_near = asks_lvls[:, :near_k].sum(axis=1)\n",
    "        bid_far = bids_lvls[:, near_k:].sum(axis=1)\n",
    "        ask_far = asks_lvls[:, near_k:].sum(axis=1)\n",
    "\n",
    "        near_ratio_bid = (bid_near / (bid_far + EPS)).astype(np.float32)\n",
    "        near_ratio_ask = (ask_near / (ask_far + EPS)).astype(np.float32)\n",
    "\n",
    "        di_near = ((bid_near - ask_near) / (bid_near + ask_near + EPS)).astype(np.float32)\n",
    "        di_far = ((bid_far - ask_far) / (bid_far + ask_far + EPS)).astype(np.float32)\n",
    "\n",
    "        Xa = np.column_stack([\n",
    "            lr, spread,\n",
    "            log_buys, log_sells, ofi,\n",
    "            di_15,\n",
    "            di_l0_4[:, 0], di_l0_4[:, 1], di_l0_4[:, 2], di_l0_4[:, 3], di_l0_4[:, 4],\n",
    "            near_ratio_bid, near_ratio_ask,\n",
    "            di_near, di_far,\n",
    "        ]).astype(np.float32)\n",
    "\n",
    "        feats_all.append(Xa)\n",
    "\n",
    "    X = np.stack(feats_all, axis=1).astype(np.float32)  # (T,N,F)\n",
    "    return X, feat_names\n",
    "\n",
    "\n",
    "X_node_raw, node_feat_names = build_node_tensor(df)\n",
    "T = len(df)\n",
    "L = CFG[\"lookback\"]\n",
    "H = CFG[\"tb_horizon\"]\n",
    "\n",
    "t_min = L - 1\n",
    "t_max = T - H - 2\n",
    "sample_t = np.arange(t_min, t_max + 1)\n",
    "n_samples = len(sample_t)\n",
    "\n",
    "print(\"X_node_raw:\", X_node_raw.shape, \"edge_feat:\", edge_feat.shape)\n",
    "print(\"node_feat_names:\", node_feat_names)\n",
    "print(\"n_samples:\", n_samples, \"| t range:\", int(sample_t[0]), \"->\", int(sample_t[-1]))\n",
    "\n",
    "# quick feature sanity\n",
    "print(\"Feature stats (TARGET asset, lr):\",\n",
    "      \"mean=\", float(X_node_raw[:, TARGET_NODE, node_feat_names.index(\"lr\")].mean()),\n",
    "      \"std=\", float(X_node_raw[:, TARGET_NODE, node_feat_names.index(\"lr\")].std()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Step 5 — Final holdout split (time-ordered) + walk-forward splits (CV-part only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holdout split:\n",
      "  n_samples total: 13552\n",
      "  n_samples CV   : 12197 (90.0%)\n",
      "  n_samples FINAL: 1355 (10.0%)\n",
      "  CV range   : 0 12196\n",
      "  FINAL range: 12197 13551\n",
      "\n",
      "Walk-forward folds: 4\n",
      "  fold 1: train=6098 | val=1219 | test=1219\n",
      "  fold 2: train=7317 | val=1219 | test=1219\n",
      "  fold 3: train=8536 | val=1219 | test=1219\n",
      "  fold 4: train=9755 | val=1219 | test=1219\n"
     ]
    }
   ],
   "source": [
    "def make_final_holdout_split(n_samples_: int, final_test_frac: float) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    if not (0.0 < final_test_frac < 0.5):\n",
    "        raise ValueError(\"final_test_frac should be in (0, 0.5)\")\n",
    "    n_final = max(1, int(round(final_test_frac * n_samples_)))\n",
    "    n_cv = n_samples_ - n_final\n",
    "    if n_cv <= 50:\n",
    "        raise ValueError(\"Too few samples left for CV after holdout split.\")\n",
    "    idx_cv = np.arange(0, n_cv, dtype=np.int64)\n",
    "    idx_final = np.arange(n_cv, n_samples_, dtype=np.int64)\n",
    "    return idx_cv, idx_final\n",
    "\n",
    "\n",
    "def make_walk_forward_splits(\n",
    "    n_samples_: int,\n",
    "    train_min_frac: float,\n",
    "    val_window_frac: float,\n",
    "    test_window_frac: float,\n",
    "    step_window_frac: float,\n",
    ") -> List[Tuple[np.ndarray, np.ndarray, np.ndarray]]:\n",
    "    train_min = int(train_min_frac * n_samples_)\n",
    "    val_w = max(1, int(val_window_frac * n_samples_))\n",
    "    test_w = max(1, int(test_window_frac * n_samples_))\n",
    "    step_w = max(1, int(step_window_frac * n_samples_))\n",
    "\n",
    "    splits = []\n",
    "    start = train_min\n",
    "    while True:\n",
    "        tr_end = start\n",
    "        va_end = tr_end + val_w\n",
    "        te_end = va_end + test_w\n",
    "        if te_end > n_samples_:\n",
    "            break\n",
    "\n",
    "        idx_train = np.arange(0, tr_end, dtype=np.int64)\n",
    "        idx_val = np.arange(tr_end, va_end, dtype=np.int64)\n",
    "        idx_test = np.arange(va_end, te_end, dtype=np.int64)\n",
    "        splits.append((idx_train, idx_val, idx_test))\n",
    "\n",
    "        start += step_w\n",
    "\n",
    "    return splits\n",
    "\n",
    "\n",
    "idx_cv_all, idx_final_test = make_final_holdout_split(n_samples, CFG[\"final_test_frac\"])\n",
    "n_samples_cv = len(idx_cv_all)\n",
    "n_samples_final = len(idx_final_test)\n",
    "\n",
    "print(\"Holdout split:\")\n",
    "print(f\"  n_samples total: {n_samples}\")\n",
    "print(f\"  n_samples CV   : {n_samples_cv} ({100 * n_samples_cv / n_samples:.1f}%)\")\n",
    "print(f\"  n_samples FINAL: {n_samples_final} ({100 * n_samples_final / n_samples:.1f}%)\")\n",
    "print(\"  CV range   :\", int(idx_cv_all[0]), int(idx_cv_all[-1]))\n",
    "print(\"  FINAL range:\", int(idx_final_test[0]), int(idx_final_test[-1]))\n",
    "\n",
    "walk_splits = make_walk_forward_splits(\n",
    "    n_samples_=n_samples_cv,\n",
    "    train_min_frac=CFG[\"train_min_frac\"],\n",
    "    val_window_frac=CFG[\"val_window_frac\"],\n",
    "    test_window_frac=CFG[\"test_window_frac\"],\n",
    "    step_window_frac=CFG[\"step_window_frac\"],\n",
    ")\n",
    "\n",
    "print(\"\\nWalk-forward folds:\", len(walk_splits))\n",
    "for i, (a, b, c) in enumerate(walk_splits, 1):\n",
    "    print(f\"  fold {i}: train={len(a)} | val={len(b)} | test={len(c)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Step 6 — Dataset + scaling (train-only) + helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "class LobGraphSequenceDataset2Stage(Dataset):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      x_seq: (L,N,F)\n",
    "      e_seq: (L,E,edge_dim)\n",
    "      y_trade: scalar\n",
    "      y_dir: scalar\n",
    "      exit_ret: scalar\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        X_node: np.ndarray,\n",
    "        E_feat: np.ndarray,\n",
    "        y_trade_arr: np.ndarray,\n",
    "        y_dir_arr: np.ndarray,\n",
    "        exit_ret_arr: np.ndarray,\n",
    "        sample_t_: np.ndarray,\n",
    "        indices: np.ndarray,\n",
    "        lookback: int,\n",
    "    ):\n",
    "        self.X_node = X_node\n",
    "        self.E_feat = E_feat\n",
    "        self.y_trade = y_trade_arr\n",
    "        self.y_dir = y_dir_arr\n",
    "        self.exit_ret = exit_ret_arr\n",
    "        self.sample_t = sample_t_\n",
    "        self.indices = indices.astype(np.int64)\n",
    "        self.L = int(lookback)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return int(len(self.indices))\n",
    "\n",
    "    def __getitem__(self, i: int):\n",
    "        sidx = int(self.indices[i])\n",
    "        t = int(self.sample_t[sidx])\n",
    "        t0 = t - self.L + 1\n",
    "\n",
    "        x_seq = self.X_node[t0:t + 1]  # (L,N,F)\n",
    "        e_seq = self.E_feat[t0:t + 1]  # (L,E,edge_dim)\n",
    "\n",
    "        yt = int(self.y_trade[t])\n",
    "        yd = int(self.y_dir[t])\n",
    "        er = float(self.exit_ret[t])\n",
    "\n",
    "        return (\n",
    "            torch.from_numpy(x_seq),\n",
    "            torch.from_numpy(e_seq),\n",
    "            torch.tensor(yt, dtype=torch.long),\n",
    "            torch.tensor(yd, dtype=torch.long),\n",
    "            torch.tensor(er, dtype=torch.float32),\n",
    "        )\n",
    "\n",
    "\n",
    "def collate_fn_2stage(batch):\n",
    "    xs, es, yts, yds, ers = zip(*batch)\n",
    "    return (\n",
    "        torch.stack(xs, 0),   # (B,L,N,F)\n",
    "        torch.stack(es, 0),   # (B,L,E,edge_dim)\n",
    "        torch.stack(yts, 0),  # (B,)\n",
    "        torch.stack(yds, 0),  # (B,)\n",
    "        torch.stack(ers, 0),  # (B,)\n",
    "    )\n",
    "\n",
    "\n",
    "def fit_scale_nodes_train_only(\n",
    "    X_node_raw_: np.ndarray,\n",
    "    sample_t_: np.ndarray,\n",
    "    idx_train: np.ndarray,\n",
    "    max_abs: float = 10.0\n",
    ") -> Tuple[np.ndarray, RobustScaler]:\n",
    "    last_train_t = int(sample_t_[int(idx_train[-1])])\n",
    "    train_time_mask = np.arange(0, last_train_t + 1)\n",
    "\n",
    "    X_train_time = X_node_raw_[train_time_mask]  # (Ttr,N,F)\n",
    "    Ttr, N, Fdim = X_train_time.shape\n",
    "\n",
    "    scaler = RobustScaler(with_centering=True, with_scaling=True, quantile_range=(5.0, 95.0))\n",
    "    scaler.fit(X_train_time.reshape(-1, Fdim))\n",
    "\n",
    "    X_scaled = scaler.transform(X_node_raw_.reshape(-1, Fdim)).reshape(X_node_raw_.shape).astype(np.float32)\n",
    "    X_scaled = np.clip(X_scaled, -max_abs, max_abs).astype(np.float32)\n",
    "    X_scaled = np.nan_to_num(X_scaled, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n",
    "    return X_scaled, scaler\n",
    "\n",
    "\n",
    "def fit_scale_edges_train_only(\n",
    "    E_raw_: np.ndarray,\n",
    "    sample_t_: np.ndarray,\n",
    "    idx_train: np.ndarray,\n",
    "    max_abs: float = 6.0\n",
    ") -> Tuple[np.ndarray, RobustScaler]:\n",
    "    \"\"\"\n",
    "    Robust-scale edge features per fold (train timeline only).\n",
    "    This is important because fisher-transformed correlations can have long tails.\n",
    "    \"\"\"\n",
    "    last_train_t = int(sample_t_[int(idx_train[-1])])\n",
    "    train_time_mask = np.arange(0, last_train_t + 1)\n",
    "\n",
    "    E_train_time = E_raw_[train_time_mask]  # (Ttr,E,D)\n",
    "    Ttr, Ecnt, D = E_train_time.shape\n",
    "\n",
    "    scaler = RobustScaler(with_centering=True, with_scaling=True, quantile_range=(5.0, 95.0))\n",
    "    scaler.fit(E_train_time.reshape(-1, D))\n",
    "\n",
    "    E_scaled = scaler.transform(E_raw_.reshape(-1, D)).reshape(E_raw_.shape).astype(np.float32)\n",
    "    E_scaled = np.clip(E_scaled, -max_abs, max_abs).astype(np.float32)\n",
    "    E_scaled = np.nan_to_num(E_scaled, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n",
    "    return E_scaled, scaler\n",
    "\n",
    "\n",
    "def subset_trade_indices(indices: np.ndarray, sample_t_: np.ndarray, y_trade_arr: np.ndarray) -> np.ndarray:\n",
    "    tt = sample_t_[indices]\n",
    "    mask = (y_trade_arr[tt] == 1)\n",
    "    return indices[mask]\n",
    "\n",
    "\n",
    "def split_trade_ratio(indices: np.ndarray, sample_t_: np.ndarray, y_trade_arr: np.ndarray) -> float:\n",
    "    tt = sample_t_[indices]\n",
    "    return float(y_trade_arr[tt].mean()) if len(tt) else float(\"nan\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Step 7 — Model (SGA-TCN) — drop-in logits (B,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model sanity logits: torch.Size([2, 2]) | finite: True\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "class SpatialGraphAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Attention with edge_attr in attention scorer:\n",
    "      score_e = a^T [h_src || h_dst || edge_emb]\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim: int, out_dim: int, edge_dim: int, heads: int = 1, dropout: float = 0.1, edge_dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.in_dim = int(in_dim)\n",
    "        self.out_dim = int(out_dim)\n",
    "        self.heads = max(1, int(heads))\n",
    "        self.dropout = float(dropout)\n",
    "        self.edge_dropout = float(edge_dropout)\n",
    "\n",
    "        self.head_dim = max(1, int(math.ceil(out_dim / self.heads)))\n",
    "        self.inner_dim = self.heads * self.head_dim\n",
    "\n",
    "        self.lin_node = nn.Linear(self.in_dim, self.inner_dim, bias=False)\n",
    "        self.lin_edge = nn.Linear(edge_dim, self.inner_dim, bias=False)\n",
    "        self.lin_msg = nn.Linear(self.inner_dim, self.inner_dim, bias=False)\n",
    "\n",
    "        self.attn_vec = nn.Parameter(torch.empty(self.heads, 3 * self.head_dim))\n",
    "\n",
    "        self.out_proj = nn.Linear(self.inner_dim, self.out_dim, bias=False)\n",
    "        self.res_proj = nn.Identity() if self.in_dim == self.out_dim else nn.Linear(self.in_dim, self.out_dim, bias=False)\n",
    "\n",
    "        self.ln = nn.LayerNorm(self.out_dim)\n",
    "        self.attn_drop = nn.Dropout(self.dropout)\n",
    "        self.out_drop = nn.Dropout(self.dropout)\n",
    "        self.act = nn.LeakyReLU(0.2)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        for m in [self.lin_node, self.lin_edge, self.lin_msg, self.out_proj]:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "        if isinstance(self.res_proj, nn.Linear):\n",
    "            nn.init.xavier_uniform_(self.res_proj.weight)\n",
    "        nn.init.xavier_uniform_(self.attn_vec)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, edge_attr: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:\n",
    "        x = torch.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        edge_attr = torch.nan_to_num(edge_attr, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "        if self.training and self.edge_dropout > 0:\n",
    "            edge_attr = F.dropout(edge_attr, p=self.edge_dropout, training=True)\n",
    "\n",
    "        B, N, _ = x.shape\n",
    "        E = edge_index.shape[0]\n",
    "\n",
    "        src_idx = edge_index[:, 0]\n",
    "        dst_idx = edge_index[:, 1]\n",
    "\n",
    "        h = self.lin_node(x).view(B, N, self.heads, self.head_dim)                 # (B,N,Hh,dh)\n",
    "        eemb = self.lin_edge(edge_attr).view(B, E, self.heads, self.head_dim)      # (B,E,Hh,dh)\n",
    "\n",
    "        h_src = h[:, src_idx, :, :]  # (B,E,Hh,dh)\n",
    "        h_dst = h[:, dst_idx, :, :]  # (B,E,Hh,dh)\n",
    "\n",
    "        cat = torch.cat([h_src, h_dst, eemb], dim=-1)                               # (B,E,Hh,3*dh)\n",
    "        scores = (cat * self.attn_vec[None, None, :, :]).sum(dim=-1)               # (B,E,Hh)\n",
    "        scores = self.act(scores)\n",
    "\n",
    "        # softmax per dst-node\n",
    "        alphas = torch.zeros_like(scores)\n",
    "        for n in range(N):\n",
    "            mask = (dst_idx == n)\n",
    "            if int(mask.sum()) == 0:\n",
    "                continue\n",
    "            s = scores[:, mask, :]\n",
    "            a = torch.softmax(s, dim=1)\n",
    "            a = self.attn_drop(a)\n",
    "            alphas[:, mask, :] = a\n",
    "\n",
    "        msg = self.lin_msg(h_src.reshape(B, E, self.inner_dim)).view(B, E, self.heads, self.head_dim)\n",
    "\n",
    "        agg = torch.zeros((B, N, self.heads, self.head_dim), device=x.device, dtype=x.dtype)\n",
    "        for e_i in range(E):\n",
    "            dst = int(dst_idx[e_i].item())\n",
    "            agg[:, dst, :, :] += alphas[:, e_i, :].unsqueeze(-1) * msg[:, e_i, :, :]\n",
    "\n",
    "        out = agg.reshape(B, N, self.inner_dim)\n",
    "        out = self.out_proj(out)\n",
    "        out = self.out_drop(out)\n",
    "\n",
    "        res = self.res_proj(x)\n",
    "        y = self.ln(res + out)\n",
    "        return torch.nan_to_num(y, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "\n",
    "class SpatialGraphAttentionMP(nn.Module):\n",
    "    \"\"\"Applies SpatialGraphAttentionLayer independently at each timestep.\"\"\"\n",
    "    def __init__(self, in_dim: int, hidden: int, edge_dim: int, heads: int, dropout: float, edge_dropout: float):\n",
    "        super().__init__()\n",
    "        self.gat = SpatialGraphAttentionLayer(\n",
    "            in_dim=in_dim, out_dim=hidden, edge_dim=edge_dim,\n",
    "            heads=heads, dropout=dropout, edge_dropout=edge_dropout\n",
    "        )\n",
    "\n",
    "    def forward(self, x_seq: torch.Tensor, e_seq: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:\n",
    "        B, L_, N, F_ = x_seq.shape\n",
    "        x_flat = x_seq.reshape(B * L_, N, F_)\n",
    "        e_flat = e_seq.reshape(B * L_, e_seq.size(2), e_seq.size(3))\n",
    "        h_flat = self.gat(x_flat, e_flat, edge_index)  # (B*L,N,H)\n",
    "        return h_flat.reshape(B, L_, N, -1)\n",
    "\n",
    "\n",
    "class CausalConv1d(nn.Module):\n",
    "    def __init__(self, in_ch: int, out_ch: int, kernel_size: int, dilation: int = 1):\n",
    "        super().__init__()\n",
    "        self.kernel_size = int(kernel_size)\n",
    "        self.dilation = int(dilation)\n",
    "        self.conv = nn.Conv1d(in_ch, out_ch, kernel_size=self.kernel_size, dilation=self.dilation)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        pad_left = (self.kernel_size - 1) * self.dilation\n",
    "        x = F.pad(x, (pad_left, 0))\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, in_ch: int, out_ch: int, kernel_size: int, dilation: int, dropout: float, causal: bool = True):\n",
    "        super().__init__()\n",
    "        self.causal = bool(causal)\n",
    "\n",
    "        if self.causal:\n",
    "            self.conv1 = CausalConv1d(in_ch, out_ch, kernel_size, dilation=dilation)\n",
    "            self.conv2 = CausalConv1d(out_ch, out_ch, kernel_size, dilation=dilation)\n",
    "        else:\n",
    "            pad = ((kernel_size - 1) * dilation) // 2\n",
    "            self.conv1 = nn.Conv1d(in_ch, out_ch, kernel_size, dilation=dilation, padding=pad)\n",
    "            self.conv2 = nn.Conv1d(out_ch, out_ch, kernel_size, dilation=dilation, padding=pad)\n",
    "\n",
    "        self.act = nn.GELU()\n",
    "        self.drop = nn.Dropout(float(dropout))\n",
    "        self.downsample = nn.Identity() if in_ch == out_ch else nn.Conv1d(in_ch, out_ch, kernel_size=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = torch.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        y = self.drop(self.act(self.conv1(x)))\n",
    "        y = self.drop(self.act(self.conv2(y)))\n",
    "        res = self.downsample(x)\n",
    "        return torch.nan_to_num(self.act(y + res), nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "\n",
    "class TemporalConvNet(nn.Module):\n",
    "    def __init__(self, in_ch: int, channels: List[int], kernel_size: int, dropout: float, causal: bool = True):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        cur = int(in_ch)\n",
    "        for i, out_ch in enumerate(channels):\n",
    "            dilation = 2 ** i\n",
    "            layers.append(TemporalBlock(cur, int(out_ch), int(kernel_size), int(dilation), float(dropout), causal=causal))\n",
    "            cur = int(out_ch)\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class GNN_TCN_Classifier(nn.Module):\n",
    "    def __init__(self, node_in: int, edge_dim: int, cfg: Dict, target_node: int, n_classes: int = 2):\n",
    "        super().__init__()\n",
    "        self.target_node = int(target_node)\n",
    "\n",
    "        hidden = int(cfg[\"hidden\"])\n",
    "        dropout = float(cfg[\"dropout\"])\n",
    "        edge_dropout = float(cfg.get(\"edge_dropout\", 0.0))\n",
    "\n",
    "        gat_heads = int(cfg[\"gat_heads\"])\n",
    "        tcn_channels = int(cfg[\"tcn_channels\"])\n",
    "        tcn_layers_n = int(cfg[\"tcn_layers\"])\n",
    "        tcn_kernel = int(cfg[\"tcn_kernel\"])\n",
    "        tcn_dropout = float(cfg[\"tcn_dropout\"])\n",
    "        tcn_causal = bool(cfg[\"tcn_causal\"])\n",
    "        self.tcn_pool = str(cfg[\"tcn_pool\"])\n",
    "\n",
    "        self.gnns = nn.ModuleList()\n",
    "        for i in range(int(cfg[\"gnn_layers\"])):\n",
    "            in_dim = int(node_in) if i == 0 else hidden\n",
    "            self.gnns.append(\n",
    "                SpatialGraphAttentionMP(\n",
    "                    in_dim=in_dim, hidden=hidden, edge_dim=int(edge_dim),\n",
    "                    heads=gat_heads, dropout=dropout, edge_dropout=edge_dropout\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.tcn_in = nn.Linear(hidden, tcn_channels)\n",
    "        self.tcn = TemporalConvNet(\n",
    "            in_ch=tcn_channels,\n",
    "            channels=[tcn_channels] * tcn_layers_n,\n",
    "            kernel_size=tcn_kernel,\n",
    "            dropout=tcn_dropout,\n",
    "            causal=tcn_causal,\n",
    "        )\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(tcn_channels),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(tcn_channels, tcn_channels),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(tcn_channels, n_classes),\n",
    "        )\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, e: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:\n",
    "        x = torch.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        e = torch.nan_to_num(e, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "        h = x\n",
    "        for li, gnn in enumerate(self.gnns):\n",
    "            h = gnn(h, e, edge_index)  # (B,L,N,H)\n",
    "            if li < len(self.gnns) - 1:\n",
    "                h = F.gelu(h)          # <-- важно: нелинейность между слоями\n",
    "\n",
    "        h_tgt = h[:, :, self.target_node, :]  # (B,L,H)\n",
    "        z = self.tcn_in(h_tgt)                # (B,L,C)\n",
    "        z = z.transpose(1, 2)                 # (B,C,L)\n",
    "\n",
    "        y = self.tcn(z)                       # (B,C,L)\n",
    "        emb = y[:, :, -1] if self.tcn_pool == \"last\" else y.mean(dim=-1)\n",
    "        logits = self.head(emb)               # (B,2)\n",
    "        return torch.nan_to_num(logits, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "\n",
    "# sanity\n",
    "B_ = 2\n",
    "Fdim = X_node_raw.shape[-1]\n",
    "E_ = EDGE_INDEX.shape[0]\n",
    "Dedge = edge_feat.shape[-1]\n",
    "x_dummy = torch.randn(B_, L, len(ASSETS), Fdim)\n",
    "e_dummy = torch.randn(B_, L, E_, Dedge)\n",
    "m_dummy = GNN_TCN_Classifier(node_in=Fdim, edge_dim=Dedge, cfg=CFG, target_node=TARGET_NODE).to(DEVICE)\n",
    "with torch.no_grad():\n",
    "    out = m_dummy(x_dummy.to(DEVICE), e_dummy.to(DEVICE), EDGE_INDEX.to(DEVICE))\n",
    "print(\"Model sanity logits:\", out.shape, \"| finite:\", bool(torch.isfinite(out).all().item()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Step 8 — Train/Eval helpers (AUC-oriented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "@torch.no_grad()\n",
    "def eval_binary(model: nn.Module, loader: DataLoader, loss_fn: nn.Module, y_key: str) -> Dict:\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    n = 0\n",
    "\n",
    "    ys = []\n",
    "    probs = []\n",
    "    ers = []\n",
    "\n",
    "    for x, e, y_trade_b, y_dir_b, er in loader:\n",
    "        x = x.to(DEVICE).float()\n",
    "        e = e.to(DEVICE).float()\n",
    "        y = (y_trade_b if y_key == \"trade\" else y_dir_b).to(DEVICE).long()\n",
    "\n",
    "        logits = model(x, e, EDGE_INDEX.to(DEVICE))\n",
    "        loss = loss_fn(logits, y)\n",
    "\n",
    "        total_loss += float(loss.item()) * int(y.size(0))\n",
    "        n += int(y.size(0))\n",
    "\n",
    "        p = torch.softmax(logits, dim=-1).detach().cpu().numpy()\n",
    "        ys.append(y.detach().cpu().numpy())\n",
    "        probs.append(p)\n",
    "        ers.append(er.detach().cpu().numpy())\n",
    "\n",
    "    ys = np.concatenate(ys) if ys else np.array([], dtype=np.int64)\n",
    "    probs = np.concatenate(probs) if probs else np.zeros((0, 2), dtype=np.float32)\n",
    "    ers = np.concatenate(ers) if ers else np.array([], dtype=np.float32)\n",
    "\n",
    "    if len(ys) == 0:\n",
    "        return {\"loss\": np.nan, \"acc\": np.nan, \"f1m\": np.nan, \"auc\": np.nan, \"cm\": None, \"y\": ys, \"prob\": probs, \"er\": ers}\n",
    "\n",
    "    y_pred = probs.argmax(axis=1)\n",
    "    acc = accuracy_score(ys, y_pred)\n",
    "    f1m = f1_score(ys, y_pred, average=\"macro\")\n",
    "    auc = roc_auc_score(ys, probs[:, 1]) if len(np.unique(ys)) == 2 else np.nan\n",
    "    cm = confusion_matrix(ys, y_pred)\n",
    "\n",
    "    return {\n",
    "        \"loss\": total_loss / max(1, n),\n",
    "        \"acc\": float(acc),\n",
    "        \"f1m\": float(f1m),\n",
    "        \"auc\": float(auc) if np.isfinite(auc) else np.nan,\n",
    "        \"cm\": cm,\n",
    "        \"y\": ys,\n",
    "        \"prob\": probs,\n",
    "        \"er\": ers,\n",
    "    }\n",
    "\n",
    "\n",
    "def make_ce_weights_binary(y_np: np.ndarray) -> torch.Tensor:\n",
    "    y_np = np.asarray(y_np, dtype=np.int64)\n",
    "    counts = np.bincount(y_np, minlength=2).astype(np.float64)\n",
    "    counts = np.maximum(counts, 1.0)\n",
    "    w = counts.sum() / (2.0 * counts)\n",
    "    return torch.tensor(w, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "\n",
    "def make_weighted_sampler_from_labels(y_np: np.ndarray) -> WeightedRandomSampler:\n",
    "    y_np = np.asarray(y_np, dtype=np.int64)\n",
    "    counts = np.bincount(y_np, minlength=2).astype(np.float64)\n",
    "    counts = np.maximum(counts, 1.0)\n",
    "    class_w = counts.sum() / (2.0 * counts)          # same idea as CE weights\n",
    "    sample_w = class_w[y_np].astype(np.float64)\n",
    "    sample_w = torch.tensor(sample_w, dtype=torch.double)\n",
    "    return WeightedRandomSampler(weights=sample_w, num_samples=len(sample_w), replacement=True)\n",
    "\n",
    "\n",
    "def train_binary_classifier(\n",
    "    X_scaled: np.ndarray,\n",
    "    edge_scaled: np.ndarray,\n",
    "    y_trade_arr: np.ndarray,\n",
    "    y_dir_arr: np.ndarray,\n",
    "    exit_ret_arr: np.ndarray,\n",
    "    sample_t_: np.ndarray,\n",
    "    idx_train: np.ndarray,\n",
    "    idx_val: np.ndarray,\n",
    "    idx_test: np.ndarray,\n",
    "    cfg: Dict,\n",
    "    stage_name: str,  # \"trade\" or \"dir\"\n",
    ") -> Tuple[nn.Module, Dict]:\n",
    "    assert stage_name in (\"trade\", \"dir\")\n",
    "\n",
    "    L_ = int(cfg[\"lookback\"])\n",
    "    bs = int(cfg[\"batch_size\"])\n",
    "\n",
    "    tr_ds = LobGraphSequenceDataset2Stage(X_scaled, edge_scaled, y_trade_arr, y_dir_arr, exit_ret_arr, sample_t_, idx_train, L_)\n",
    "    va_ds = LobGraphSequenceDataset2Stage(X_scaled, edge_scaled, y_trade_arr, y_dir_arr, exit_ret_arr, sample_t_, idx_val,   L_)\n",
    "    te_ds = LobGraphSequenceDataset2Stage(X_scaled, edge_scaled, y_trade_arr, y_dir_arr, exit_ret_arr, sample_t_, idx_test,  L_)\n",
    "\n",
    "    # labels for sampler/weights (TRAIN only)\n",
    "    t_train = sample_t_[idx_train]\n",
    "    y_train_np = (y_trade_arr[t_train] if stage_name == \"trade\" else y_dir_arr[t_train]).astype(np.int64)\n",
    "\n",
    "    sampler = None\n",
    "    shuffle = True\n",
    "    if bool(cfg.get(\"use_weighted_sampler\", True)):\n",
    "        sampler = make_weighted_sampler_from_labels(y_train_np)\n",
    "        shuffle = False\n",
    "\n",
    "    tr_loader = DataLoader(tr_ds, batch_size=bs, shuffle=shuffle, sampler=sampler, drop_last=False, collate_fn=collate_fn_2stage, num_workers=0)\n",
    "    va_loader = DataLoader(va_ds, batch_size=bs, shuffle=False, drop_last=False, collate_fn=collate_fn_2stage, num_workers=0)\n",
    "    te_loader = DataLoader(te_ds, batch_size=bs, shuffle=False, drop_last=False, collate_fn=collate_fn_2stage, num_workers=0)\n",
    "\n",
    "    node_in = int(X_scaled.shape[-1])\n",
    "    edge_dim = int(edge_scaled.shape[-1])\n",
    "    model = GNN_TCN_Classifier(node_in=node_in, edge_dim=edge_dim, cfg=cfg, target_node=TARGET_NODE).to(DEVICE)\n",
    "\n",
    "    ce_w = make_ce_weights_binary(y_train_np)\n",
    "    loss_fn = nn.CrossEntropyLoss(weight=ce_w, label_smoothing=float(cfg.get(\"label_smoothing\", 0.0)))\n",
    "\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=float(cfg[\"lr\"]), weight_decay=float(cfg[\"weight_decay\"]))\n",
    "\n",
    "    use_onecycle = bool(cfg.get(\"use_onecycle\", True))\n",
    "    if use_onecycle:\n",
    "        sch = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            opt,\n",
    "            max_lr=float(cfg[\"lr\"]),\n",
    "            epochs=int(cfg[\"epochs\"]),\n",
    "            steps_per_epoch=max(1, len(tr_loader)),\n",
    "            pct_start=0.15,\n",
    "            div_factor=10.0,\n",
    "            final_div_factor=50.0,\n",
    "        )\n",
    "    else:\n",
    "        sch = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"max\", factor=0.5, patience=3)\n",
    "\n",
    "    best_auc = -1e18\n",
    "    best_state = None\n",
    "    best_epoch = -1\n",
    "\n",
    "    patience = 7\n",
    "    bad = 0\n",
    "\n",
    "    for ep in range(1, int(cfg[\"epochs\"]) + 1):\n",
    "        model.train()\n",
    "        tot_loss = 0.0\n",
    "        n = 0\n",
    "\n",
    "        for x, e, y_trade_b, y_dir_b, _er in tr_loader:\n",
    "            x = x.to(DEVICE).float()\n",
    "            e = e.to(DEVICE).float()\n",
    "            y = (y_trade_b if stage_name == \"trade\" else y_dir_b).to(DEVICE).long()\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            logits = model(x, e, EDGE_INDEX.to(DEVICE))\n",
    "            loss = loss_fn(logits, y)\n",
    "            if not torch.isfinite(loss):\n",
    "                continue\n",
    "\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), float(cfg[\"grad_clip\"]))\n",
    "            opt.step()\n",
    "\n",
    "            if use_onecycle:\n",
    "                sch.step()\n",
    "\n",
    "            tot_loss += float(loss.item()) * int(y.size(0))\n",
    "            n += int(y.size(0))\n",
    "\n",
    "        tr_loss = tot_loss / max(1, n)\n",
    "\n",
    "        va = eval_binary(model, va_loader, loss_fn, y_key=stage_name)\n",
    "        va_auc = va[\"auc\"]\n",
    "        sel_auc = float(va_auc) if np.isfinite(va_auc) else -1e18\n",
    "\n",
    "        if sel_auc > best_auc:\n",
    "            best_auc = sel_auc\n",
    "            best_epoch = ep\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            bad = 0\n",
    "        else:\n",
    "            bad += 1\n",
    "\n",
    "        if not use_onecycle:\n",
    "            sch.step(sel_auc)\n",
    "\n",
    "        lr_now = opt.param_groups[0][\"lr\"]\n",
    "        print(\n",
    "            f\"[{stage_name}] ep {ep:02d} lr={lr_now:.2e} \"\n",
    "            f\"tr_loss={tr_loss:.4f} va_loss={va['loss']:.4f} va_auc={va_auc:.3f} \"\n",
    "            f\"best={best_auc:.3f}@ep{best_epoch:02d}\"\n",
    "        )\n",
    "\n",
    "        if bad >= patience:\n",
    "            break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    va = eval_binary(model, va_loader, loss_fn, y_key=stage_name)\n",
    "    te = eval_binary(model, te_loader, loss_fn, y_key=stage_name)\n",
    "\n",
    "    res = {\n",
    "        \"best_epoch\": int(best_epoch),\n",
    "        \"best_val_auc\": float(best_auc) if np.isfinite(best_auc) else np.nan,\n",
    "        \"val\": va,\n",
    "        \"test\": te,\n",
    "    }\n",
    "    return model, res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Step 9 — Two-stage PnL + threshold sweep (val only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def build_trade_threshold_grid(\n",
    "    p_trade: np.ndarray,\n",
    "    base_grid: Optional[List[float]] = None,\n",
    "    target_trades_list: Optional[List[int]] = None,\n",
    "    min_thr: float = 0.01,\n",
    "    max_thr: float = 0.99,\n",
    ") -> List[float]:\n",
    "    p_trade = np.asarray(p_trade, dtype=np.float64)\n",
    "    p_trade = p_trade[np.isfinite(p_trade)]\n",
    "    if p_trade.size == 0:\n",
    "        return base_grid or [0.5]\n",
    "\n",
    "    thrs = set(float(t) for t in (base_grid or []))\n",
    "\n",
    "    if target_trades_list:\n",
    "        N = int(p_trade.size)\n",
    "        for k in target_trades_list:\n",
    "            k = int(k)\n",
    "            if k <= 0:\n",
    "                continue\n",
    "            if k >= N:\n",
    "                thr = float(np.min(p_trade))\n",
    "            else:\n",
    "                q = 1.0 - (k / N)\n",
    "                thr = float(np.quantile(p_trade, q))\n",
    "            thrs.add(float(np.clip(thr, min_thr, max_thr)))\n",
    "\n",
    "    out = sorted(thrs)\n",
    "    cleaned = []\n",
    "    for t in out:\n",
    "        if not cleaned or abs(t - cleaned[-1]) > 1e-6:\n",
    "            cleaned.append(float(t))\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def two_stage_trade_mask(prob_trade: np.ndarray, prob_dir: np.ndarray, thr_trade: float, thr_dir: float) -> np.ndarray:\n",
    "    p_trade = prob_trade[:, 1]\n",
    "    p_up = prob_dir[:, 1]\n",
    "    conf_dir = np.maximum(p_up, 1.0 - p_up)\n",
    "    return (p_trade >= float(thr_trade)) & (conf_dir >= float(thr_dir))\n",
    "\n",
    "\n",
    "def two_stage_pnl_by_threshold(\n",
    "    prob_trade: np.ndarray,\n",
    "    prob_dir: np.ndarray,\n",
    "    exit_ret_arr: np.ndarray,\n",
    "    thr_trade: float,\n",
    "    thr_dir: float,\n",
    "    cost_bps: float,\n",
    ") -> Dict:\n",
    "    p_up = prob_dir[:, 1]\n",
    "    mask = two_stage_trade_mask(prob_trade, prob_dir, thr_trade, thr_dir)\n",
    "\n",
    "    action = np.zeros_like(exit_ret_arr, dtype=np.float32)\n",
    "    action[mask] = np.where(p_up[mask] >= 0.5, 1.0, -1.0).astype(np.float32)\n",
    "\n",
    "    cost = (float(cost_bps) * 1e-4) * mask.astype(np.float32)\n",
    "    pnl = action * exit_ret_arr - cost\n",
    "\n",
    "    n = int(len(exit_ret_arr))\n",
    "    n_tr = int(mask.sum())\n",
    "\n",
    "    return {\n",
    "        \"n\": n,\n",
    "        \"n_trades\": n_tr,\n",
    "        \"trade_rate\": float(n_tr / max(1, n)),\n",
    "        \"pnl_sum\": float(pnl.sum()),\n",
    "        \"pnl_mean\": float(pnl.mean()) if n else np.nan,\n",
    "        \"pnl_per_trade\": float(pnl.sum() / max(1, n_tr)),\n",
    "        \"pnl_sharpe\": float((pnl.mean() / (pnl.std() + 1e-12)) * np.sqrt(288)) if n else np.nan,\n",
    "    }\n",
    "\n",
    "\n",
    "def sweep_thresholds(\n",
    "    prob_trade: np.ndarray,\n",
    "    prob_dir: np.ndarray,\n",
    "    exit_ret_arr: np.ndarray,\n",
    "    cfg: Dict,\n",
    "    min_trades: int = 0,\n",
    "    target_trade_rate: Optional[float] = None,\n",
    ") -> pd.DataFrame:\n",
    "    p_trade = prob_trade[:, 1]\n",
    "    thr_trade_grid = build_trade_threshold_grid(\n",
    "        p_trade=p_trade,\n",
    "        base_grid=cfg.get(\"thr_trade_grid\", [0.5]),\n",
    "        target_trades_list=cfg.get(\"proxy_target_trades\", None),\n",
    "        min_thr=0.01,\n",
    "        max_thr=0.99,\n",
    "    )\n",
    "    thr_dir_grid = cfg.get(\"thr_dir_grid\", [0.5])\n",
    "\n",
    "    obj = str(cfg.get(\"thr_objective\", \"pnl_sum\"))\n",
    "    max_rate = cfg.get(\"max_trade_rate_val\", None)\n",
    "    penalty = float(cfg.get(\"trade_rate_penalty\", 0.0))\n",
    "\n",
    "    rows = []\n",
    "    for thr_t in thr_trade_grid:\n",
    "        for thr_d in thr_dir_grid:\n",
    "            m = two_stage_pnl_by_threshold(prob_trade, prob_dir, exit_ret_arr, thr_t, thr_d, cfg[\"cost_bps\"])\n",
    "            if int(m[\"n_trades\"]) < int(min_trades):\n",
    "                continue\n",
    "            if max_rate is not None and float(m[\"trade_rate\"]) > float(max_rate):\n",
    "                continue\n",
    "\n",
    "            base = float(m.get(obj, np.nan))\n",
    "            if not np.isfinite(base):\n",
    "                continue\n",
    "\n",
    "            if target_trade_rate is not None:\n",
    "                score = base - penalty * abs(float(m[\"trade_rate\"]) - float(target_trade_rate))\n",
    "            else:\n",
    "                score = base - penalty * float(m[\"trade_rate\"])\n",
    "\n",
    "            rows.append({\"thr_trade\": float(thr_t), \"thr_dir\": float(thr_d), \"score\": float(score), **m})\n",
    "\n",
    "    if not rows:\n",
    "        # soften constraints\n",
    "        return sweep_thresholds(prob_trade, prob_dir, exit_ret_arr, cfg, min_trades=1, target_trade_rate=target_trade_rate)\n",
    "\n",
    "    df_ = pd.DataFrame(rows).sort_values([\"score\", \"pnl_sum\"], ascending=False)\n",
    "    return df_\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_probs_on_indices(model: nn.Module, X_scaled: np.ndarray, edge_scaled: np.ndarray, indices: np.ndarray, cfg: Dict) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    ds = LobGraphSequenceDataset2Stage(X_scaled, edge_scaled, y_trade, y_dir, exit_ret, sample_t, indices, cfg[\"lookback\"])\n",
    "    loader = DataLoader(ds, batch_size=int(cfg[\"batch_size\"]), shuffle=False, collate_fn=collate_fn_2stage, num_workers=0)\n",
    "\n",
    "    model.eval()\n",
    "    probs = []\n",
    "    ers = []\n",
    "    for x, e, _yt, _yd, er in loader:\n",
    "        x = x.to(DEVICE).float()\n",
    "        e = e.to(DEVICE).float()\n",
    "        logits = model(x, e, EDGE_INDEX.to(DEVICE))\n",
    "        p = torch.softmax(logits, dim=-1).cpu().numpy()\n",
    "        probs.append(p)\n",
    "        ers.append(er.cpu().numpy())\n",
    "\n",
    "    return np.concatenate(probs, axis=0), np.concatenate(ers, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Step 10 — Run walk-forward folds (CV-part): train trade → train dir → test PnL + trade share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FOLD 1/4 sizes: train=6098 val=1219 test=1219\n",
      "True trade ratio (val):  0.377\n",
      "True trade ratio (test): 0.295\n",
      "[trade] ep 01 lr=3.00e-04 tr_loss=0.7827 va_loss=0.6950 va_auc=0.526 best=0.526@ep01\n",
      "[trade] ep 02 lr=3.00e-04 tr_loss=0.7123 va_loss=0.6942 va_auc=0.555 best=0.555@ep02\n",
      "[trade] ep 03 lr=3.00e-04 tr_loss=0.7028 va_loss=0.6925 va_auc=0.533 best=0.555@ep02\n",
      "[trade] ep 04 lr=3.00e-04 tr_loss=0.6985 va_loss=0.6968 va_auc=0.522 best=0.555@ep02\n",
      "[trade] ep 05 lr=3.00e-04 tr_loss=0.6969 va_loss=0.6944 va_auc=0.525 best=0.555@ep02\n",
      "[trade] ep 06 lr=1.50e-04 tr_loss=0.6942 va_loss=0.6969 va_auc=0.528 best=0.555@ep02\n",
      "[trade] ep 07 lr=1.50e-04 tr_loss=0.6875 va_loss=0.6955 va_auc=0.538 best=0.555@ep02\n",
      "[trade] ep 08 lr=1.50e-04 tr_loss=0.6868 va_loss=0.6920 va_auc=0.541 best=0.555@ep02\n",
      "[trade] ep 09 lr=1.50e-04 tr_loss=0.6821 va_loss=0.6991 va_auc=0.539 best=0.555@ep02\n",
      "[dir] ep 01 lr=3.00e-04 tr_loss=0.8452 va_loss=0.7344 va_auc=0.494 best=0.494@ep01\n",
      "[dir] ep 02 lr=3.00e-04 tr_loss=0.7740 va_loss=0.7045 va_auc=0.520 best=0.520@ep02\n",
      "[dir] ep 03 lr=3.00e-04 tr_loss=0.7122 va_loss=0.7016 va_auc=0.545 best=0.545@ep03\n",
      "[dir] ep 04 lr=3.00e-04 tr_loss=0.7109 va_loss=0.6929 va_auc=0.550 best=0.550@ep04\n",
      "[dir] ep 05 lr=3.00e-04 tr_loss=0.6962 va_loss=0.6882 va_auc=0.571 best=0.571@ep05\n",
      "[dir] ep 06 lr=3.00e-04 tr_loss=0.6988 va_loss=0.6876 va_auc=0.559 best=0.571@ep05\n",
      "[dir] ep 07 lr=3.00e-04 tr_loss=0.6944 va_loss=0.6899 va_auc=0.534 best=0.571@ep05\n",
      "[dir] ep 08 lr=3.00e-04 tr_loss=0.6958 va_loss=0.6921 va_auc=0.537 best=0.571@ep05\n",
      "[dir] ep 09 lr=1.50e-04 tr_loss=0.6852 va_loss=0.6958 va_auc=0.540 best=0.571@ep05\n",
      "[dir] ep 10 lr=1.50e-04 tr_loss=0.6752 va_loss=0.7028 va_auc=0.528 best=0.571@ep05\n",
      "[dir] ep 11 lr=1.50e-04 tr_loss=0.6723 va_loss=0.7026 va_auc=0.533 best=0.571@ep05\n",
      "[dir] ep 12 lr=1.50e-04 tr_loss=0.6612 va_loss=0.7143 va_auc=0.529 best=0.571@ep05\n",
      "\n",
      "Chosen thresholds (from VAL):\n",
      "  thr_trade*=0.550 thr_dir*=0.500 | score=0.1650\n",
      "  val trade_rate(pred)=0.397 | val pnl_sum=0.1689 | val sharpe=1.118\n",
      "\n",
      "Top-5 VAL threshold candidates:\n",
      "    thr_trade  thr_dir     score  trade_rate   pnl_sum  pnl_sharpe  n_trades\n",
      "8    0.550000      0.5  0.164950    0.397047  0.168888    1.118062       484\n",
      "6    0.500000      0.6  0.111082    0.228876  0.140778    1.226890       279\n",
      "11   0.577248      0.5  0.107093    0.164069  0.149751    1.420244       200\n",
      "0    0.350000      0.6  0.103401    0.232158  0.132441    1.150159       283\n",
      "2    0.400000      0.6  0.103401    0.232158  0.132441    1.150159       283\n",
      "\n",
      "TEST (fixed thr from VAL):\n",
      "  trade_rate(pred)=0.446 | pnl_sum=-0.2652 | pnl_mean=-0.000218 | trades=544\n",
      "\n",
      "================================================================================\n",
      "FOLD 2/4 sizes: train=7317 val=1219 test=1219\n",
      "True trade ratio (val):  0.295\n",
      "True trade ratio (test): 0.397\n",
      "[trade] ep 01 lr=3.00e-04 tr_loss=0.7556 va_loss=0.7111 va_auc=0.541 best=0.541@ep01\n",
      "[trade] ep 02 lr=3.00e-04 tr_loss=0.7059 va_loss=0.6964 va_auc=0.548 best=0.548@ep02\n",
      "[trade] ep 03 lr=3.00e-04 tr_loss=0.7018 va_loss=0.6936 va_auc=0.547 best=0.548@ep02\n",
      "[trade] ep 04 lr=3.00e-04 tr_loss=0.6962 va_loss=0.7106 va_auc=0.539 best=0.548@ep02\n",
      "[trade] ep 05 lr=3.00e-04 tr_loss=0.6942 va_loss=0.7069 va_auc=0.536 best=0.548@ep02\n",
      "[trade] ep 06 lr=1.50e-04 tr_loss=0.6879 va_loss=0.6865 va_auc=0.523 best=0.548@ep02\n",
      "[trade] ep 07 lr=1.50e-04 tr_loss=0.6819 va_loss=0.6999 va_auc=0.534 best=0.548@ep02\n",
      "[trade] ep 08 lr=1.50e-04 tr_loss=0.6813 va_loss=0.6856 va_auc=0.547 best=0.548@ep02\n",
      "[trade] ep 09 lr=1.50e-04 tr_loss=0.6791 va_loss=0.6889 va_auc=0.551 best=0.551@ep09\n",
      "[trade] ep 10 lr=1.50e-04 tr_loss=0.6718 va_loss=0.7176 va_auc=0.549 best=0.551@ep09\n",
      "[trade] ep 11 lr=1.50e-04 tr_loss=0.6711 va_loss=0.6898 va_auc=0.559 best=0.559@ep11\n",
      "[trade] ep 12 lr=1.50e-04 tr_loss=0.6624 va_loss=0.6990 va_auc=0.559 best=0.559@ep12\n",
      "[trade] ep 13 lr=1.50e-04 tr_loss=0.6610 va_loss=0.7072 va_auc=0.578 best=0.578@ep13\n",
      "[trade] ep 14 lr=1.50e-04 tr_loss=0.6562 va_loss=0.6752 va_auc=0.566 best=0.578@ep13\n",
      "[trade] ep 15 lr=1.50e-04 tr_loss=0.6483 va_loss=0.6948 va_auc=0.576 best=0.578@ep13\n",
      "[trade] ep 16 lr=1.50e-04 tr_loss=0.6376 va_loss=0.7079 va_auc=0.576 best=0.578@ep13\n",
      "[trade] ep 17 lr=7.50e-05 tr_loss=0.6321 va_loss=0.7211 va_auc=0.566 best=0.578@ep13\n",
      "[trade] ep 18 lr=7.50e-05 tr_loss=0.6191 va_loss=0.7287 va_auc=0.572 best=0.578@ep13\n",
      "[trade] ep 19 lr=7.50e-05 tr_loss=0.6066 va_loss=0.7137 va_auc=0.577 best=0.578@ep13\n",
      "[trade] ep 20 lr=7.50e-05 tr_loss=0.5980 va_loss=0.7616 va_auc=0.566 best=0.578@ep13\n",
      "[dir] ep 01 lr=3.00e-04 tr_loss=0.8636 va_loss=0.7023 va_auc=0.514 best=0.514@ep01\n",
      "[dir] ep 02 lr=3.00e-04 tr_loss=0.7583 va_loss=0.7199 va_auc=0.453 best=0.514@ep01\n",
      "[dir] ep 03 lr=3.00e-04 tr_loss=0.7349 va_loss=0.7005 va_auc=0.465 best=0.514@ep01\n",
      "[dir] ep 04 lr=3.00e-04 tr_loss=0.7074 va_loss=0.6796 va_auc=0.474 best=0.514@ep01\n",
      "[dir] ep 05 lr=1.50e-04 tr_loss=0.7103 va_loss=0.6890 va_auc=0.439 best=0.514@ep01\n",
      "[dir] ep 06 lr=1.50e-04 tr_loss=0.7027 va_loss=0.6761 va_auc=0.446 best=0.514@ep01\n",
      "[dir] ep 07 lr=1.50e-04 tr_loss=0.7010 va_loss=0.7046 va_auc=0.457 best=0.514@ep01\n",
      "[dir] ep 08 lr=1.50e-04 tr_loss=0.6932 va_loss=0.7097 va_auc=0.449 best=0.514@ep01\n",
      "\n",
      "Chosen thresholds (from VAL):\n",
      "  thr_trade*=0.400 thr_dir*=0.650 | score=0.1335\n",
      "  val trade_rate(pred)=0.248 | val pnl_sum=0.1429 | val sharpe=1.383\n",
      "\n",
      "Top-5 VAL threshold candidates:\n",
      "    thr_trade  thr_dir     score  trade_rate   pnl_sum  pnl_sharpe  n_trades\n",
      "5        0.40     0.65  0.133506    0.247744  0.142858    1.383194       302\n",
      "1        0.35     0.65  0.126001    0.283019  0.128298    1.188454       345\n",
      "12       0.50     0.55  0.113923    0.321575  0.119337    0.986009       392\n",
      "4        0.40     0.60  0.090613    0.394586  0.110629    0.884423       481\n",
      "8        0.45     0.60  0.087127    0.305168  0.089260    0.772057       372\n",
      "\n",
      "TEST (fixed thr from VAL):\n",
      "  trade_rate(pred)=0.278 | pnl_sum=0.0208 | pnl_mean=0.000017 | trades=339\n",
      "\n",
      "================================================================================\n",
      "FOLD 3/4 sizes: train=8536 val=1219 test=1219\n",
      "True trade ratio (val):  0.397\n",
      "True trade ratio (test): 0.472\n",
      "[trade] ep 01 lr=3.00e-04 tr_loss=0.7687 va_loss=0.6892 va_auc=0.554 best=0.554@ep01\n",
      "[trade] ep 02 lr=3.00e-04 tr_loss=0.7065 va_loss=0.6939 va_auc=0.540 best=0.554@ep01\n",
      "[trade] ep 03 lr=3.00e-04 tr_loss=0.6955 va_loss=0.6888 va_auc=0.559 best=0.559@ep03\n",
      "[trade] ep 04 lr=3.00e-04 tr_loss=0.6949 va_loss=0.6856 va_auc=0.571 best=0.571@ep04\n",
      "[trade] ep 05 lr=3.00e-04 tr_loss=0.6914 va_loss=0.7160 va_auc=0.605 best=0.605@ep05\n",
      "[trade] ep 06 lr=3.00e-04 tr_loss=0.6915 va_loss=0.6876 va_auc=0.557 best=0.605@ep05\n",
      "[trade] ep 07 lr=3.00e-04 tr_loss=0.6824 va_loss=0.6914 va_auc=0.547 best=0.605@ep05\n",
      "[trade] ep 08 lr=3.00e-04 tr_loss=0.6780 va_loss=0.6900 va_auc=0.540 best=0.605@ep05\n",
      "[trade] ep 09 lr=1.50e-04 tr_loss=0.6711 va_loss=0.7182 va_auc=0.555 best=0.605@ep05\n",
      "[trade] ep 10 lr=1.50e-04 tr_loss=0.6614 va_loss=0.7035 va_auc=0.574 best=0.605@ep05\n",
      "[trade] ep 11 lr=1.50e-04 tr_loss=0.6573 va_loss=0.7768 va_auc=0.583 best=0.605@ep05\n",
      "[trade] ep 12 lr=1.50e-04 tr_loss=0.6496 va_loss=0.7452 va_auc=0.572 best=0.605@ep05\n",
      "[dir] ep 01 lr=3.00e-04 tr_loss=0.8494 va_loss=0.6836 va_auc=0.580 best=0.580@ep01\n",
      "[dir] ep 02 lr=3.00e-04 tr_loss=0.7475 va_loss=0.6642 va_auc=0.586 best=0.586@ep02\n",
      "[dir] ep 03 lr=3.00e-04 tr_loss=0.7233 va_loss=0.6770 va_auc=0.609 best=0.609@ep03\n",
      "[dir] ep 04 lr=3.00e-04 tr_loss=0.7074 va_loss=0.6905 va_auc=0.556 best=0.609@ep03\n",
      "[dir] ep 05 lr=3.00e-04 tr_loss=0.7039 va_loss=0.6808 va_auc=0.571 best=0.609@ep03\n",
      "[dir] ep 06 lr=3.00e-04 tr_loss=0.6987 va_loss=0.6855 va_auc=0.555 best=0.609@ep03\n",
      "[dir] ep 07 lr=1.50e-04 tr_loss=0.6964 va_loss=0.6709 va_auc=0.579 best=0.609@ep03\n",
      "[dir] ep 08 lr=1.50e-04 tr_loss=0.6959 va_loss=0.6651 va_auc=0.599 best=0.609@ep03\n",
      "[dir] ep 09 lr=1.50e-04 tr_loss=0.6930 va_loss=0.6794 va_auc=0.599 best=0.609@ep03\n",
      "[dir] ep 10 lr=1.50e-04 tr_loss=0.6829 va_loss=0.6710 va_auc=0.588 best=0.609@ep03\n",
      "\n",
      "Chosen thresholds (from VAL):\n",
      "  thr_trade*=0.450 thr_dir*=0.550 | score=0.2869\n",
      "  val trade_rate(pred)=0.398 | val pnl_sum=0.2871 | val sharpe=1.753\n",
      "\n",
      "Top-5 VAL threshold candidates:\n",
      "   thr_trade  thr_dir     score  trade_rate   pnl_sum  pnl_sharpe  n_trades\n",
      "4       0.45     0.55  0.286947    0.397867  0.287111    1.752693       485\n",
      "6       0.50     0.55  0.284775    0.353568  0.293471    1.850986       431\n",
      "0       0.35     0.55  0.281229    0.410172  0.283855    1.714773       500\n",
      "2       0.40     0.55  0.281229    0.410172  0.283855    1.714773       500\n",
      "8       0.55     0.55  0.240791    0.283019  0.263596    1.818603       345\n",
      "\n",
      "TEST (fixed thr from VAL):\n",
      "  trade_rate(pred)=0.371 | pnl_sum=-0.2028 | pnl_mean=-0.000166 | trades=452\n",
      "\n",
      "================================================================================\n",
      "FOLD 4/4 sizes: train=9755 val=1219 test=1219\n",
      "True trade ratio (val):  0.472\n",
      "True trade ratio (test): 0.411\n",
      "[trade] ep 01 lr=3.00e-04 tr_loss=0.7617 va_loss=0.7004 va_auc=0.561 best=0.561@ep01\n",
      "[trade] ep 02 lr=3.00e-04 tr_loss=0.7032 va_loss=0.6685 va_auc=0.589 best=0.589@ep02\n",
      "[trade] ep 03 lr=3.00e-04 tr_loss=0.6965 va_loss=0.6598 va_auc=0.600 best=0.600@ep03\n",
      "[trade] ep 04 lr=3.00e-04 tr_loss=0.6899 va_loss=0.6651 va_auc=0.581 best=0.600@ep03\n",
      "[trade] ep 05 lr=3.00e-04 tr_loss=0.6888 va_loss=0.6684 va_auc=0.592 best=0.600@ep03\n",
      "[trade] ep 06 lr=3.00e-04 tr_loss=0.6824 va_loss=0.6637 va_auc=0.606 best=0.606@ep06\n",
      "[trade] ep 07 lr=3.00e-04 tr_loss=0.6731 va_loss=0.6655 va_auc=0.592 best=0.606@ep06\n",
      "[trade] ep 08 lr=3.00e-04 tr_loss=0.6609 va_loss=0.6807 va_auc=0.586 best=0.606@ep06\n",
      "[trade] ep 09 lr=3.00e-04 tr_loss=0.6539 va_loss=0.6818 va_auc=0.579 best=0.606@ep06\n",
      "[trade] ep 10 lr=1.50e-04 tr_loss=0.6441 va_loss=0.7058 va_auc=0.583 best=0.606@ep06\n",
      "[trade] ep 11 lr=1.50e-04 tr_loss=0.6337 va_loss=0.7087 va_auc=0.574 best=0.606@ep06\n",
      "[trade] ep 12 lr=1.50e-04 tr_loss=0.6126 va_loss=0.7198 va_auc=0.574 best=0.606@ep06\n",
      "[trade] ep 13 lr=1.50e-04 tr_loss=0.6059 va_loss=0.7368 va_auc=0.576 best=0.606@ep06\n",
      "[dir] ep 01 lr=3.00e-04 tr_loss=0.8078 va_loss=0.7088 va_auc=0.462 best=0.462@ep01\n",
      "[dir] ep 02 lr=3.00e-04 tr_loss=0.7165 va_loss=0.7012 va_auc=0.482 best=0.482@ep02\n",
      "[dir] ep 03 lr=3.00e-04 tr_loss=0.7066 va_loss=0.7032 va_auc=0.444 best=0.482@ep02\n",
      "[dir] ep 04 lr=3.00e-04 tr_loss=0.6986 va_loss=0.7070 va_auc=0.449 best=0.482@ep02\n",
      "[dir] ep 05 lr=3.00e-04 tr_loss=0.6939 va_loss=0.7135 va_auc=0.451 best=0.482@ep02\n",
      "[dir] ep 06 lr=1.50e-04 tr_loss=0.6965 va_loss=0.7138 va_auc=0.447 best=0.482@ep02\n",
      "[dir] ep 07 lr=1.50e-04 tr_loss=0.6887 va_loss=0.7185 va_auc=0.450 best=0.482@ep02\n",
      "[dir] ep 08 lr=1.50e-04 tr_loss=0.6888 va_loss=0.7224 va_auc=0.451 best=0.482@ep02\n",
      "[dir] ep 09 lr=1.50e-04 tr_loss=0.6873 va_loss=0.7241 va_auc=0.461 best=0.482@ep02\n",
      "\n",
      "Chosen thresholds (from VAL):\n",
      "  thr_trade*=0.550 thr_dir*=0.550 | score=-0.1253\n",
      "  val trade_rate(pred)=0.242 | val pnl_sum=-0.0794 | val sharpe=-0.391\n",
      "\n",
      "Top-5 VAL threshold candidates:\n",
      "    thr_trade  thr_dir     score  trade_rate   pnl_sum  pnl_sharpe  n_trades\n",
      "5    0.550000     0.55 -0.125314    0.242002 -0.079375   -0.390572       295\n",
      "3    0.500000     0.55 -0.132417    0.294504 -0.096979   -0.435799       359\n",
      "17   0.837118     0.50 -0.142931    0.041017 -0.056795   -0.494747        50\n",
      "7    0.600000     0.55 -0.149839    0.198523 -0.095204   -0.502650       242\n",
      "2    0.450000     0.55 -0.152590    0.369975 -0.132246   -0.548940       451\n",
      "\n",
      "TEST (fixed thr from VAL):\n",
      "  trade_rate(pred)=0.155 | pnl_sum=0.0422 | pnl_mean=0.000035 | trades=189\n",
      "\n",
      "================================================================================\n",
      "CV summary (fold TEST, fixed thresholds from VAL):\n",
      "   fold  trade_test_auc  dir_test_auc  test_trade_rate_pred  test_pnl_sum  \\\n",
      "0     1        0.540128      0.490773              0.446267     -0.265237   \n",
      "1     2        0.585065      0.486859              0.278097      0.020781   \n",
      "2     3        0.624261      0.494032              0.370796     -0.202762   \n",
      "3     4        0.567122      0.534207              0.155045      0.042181   \n",
      "\n",
      "   test_pnl_mean  thr_trade  thr_dir  n_trades  \n",
      "0      -0.000218       0.55     0.50       544  \n",
      "1       0.000017       0.40     0.65       339  \n",
      "2      -0.000166       0.45     0.55       452  \n",
      "3       0.000035       0.55     0.55       189  \n",
      "\n",
      "Means:\n",
      "fold                      2.500000\n",
      "trade_test_auc            0.579144\n",
      "dir_test_auc              0.501468\n",
      "test_trade_rate_pred      0.312551\n",
      "test_pnl_sum             -0.101259\n",
      "test_pnl_mean            -0.000083\n",
      "thr_trade                 0.487500\n",
      "thr_dir                   0.562500\n",
      "n_trades                381.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "def run_walk_forward_cv() -> pd.DataFrame:\n",
    "    rows = []\n",
    "\n",
    "    for fi, (idx_tr, idx_va, idx_te) in enumerate(walk_splits, 1):\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(f\"FOLD {fi}/{len(walk_splits)} sizes: train={len(idx_tr)} val={len(idx_va)} test={len(idx_te)}\")\n",
    "        true_val_trade = split_trade_ratio(idx_va, sample_t, y_trade)\n",
    "        true_te_trade = split_trade_ratio(idx_te, sample_t, y_trade)\n",
    "        print(f\"True trade ratio (val):  {true_val_trade:.3f}\")\n",
    "        print(f\"True trade ratio (test): {true_te_trade:.3f}\")\n",
    "\n",
    "        # scale per fold (fit only on train timeline)\n",
    "        X_scaled, _ = fit_scale_nodes_train_only(X_node_raw, sample_t, idx_tr, max_abs=CFG[\"max_abs_feat\"])\n",
    "\n",
    "        # edge scaling per fold (важно!)\n",
    "        if bool(CFG.get(\"edge_scale\", True)):\n",
    "            edge_scaled, _ = fit_scale_edges_train_only(edge_feat, sample_t, idx_tr, max_abs=CFG[\"max_abs_edge\"])\n",
    "        else:\n",
    "            edge_scaled = edge_feat\n",
    "\n",
    "        # Stage A\n",
    "        m_trade, r_trade = train_binary_classifier(\n",
    "            X_scaled, edge_scaled, y_trade, y_dir, exit_ret, sample_t,\n",
    "            idx_tr, idx_va, idx_te, CFG, stage_name=\"trade\"\n",
    "        )\n",
    "\n",
    "        # Stage B (trade-only indices)\n",
    "        idx_tr_T = subset_trade_indices(idx_tr, sample_t, y_trade)\n",
    "        idx_va_T = subset_trade_indices(idx_va, sample_t, y_trade)\n",
    "        idx_te_T = subset_trade_indices(idx_te, sample_t, y_trade)\n",
    "\n",
    "        if len(idx_tr_T) < max(200, 2 * CFG[\"batch_size\"]) or len(idx_te_T) < 50:\n",
    "            print(\"[dir] skip: not enough trade samples in this fold.\")\n",
    "            rows.append({\n",
    "                \"fold\": fi,\n",
    "                \"trade_test_auc\": r_trade[\"test\"][\"auc\"],\n",
    "                \"dir_test_auc\": np.nan,\n",
    "                \"test_trade_rate_pred\": np.nan,\n",
    "                \"test_pnl_sum\": np.nan,\n",
    "                \"test_pnl_mean\": np.nan,\n",
    "                \"thr_trade\": np.nan,\n",
    "                \"thr_dir\": np.nan,\n",
    "                \"n_trades\": np.nan,\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        m_dir, r_dir = train_binary_classifier(\n",
    "            X_scaled, edge_scaled, y_trade, y_dir, exit_ret, sample_t,\n",
    "            idx_tr_T, idx_va_T, idx_te_T, CFG, stage_name=\"dir\"\n",
    "        )\n",
    "\n",
    "        # Choose thresholds on VAL with anti-overtrading constraint\n",
    "        prob_trade_val, er_val = predict_probs_on_indices(m_trade, X_scaled, edge_scaled, idx_va, CFG)\n",
    "        prob_dir_val, _ = predict_probs_on_indices(m_dir, X_scaled, edge_scaled, idx_va, CFG)\n",
    "\n",
    "        sweep_val = sweep_thresholds(\n",
    "            prob_trade_val, prob_dir_val, er_val, CFG,\n",
    "            min_trades=int(CFG[\"eval_min_trades\"]),\n",
    "            target_trade_rate=float(true_val_trade),\n",
    "        )\n",
    "        best_val = sweep_val.iloc[0].to_dict()\n",
    "\n",
    "        thr_trade_star = float(best_val[\"thr_trade\"])\n",
    "        thr_dir_star = float(best_val[\"thr_dir\"])\n",
    "\n",
    "        val_metrics = two_stage_pnl_by_threshold(prob_trade_val, prob_dir_val, er_val, thr_trade_star, thr_dir_star, CFG[\"cost_bps\"])\n",
    "        print(\"\\nChosen thresholds (from VAL):\")\n",
    "        print(f\"  thr_trade*={thr_trade_star:.3f} thr_dir*={thr_dir_star:.3f} | score={best_val['score']:.4f}\")\n",
    "        print(f\"  val trade_rate(pred)={val_metrics['trade_rate']:.3f} | val pnl_sum={val_metrics['pnl_sum']:.4f} | val sharpe={val_metrics['pnl_sharpe']:.3f}\")\n",
    "\n",
    "        print(\"\\nTop-5 VAL threshold candidates:\")\n",
    "        print(sweep_val.head(5)[[\"thr_trade\", \"thr_dir\", \"score\", \"trade_rate\", \"pnl_sum\", \"pnl_sharpe\", \"n_trades\"]])\n",
    "\n",
    "        # Evaluate on TEST with fixed thresholds\n",
    "        prob_trade_te, er_te = predict_probs_on_indices(m_trade, X_scaled, edge_scaled, idx_te, CFG)\n",
    "        prob_dir_te, _ = predict_probs_on_indices(m_dir, X_scaled, edge_scaled, idx_te, CFG)\n",
    "        te_metrics = two_stage_pnl_by_threshold(prob_trade_te, prob_dir_te, er_te, thr_trade_star, thr_dir_star, CFG[\"cost_bps\"])\n",
    "\n",
    "        print(\"\\nTEST (fixed thr from VAL):\")\n",
    "        print(f\"  trade_rate(pred)={te_metrics['trade_rate']:.3f} | pnl_sum={te_metrics['pnl_sum']:.4f} | pnl_mean={te_metrics['pnl_mean']:.6f} | trades={te_metrics['n_trades']}\")\n",
    "\n",
    "        rows.append({\n",
    "            \"fold\": fi,\n",
    "            \"trade_test_auc\": r_trade[\"test\"][\"auc\"],\n",
    "            \"dir_test_auc\": r_dir[\"test\"][\"auc\"],\n",
    "            \"test_trade_rate_pred\": te_metrics[\"trade_rate\"],\n",
    "            \"test_pnl_sum\": te_metrics[\"pnl_sum\"],\n",
    "            \"test_pnl_mean\": te_metrics[\"pnl_mean\"],\n",
    "            \"thr_trade\": thr_trade_star,\n",
    "            \"thr_dir\": thr_dir_star,\n",
    "            \"n_trades\": te_metrics[\"n_trades\"],\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "cv_summary = run_walk_forward_cv()\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CV summary (fold TEST, fixed thresholds from VAL):\")\n",
    "print(cv_summary)\n",
    "print(\"\\nMeans:\")\n",
    "print(cv_summary.mean(numeric_only=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Step 11 — Final train on CV(90%) and evaluate on FINAL holdout (10%)\n",
    "\n",
    " Здесь печатаем:\n",
    "\n",
    " - AUC по trade/dir\n",
    "\n",
    " - пороги, выбранные на val_final\n",
    "\n",
    " - долю трейдинга на holdout при текущих параметрах (trade_rate(pred))\n",
    "\n",
    " - и oracle на holdout (только как верхняя оценка, НЕ использовать для выбора)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FINAL TRAIN/VAL on CV-part (90%) -> EVAL on FINAL holdout (10%)\n",
      "Final split sizes:\n",
      "  train_final: 10978\n",
      "  val_final  : 1219\n",
      "  holdout    : 1355\n",
      "True trade ratio (val_final):  0.414\n",
      "True trade ratio (holdout):    0.401\n",
      "[trade] ep 01 lr=3.00e-04 tr_loss=0.7507 va_loss=0.6892 va_auc=0.515 best=0.515@ep01\n",
      "[trade] ep 02 lr=3.00e-04 tr_loss=0.6958 va_loss=0.6893 va_auc=0.542 best=0.542@ep02\n",
      "[trade] ep 03 lr=3.00e-04 tr_loss=0.6874 va_loss=0.7016 va_auc=0.538 best=0.542@ep02\n",
      "[trade] ep 04 lr=3.00e-04 tr_loss=0.6879 va_loss=0.6892 va_auc=0.547 best=0.547@ep04\n",
      "[trade] ep 05 lr=3.00e-04 tr_loss=0.6833 va_loss=0.6885 va_auc=0.549 best=0.549@ep05\n",
      "[trade] ep 06 lr=3.00e-04 tr_loss=0.6787 va_loss=0.6962 va_auc=0.564 best=0.564@ep06\n",
      "[trade] ep 07 lr=3.00e-04 tr_loss=0.6771 va_loss=0.6885 va_auc=0.571 best=0.571@ep07\n",
      "[trade] ep 08 lr=3.00e-04 tr_loss=0.6715 va_loss=0.6860 va_auc=0.579 best=0.579@ep08\n",
      "[trade] ep 09 lr=3.00e-04 tr_loss=0.6606 va_loss=0.7093 va_auc=0.569 best=0.579@ep08\n",
      "[trade] ep 10 lr=3.00e-04 tr_loss=0.6553 va_loss=0.6792 va_auc=0.583 best=0.583@ep10\n",
      "[trade] ep 11 lr=3.00e-04 tr_loss=0.6470 va_loss=0.7037 va_auc=0.582 best=0.583@ep10\n",
      "[trade] ep 12 lr=3.00e-04 tr_loss=0.6367 va_loss=0.7354 va_auc=0.590 best=0.590@ep12\n",
      "[trade] ep 13 lr=3.00e-04 tr_loss=0.6275 va_loss=0.6929 va_auc=0.585 best=0.590@ep12\n",
      "[trade] ep 14 lr=3.00e-04 tr_loss=0.6135 va_loss=0.7597 va_auc=0.596 best=0.596@ep14\n",
      "[trade] ep 15 lr=3.00e-04 tr_loss=0.5934 va_loss=0.7754 va_auc=0.588 best=0.596@ep14\n",
      "[trade] ep 16 lr=3.00e-04 tr_loss=0.5816 va_loss=0.7404 va_auc=0.613 best=0.613@ep16\n",
      "[trade] ep 17 lr=3.00e-04 tr_loss=0.5643 va_loss=0.7619 va_auc=0.589 best=0.613@ep16\n",
      "[trade] ep 18 lr=3.00e-04 tr_loss=0.5442 va_loss=0.7806 va_auc=0.580 best=0.613@ep16\n",
      "[trade] ep 19 lr=3.00e-04 tr_loss=0.5297 va_loss=0.7705 va_auc=0.591 best=0.613@ep16\n",
      "[trade] ep 20 lr=1.50e-04 tr_loss=0.5125 va_loss=0.7923 va_auc=0.576 best=0.613@ep16\n",
      "\n",
      "Trade-only sizes for DIR:\n",
      "  train_final_T: 3903\n",
      "  val_final_T  : 505\n",
      "  holdout_T    : 544\n",
      "[dir] ep 01 lr=3.00e-04 tr_loss=0.8055 va_loss=0.7100 va_auc=0.536 best=0.536@ep01\n",
      "[dir] ep 02 lr=3.00e-04 tr_loss=0.7192 va_loss=0.6950 va_auc=0.543 best=0.543@ep02\n",
      "[dir] ep 03 lr=3.00e-04 tr_loss=0.7000 va_loss=0.6933 va_auc=0.530 best=0.543@ep02\n",
      "[dir] ep 04 lr=3.00e-04 tr_loss=0.7025 va_loss=0.6957 va_auc=0.538 best=0.543@ep02\n",
      "[dir] ep 05 lr=3.00e-04 tr_loss=0.6994 va_loss=0.6913 va_auc=0.586 best=0.586@ep05\n",
      "[dir] ep 06 lr=3.00e-04 tr_loss=0.6923 va_loss=0.6972 va_auc=0.583 best=0.586@ep05\n",
      "[dir] ep 07 lr=3.00e-04 tr_loss=0.6905 va_loss=0.6895 va_auc=0.591 best=0.591@ep07\n",
      "[dir] ep 08 lr=3.00e-04 tr_loss=0.6859 va_loss=0.6871 va_auc=0.588 best=0.591@ep07\n",
      "[dir] ep 09 lr=3.00e-04 tr_loss=0.6827 va_loss=0.6902 va_auc=0.606 best=0.606@ep09\n",
      "[dir] ep 10 lr=3.00e-04 tr_loss=0.6758 va_loss=0.6956 va_auc=0.570 best=0.606@ep09\n",
      "[dir] ep 11 lr=3.00e-04 tr_loss=0.6642 va_loss=0.7370 va_auc=0.499 best=0.606@ep09\n",
      "[dir] ep 12 lr=3.00e-04 tr_loss=0.6543 va_loss=0.7509 va_auc=0.482 best=0.606@ep09\n",
      "[dir] ep 13 lr=1.50e-04 tr_loss=0.6467 va_loss=0.8080 va_auc=0.564 best=0.606@ep09\n",
      "[dir] ep 14 lr=1.50e-04 tr_loss=0.6135 va_loss=0.8341 va_auc=0.493 best=0.606@ep09\n",
      "[dir] ep 15 lr=1.50e-04 tr_loss=0.6041 va_loss=0.8095 va_auc=0.521 best=0.606@ep09\n",
      "[dir] ep 16 lr=1.50e-04 tr_loss=0.5929 va_loss=0.8873 va_auc=0.492 best=0.606@ep09\n",
      "\n",
      "Chosen thresholds on val_final:\n",
      "  thr_trade*=0.350 thr_dir*=0.550 | score=0.1088\n",
      "  val trade_rate(pred)=0.340 | val pnl_sum=0.1238 | val sharpe=0.770 | trades=414\n",
      "\n",
      "FINAL HOLDOUT RESULT (fixed thresholds from val_final):\n",
      "  trade_rate(pred)=0.362  <-- доля трейдинга на holdout при текущих параметрах\n",
      "  pnl_sum=-0.3917 | pnl_mean=-0.000289 | trades=490\n",
      "  sharpe(per-bar proxy)=-1.578\n",
      "\n",
      "AUC summary:\n",
      "  TRADE: val_auc=0.613 | holdout_auc=0.559\n",
      "  DIR  : val_auc=0.606 | holdout_auc=0.468\n",
      "\n",
      "Top-5 val_final threshold candidates:\n",
      "    thr_trade  thr_dir     score  trade_rate   pnl_sum  pnl_sharpe  n_trades\n",
      "0        0.35     0.55  0.108820    0.339623  0.123750    0.770235       414\n",
      "5        0.40     0.55  0.072353    0.315012  0.092205    0.593250       384\n",
      "10       0.45     0.55  0.054354    0.290402  0.079128    0.523348       354\n",
      "15       0.50     0.55  0.037188    0.265792  0.066884    0.461373       324\n",
      "1        0.35     0.60  0.027136    0.181296  0.073732    0.633299       221\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "def run_final_train_holdout() -> None:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"FINAL TRAIN/VAL on CV-part (90%) -> EVAL on FINAL holdout (10%)\")\n",
    "\n",
    "    val_w = max(1, int(CFG[\"val_window_frac\"] * n_samples_cv))\n",
    "    train_end = n_samples_cv - val_w\n",
    "\n",
    "    idx_train_final = np.arange(0, train_end, dtype=np.int64)\n",
    "    idx_val_final = np.arange(train_end, n_samples_cv, dtype=np.int64)\n",
    "    idx_holdout = idx_final_test.astype(np.int64)\n",
    "\n",
    "    true_val_trade = split_trade_ratio(idx_val_final, sample_t, y_trade)\n",
    "    true_hold_trade = split_trade_ratio(idx_holdout, sample_t, y_trade)\n",
    "\n",
    "    print(\"Final split sizes:\")\n",
    "    print(\"  train_final:\", len(idx_train_final))\n",
    "    print(\"  val_final  :\", len(idx_val_final))\n",
    "    print(\"  holdout    :\", len(idx_holdout))\n",
    "    print(f\"True trade ratio (val_final):  {true_val_trade:.3f}\")\n",
    "    print(f\"True trade ratio (holdout):    {true_hold_trade:.3f}\")\n",
    "\n",
    "    X_scaled_final, _ = fit_scale_nodes_train_only(X_node_raw, sample_t, idx_train_final, max_abs=CFG[\"max_abs_feat\"])\n",
    "\n",
    "    if bool(CFG.get(\"edge_scale\", True)):\n",
    "        edge_scaled_final, _ = fit_scale_edges_train_only(edge_feat, sample_t, idx_train_final, max_abs=CFG[\"max_abs_edge\"])\n",
    "    else:\n",
    "        edge_scaled_final = edge_feat\n",
    "\n",
    "    # Stage A\n",
    "    m_trade, r_trade = train_binary_classifier(\n",
    "        X_scaled_final, edge_scaled_final, y_trade, y_dir, exit_ret, sample_t,\n",
    "        idx_train_final, idx_val_final, idx_holdout, CFG, stage_name=\"trade\"\n",
    "    )\n",
    "\n",
    "    # Stage B (trade-only)\n",
    "    idx_train_T = subset_trade_indices(idx_train_final, sample_t, y_trade)\n",
    "    idx_val_T = subset_trade_indices(idx_val_final, sample_t, y_trade)\n",
    "    idx_hold_T = subset_trade_indices(idx_holdout, sample_t, y_trade)\n",
    "\n",
    "    print(\"\\nTrade-only sizes for DIR:\")\n",
    "    print(\"  train_final_T:\", len(idx_train_T))\n",
    "    print(\"  val_final_T  :\", len(idx_val_T))\n",
    "    print(\"  holdout_T    :\", len(idx_hold_T))\n",
    "\n",
    "    m_dir, r_dir = train_binary_classifier(\n",
    "        X_scaled_final, edge_scaled_final, y_trade, y_dir, exit_ret, sample_t,\n",
    "        idx_train_T, idx_val_T, idx_hold_T, CFG, stage_name=\"dir\"\n",
    "    )\n",
    "\n",
    "    # choose thresholds on val_final (anti-overtrading)\n",
    "    prob_trade_val, er_val = predict_probs_on_indices(m_trade, X_scaled_final, edge_scaled_final, idx_val_final, CFG)\n",
    "    prob_dir_val, _ = predict_probs_on_indices(m_dir, X_scaled_final, edge_scaled_final, idx_val_final, CFG)\n",
    "\n",
    "    sweep_val = sweep_thresholds(\n",
    "        prob_trade_val, prob_dir_val, er_val, CFG,\n",
    "        min_trades=int(CFG[\"eval_min_trades\"]),\n",
    "        target_trade_rate=float(true_val_trade),\n",
    "    )\n",
    "    best_val = sweep_val.iloc[0].to_dict()\n",
    "    thr_trade_star = float(best_val[\"thr_trade\"])\n",
    "    thr_dir_star = float(best_val[\"thr_dir\"])\n",
    "\n",
    "    val_metrics = two_stage_pnl_by_threshold(prob_trade_val, prob_dir_val, er_val, thr_trade_star, thr_dir_star, CFG[\"cost_bps\"])\n",
    "\n",
    "    print(\"\\nChosen thresholds on val_final:\")\n",
    "    print(f\"  thr_trade*={thr_trade_star:.3f} thr_dir*={thr_dir_star:.3f} | score={best_val['score']:.4f}\")\n",
    "    print(f\"  val trade_rate(pred)={val_metrics['trade_rate']:.3f} | val pnl_sum={val_metrics['pnl_sum']:.4f} | val sharpe={val_metrics['pnl_sharpe']:.3f} | trades={val_metrics['n_trades']}\")\n",
    "\n",
    "    # evaluate holdout with fixed thresholds\n",
    "    prob_trade_hold, er_hold = predict_probs_on_indices(m_trade, X_scaled_final, edge_scaled_final, idx_holdout, CFG)\n",
    "    prob_dir_hold, _ = predict_probs_on_indices(m_dir, X_scaled_final, edge_scaled_final, idx_holdout, CFG)\n",
    "    hold_metrics = two_stage_pnl_by_threshold(prob_trade_hold, prob_dir_hold, er_hold, thr_trade_star, thr_dir_star, CFG[\"cost_bps\"])\n",
    "\n",
    "    print(\"\\nFINAL HOLDOUT RESULT (fixed thresholds from val_final):\")\n",
    "    print(f\"  trade_rate(pred)={hold_metrics['trade_rate']:.3f}  <-- доля трейдинга на holdout при текущих параметрах\")\n",
    "    print(f\"  pnl_sum={hold_metrics['pnl_sum']:.4f} | pnl_mean={hold_metrics['pnl_mean']:.6f} | trades={hold_metrics['n_trades']}\")\n",
    "    print(f\"  sharpe(per-bar proxy)={hold_metrics['pnl_sharpe']:.3f}\")\n",
    "\n",
    "    print(\"\\nAUC summary:\")\n",
    "    print(f\"  TRADE: val_auc={r_trade['val']['auc']:.3f} | holdout_auc={r_trade['test']['auc']:.3f}\")\n",
    "    print(f\"  DIR  : val_auc={r_dir['val']['auc']:.3f} | holdout_auc={r_dir['test']['auc']:.3f}\")\n",
    "\n",
    "    print(\"\\nTop-5 val_final threshold candidates:\")\n",
    "    print(sweep_val.head(5)[[\"thr_trade\", \"thr_dir\", \"score\", \"trade_rate\", \"pnl_sum\", \"pnl_sharpe\", \"n_trades\"]])\n",
    "\n",
    "\n",
    "run_final_train_holdout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Notes (коротко)\n",
    "\n",
    " - “доля трейдинга на тестовом датасете при текущих параметрах” выводится как `trade_rate(pred)`:\n",
    "\n",
    "   - на fold-test: “TEST (fixed thr from VAL)”\n",
    "\n",
    "   - на финальном holdout: “FINAL HOLDOUT RESULT … trade_rate(pred)=…”\n",
    "\n",
    "\n",
    "\n",
    " Если захочешь — в следующем шаге могу:\n",
    "\n",
    " 1) добавить калибровку вероятностей (temperature scaling) только по val,\n",
    "\n",
    " 2) сделать альтернативный objective для threshold selection (например max pnl_sum при фикс. trade_rate),\n",
    "\n",
    " 3) или сделать single-head 3-class (down/flat/up) с метрикой AUC/PR-AUC по trade и down/up отдельно."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recbole_new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
