{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTwo-stage LOB GNN — MTGNN-style experiment\\nVS Code / Jupyter compatible (.py) via #%% cells\\n\\nKey principles:\\n- Train-only scaling (time-ordered, no leakage)\\n- Stage A: trade / no-trade (AUC)\\n- Stage B: direction on trade-only (AUC on trade-only)\\n- Thresholds (thr_trade, thr_dir) are selected ONLY on val, never tuned on test/holdout\\n\\nThis notebook implements:\\n- Learnable adjacency (A_learned) + prior adjacency from edge_attr (A_prior)\\n- Regularization:\\n  (a) L1 on off-diagonal (implemented on sigmoid(adj_logits) for meaningful sparsity pressure)\\n  (b) penalty for deviation from A_prior (MSE on adjacency)\\n- Final adjacency:\\n  A_final = alpha * A_prior + (1 - alpha) * A_learned\\n  alpha is fixed or learned (clipped), controlled by CFG\\n- Temporal block:\\n  Conv (dilated) -> Attention pooling over time\\n\\nNew (per your comment):\\n- We DO NOT use mean/median thresholds as the default \"final check\".\\n- Step 10 now stores per-fold artifacts (models + thresholds + val preds).\\n- Step 11 implements 3 post-CV holdout checks WITHOUT any extra refit:\\n    1) LAST fold model + LAST fold thresholds\\n    2) BEST-VAL fold model + BEST-VAL thresholds\\n    3) LAST fold model + GLOBAL thresholds fitted on concatenated fold-VAL predictions\\n- We fix the variable naming: m_trade_last / m_dir_last are explicitly saved in Step 10.\\n- Step 12 is production-fit: train on CV(90%) with final val window, then evaluate on FINAL holdout(10%).\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Two-stage LOB GNN — MTGNN-style experiment\n",
    "VS Code / Jupyter compatible (.py) via #%% cells\n",
    "\n",
    "Key principles:\n",
    "- Train-only scaling (time-ordered, no leakage)\n",
    "- Stage A: trade / no-trade (AUC)\n",
    "- Stage B: direction on trade-only (AUC on trade-only)\n",
    "- Thresholds (thr_trade, thr_dir) are selected ONLY on val, never tuned on test/holdout\n",
    "\n",
    "This notebook implements:\n",
    "- Learnable adjacency (A_learned) + prior adjacency from edge_attr (A_prior)\n",
    "- Regularization:\n",
    "  (a) L1 on off-diagonal (implemented on sigmoid(adj_logits) for meaningful sparsity pressure)\n",
    "  (b) penalty for deviation from A_prior (MSE on adjacency)\n",
    "- Final adjacency:\n",
    "  A_final = alpha * A_prior + (1 - alpha) * A_learned\n",
    "  alpha is fixed or learned (clipped), controlled by CFG\n",
    "- Temporal block:\n",
    "  Conv (dilated) -> Attention pooling over time\n",
    "\n",
    "New (per your comment):\n",
    "- We DO NOT use mean/median thresholds as the default \"final check\".\n",
    "- Step 10 now stores per-fold artifacts (models + thresholds + val preds).\n",
    "- Step 11 implements 3 post-CV holdout checks WITHOUT any extra refit:\n",
    "    1) LAST fold model + LAST fold thresholds\n",
    "    2) BEST-VAL fold model + BEST-VAL thresholds\n",
    "    3) LAST fold model + GLOBAL thresholds fitted on concatenated fold-VAL predictions\n",
    "- We fix the variable naming: m_trade_last / m_dir_last are explicitly saved in Step 10.\n",
    "- Step 12 is production-fit: train on CV(90%) with final val window, then evaluate on FINAL holdout(10%).\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Step 0 — Imports + reproducibility + config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cpu\n",
      "EDGE_LIST: ['ADA->BTC', 'ADA->ETH', 'BTC->ADA', 'BTC->ETH', 'ETH->ADA', 'ETH->BTC', 'ADA->ADA', 'BTC->BTC', 'ETH->ETH']\n",
      "EDGE_INDEX: [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1], [0, 0], [1, 1], [2, 2]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, roc_auc_score\n",
    "\n",
    "\n",
    "def seed_everything(seed: int = 1234) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "seed_everything(100)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "\n",
    "torch.set_num_threads(max(1, os.cpu_count() or 4))\n",
    "\n",
    "CFG: Dict[str, Any] = {\n",
    "    # data\n",
    "    \"freq\": \"1min\",\n",
    "    \"data_dir\": Path(\"../dataset\"),\n",
    "    \"final_test_frac\": 0.10,\n",
    "\n",
    "    # order book\n",
    "    \"book_levels\": 15,\n",
    "    \"top_levels\": 5,\n",
    "    \"near_levels\": 5,\n",
    "\n",
    "    # walk-forward windows (in sample-space)\n",
    "    \"train_min_frac\": 0.50,\n",
    "    \"val_window_frac\": 0.10,\n",
    "    \"test_window_frac\": 0.10,\n",
    "    \"step_window_frac\": 0.10,\n",
    "\n",
    "    # scaling\n",
    "    \"max_abs_feat\": 10.0,\n",
    "    \"max_abs_edge\": 6.0,\n",
    "\n",
    "    # correlations / graph\n",
    "    \"corr_windows\": [6 * 5, 12 * 5, 24 * 5, 48 * 5, 84 * 5],  # 30m,1h,2h,4h,7h\n",
    "    \"corr_lags\": [0, 1, 2, 5],  # lead-lag (no leakage)\n",
    "    \"edges_mode\": \"all_pairs\",  # \"manual\" | \"all_pairs\"\n",
    "    \"edges\": [(\"ADA\", \"BTC\"), (\"ADA\", \"ETH\"), (\"ETH\", \"BTC\")],  # used if edges_mode=\"manual\"\n",
    "    \"add_self_loops\": True,\n",
    "    \"edge_transform\": \"fisher\",  # \"none\" | \"fisher\"\n",
    "    \"edge_scale\": True,\n",
    "    \"edge_dropout\": 0.10,\n",
    "\n",
    "    # triple-barrier\n",
    "    \"tb_horizon\": 1 * 30,\n",
    "    \"lookback\": 4 * 12 * 5,\n",
    "    \"tb_pt_mult\": 1.2,\n",
    "    \"tb_sl_mult\": 1.1,\n",
    "    \"tb_min_barrier\": 0.001,\n",
    "    \"tb_max_barrier\": 0.006,\n",
    "\n",
    "    # training\n",
    "    \"batch_size\": 128,\n",
    "    \"epochs\": 20,\n",
    "    \"lr\": 3e-4,\n",
    "    \"weight_decay\": 5e-4,\n",
    "    \"grad_clip\": 1.0,\n",
    "    \"dropout\": 0.15,\n",
    "\n",
    "    # stability tricks\n",
    "    \"label_smoothing\": 0.02,\n",
    "    \"use_weighted_sampler\": True,\n",
    "    \"use_onecycle\": True,\n",
    "\n",
    "    # model dims\n",
    "    \"hidden\": 128,\n",
    "    \"gnn_layers\": 3,\n",
    "\n",
    "    # --- Temporal (Conv -> AttnPool)\n",
    "    \"tcn_channels\": 128,\n",
    "    \"tcn_layers\": 3,\n",
    "    \"tcn_kernel\": 2,\n",
    "    \"tcn_dropout\": 0.20,\n",
    "    \"tcn_causal\": True,\n",
    "\n",
    "    \"attn_pool_hidden\": 128,\n",
    "    \"attn_pool_dropout\": 0.10,\n",
    "\n",
    "    # --- Learnable adjacency (MTGNN-style)\n",
    "    # A_learned options:\n",
    "    #   \"emb\": A = softmax((E1 @ E2^T)/temp)\n",
    "    #   \"matrix\": A = softmax(A_logits/temp)\n",
    "    \"adj_mode\": \"emb\",\n",
    "    \"adj_emb_dim\": 8,\n",
    "    \"adj_temperature\": 1.0,\n",
    "\n",
    "    # A_prior from edge_attr (last timestep of the sequence)\n",
    "    \"prior_use_abs\": False,       # if True: use abs(mean(edge_attr)) for weights\n",
    "    \"prior_diag_boost\": 1.0,      # ensure diag >= this before row-normalization\n",
    "    \"prior_row_normalize\": True,\n",
    "\n",
    "    # mixing alpha\n",
    "    \"alpha_mode\": \"learned\",      # \"fixed\" | \"learned\"\n",
    "    \"adj_alpha\": 0.50,            # used if alpha_mode=\"fixed\"\n",
    "    \"adj_alpha_min\": 0.05,        # clamp if learned\n",
    "    \"adj_alpha_max\": 0.95,\n",
    "\n",
    "    # adjacency regularization\n",
    "    \"adj_l1_lambda\": 1e-3,\n",
    "    \"adj_prior_lambda\": 1e-2,\n",
    "\n",
    "    # trading eval\n",
    "    \"cost_bps\": 1.0,\n",
    "\n",
    "    # threshold sweep grids (val only)\n",
    "    \"thr_trade_grid\": [0.50, 0.55, 0.60, 0.65, 0.70, 0.75],\n",
    "    \"thr_dir_grid\":   [0.50, 0.55, 0.60, 0.65, 0.70],\n",
    "\n",
    "    # min trades constraints\n",
    "    \"eval_min_trades\": 50,\n",
    "\n",
    "    # anti-overtrading threshold selection\n",
    "    \"max_trade_rate_val\": 0.65,\n",
    "    \"trade_rate_penalty\": 0.10,\n",
    "    \"thr_objective\": \"pnl_sum\",  # \"pnl_sum\" | \"pnl_sharpe\" | \"pnl_per_trade\"\n",
    "\n",
    "    # dynamic quantile thresholds for thr_trade\n",
    "    \"proxy_target_trades\": [50, 100, 200],\n",
    "}\n",
    "\n",
    "ASSETS = [\"ADA\", \"BTC\", \"ETH\"]\n",
    "ASSET2IDX = {a: i for i, a in enumerate(ASSETS)}\n",
    "TARGET_ASSET = \"ETH\"\n",
    "TARGET_NODE = ASSET2IDX[TARGET_ASSET]\n",
    "\n",
    "\n",
    "def build_edge_list(cfg: Dict[str, Any], assets: List[str]) -> List[Tuple[str, str]]:\n",
    "    mode = str(cfg.get(\"edges_mode\", \"manual\"))\n",
    "    if mode == \"manual\":\n",
    "        edges = list(cfg[\"edges\"])\n",
    "    elif mode == \"all_pairs\":\n",
    "        edges = [(s, t) for s in assets for t in assets if s != t]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown edges_mode={mode}\")\n",
    "\n",
    "    if bool(cfg.get(\"add_self_loops\", True)):\n",
    "        edges = edges + [(a, a) for a in assets]\n",
    "    return edges\n",
    "\n",
    "\n",
    "EDGE_LIST = build_edge_list(CFG, ASSETS)\n",
    "EDGE_NAMES = [f\"{s}->{t}\" for s, t in EDGE_LIST]\n",
    "EDGE_INDEX = torch.tensor([[ASSET2IDX[s], ASSET2IDX[t]] for (s, t) in EDGE_LIST], dtype=torch.long)\n",
    "\n",
    "print(\"EDGE_LIST:\", EDGE_NAMES)\n",
    "print(\"EDGE_INDEX:\", EDGE_INDEX.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Step 1 — Load data + log returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded df: (12831, 106)\n",
      "Columns example: ['timestamp', 'ADA', 'spread_ADA', 'buys_ADA', 'sells_ADA', 'bids_vol_ADA_0', 'bids_vol_ADA_1', 'bids_vol_ADA_2', 'bids_vol_ADA_3', 'bids_vol_ADA_4', 'bids_vol_ADA_5', 'bids_vol_ADA_6', 'bids_vol_ADA_7', 'bids_vol_ADA_8', 'bids_vol_ADA_9', 'bids_vol_ADA_10', 'bids_vol_ADA_11', 'bids_vol_ADA_12', 'bids_vol_ADA_13', 'bids_vol_ADA_14']\n",
      "Time range: 2021-04-07 11:34:00+00:00 -> 2021-04-16 10:15:00+00:00\n",
      "                  timestamp      ADA  spread_ADA      buys_ADA      sells_ADA  \\\n",
      "0 2021-04-07 11:34:00+00:00  1.16205      0.0001  56936.467913  258248.957367   \n",
      "1 2021-04-07 11:35:00+00:00  1.16800      0.0022  56491.336799   78665.286640   \n",
      "\n",
      "   bids_vol_ADA_0  bids_vol_ADA_1  bids_vol_ADA_2  bids_vol_ADA_3  \\\n",
      "0      876.869995     5984.169922        5.810000       18.240000   \n",
      "1    33769.671875    23137.169922      550.299988      550.299988   \n",
      "\n",
      "   bids_vol_ADA_4  ...  asks_vol_ETH_8  asks_vol_ETH_9  asks_vol_ETH_10  \\\n",
      "0    19844.640625  ...      373.700012      196.699997      2059.709961   \n",
      "1    19012.320312  ...     3873.709961     1954.630005       197.039993   \n",
      "\n",
      "   asks_vol_ETH_11  asks_vol_ETH_12  asks_vol_ETH_13  asks_vol_ETH_14  \\\n",
      "0      3874.989990      5901.209961       178.289993     28512.160156   \n",
      "1     12661.990234     20006.970703     28562.310547      3874.379883   \n",
      "\n",
      "     lr_ADA    lr_BTC    lr_ETH  \n",
      "0  0.000000  0.000000  0.000000  \n",
      "1  0.005107  0.000937  0.001931  \n",
      "\n",
      "[2 rows x 106 columns]\n"
     ]
    }
   ],
   "source": [
    "def load_asset(asset: str, freq: str, data_dir: Path, book_levels: int, part: Tuple[int, int] = (0, 80)) -> pd.DataFrame:\n",
    "    path = data_dir / f\"{asset}_{freq}.csv\"\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.iloc[int(len(df) * part[0] / 100): int(len(df) * part[1] / 100)]\n",
    "\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"system_time\"]).dt.round(\"min\")\n",
    "    df = df.sort_values(\"timestamp\").set_index(\"timestamp\")\n",
    "\n",
    "    bid_cols = [f\"bids_notional_{i}\" for i in range(book_levels)]\n",
    "    ask_cols = [f\"asks_notional_{i}\" for i in range(book_levels)]\n",
    "\n",
    "    needed = [\"midpoint\", \"spread\", \"buys\", \"sells\"] + bid_cols + ask_cols\n",
    "    missing = [c for c in needed if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"{asset}: missing columns in CSV: {missing[:10]}{'...' if len(missing) > 10 else ''}\")\n",
    "\n",
    "    return df[needed]\n",
    "\n",
    "\n",
    "def load_all_assets() -> pd.DataFrame:\n",
    "    freq = CFG[\"freq\"]\n",
    "    data_dir = CFG[\"data_dir\"]\n",
    "    book_levels = CFG[\"book_levels\"]\n",
    "\n",
    "    def rename_cols(df_one: pd.DataFrame, asset: str) -> pd.DataFrame:\n",
    "        rename_map = {\n",
    "            \"midpoint\": asset,\n",
    "            \"buys\": f\"buys_{asset}\",\n",
    "            \"sells\": f\"sells_{asset}\",\n",
    "            \"spread\": f\"spread_{asset}\",\n",
    "        }\n",
    "        for i in range(book_levels):\n",
    "            rename_map[f\"bids_notional_{i}\"] = f\"bids_vol_{asset}_{i}\"\n",
    "            rename_map[f\"asks_notional_{i}\"] = f\"asks_vol_{asset}_{i}\"\n",
    "        return df_one.rename(columns=rename_map)\n",
    "\n",
    "    df_ada = rename_cols(load_asset(\"ADA\", freq, data_dir, book_levels, part=(0, 75)), \"ADA\")\n",
    "    df_btc = rename_cols(load_asset(\"BTC\", freq, data_dir, book_levels, part=(0, 75)), \"BTC\")\n",
    "    df_eth = rename_cols(load_asset(\"ETH\", freq, data_dir, book_levels, part=(0, 75)), \"ETH\")\n",
    "\n",
    "    df = df_ada.join(df_btc).join(df_eth).reset_index()\n",
    "    return df\n",
    "\n",
    "\n",
    "df = load_all_assets()\n",
    "for a in ASSETS:\n",
    "    df[f\"lr_{a}\"] = np.log(df[a]).diff().fillna(0.0)\n",
    "\n",
    "print(\"Loaded df:\", df.shape)\n",
    "print(\"Columns example:\", df.columns[:20].tolist())\n",
    "print(\"Time range:\", df[\"timestamp\"].min(), \"->\", df[\"timestamp\"].max())\n",
    "print(df.head(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Step 2 — Multi-window correlations → edge features (T,E,D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edge_feat shape: (12831, 9, 20) (T,E,edge_dim)\n",
      "edge_dim = 20  = windows * lags = 20\n",
      "Edge names: ['ADA->BTC', 'ADA->ETH', 'BTC->ADA', 'BTC->ETH', 'ETH->ADA', 'ETH->BTC', 'ADA->ADA', 'BTC->BTC', 'ETH->ETH']\n",
      "edge_feat sample [t=100, first 3 edges]:\n",
      " [[ 6.7925054e-01  7.8185719e-01  8.3443433e-01  8.3443433e-01\n",
      "   8.3443433e-01  3.4026209e-01  1.4996846e-01  1.6863135e-01\n",
      "   1.6863135e-01  1.6863135e-01 -2.9266590e-02 -1.5999632e-01\n",
      "  -2.5908518e-01 -2.5908518e-01 -2.5908518e-01  1.4277337e-01\n",
      "  -1.0870378e-02  5.0888385e-04  5.0888385e-04  5.0888385e-04]\n",
      " [ 6.3383293e-01  6.9067067e-01  8.7368768e-01  8.7368768e-01\n",
      "   8.7368768e-01  3.4575835e-01  1.5627505e-01  2.0534241e-01\n",
      "   2.0534241e-01  2.0534241e-01  6.9384493e-02 -1.1163518e-01\n",
      "  -1.7551416e-01 -1.7551416e-01 -1.7551416e-01 -1.6881377e-01\n",
      "  -1.0781832e-01 -6.3380465e-02 -6.3380465e-02 -6.3380465e-02]\n",
      " [ 6.7925054e-01  7.8185719e-01  8.3443433e-01  8.3443433e-01\n",
      "   8.3443433e-01 -4.8168253e-02 -1.6241662e-01 -1.6193953e-01\n",
      "  -1.6193953e-01 -1.6193953e-01 -2.1278685e-01  4.6374347e-02\n",
      "  -1.7361922e-02 -1.7361922e-02 -1.7361922e-02  1.3684873e-01\n",
      "   3.6141057e-02  6.9459870e-02  6.9459870e-02  6.9459870e-02]]\n",
      "edge_feat stats: mean= 0.4511896073818207 std= 0.500860333442688\n"
     ]
    }
   ],
   "source": [
    "def _fisher_z(x: np.ndarray, eps: float = 1e-6) -> np.ndarray:\n",
    "    x = np.clip(x, -0.999, 0.999)\n",
    "    return 0.5 * np.log((1.0 + x + eps) / (1.0 - x + eps))\n",
    "\n",
    "\n",
    "def build_corr_array(\n",
    "    df_: pd.DataFrame,\n",
    "    corr_windows: List[int],\n",
    "    edges: List[Tuple[str, str]],\n",
    "    lags: List[int],\n",
    "    transform: str = \"fisher\",\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Edge features per time:\n",
    "      for edge s->t:\n",
    "        for lag in lags:\n",
    "          corr(lr_s.shift(lag), lr_t) over rolling window\n",
    "    No leakage: shift(lag>0) uses past of source.\n",
    "    Self-loop edges a->a: constant 1.0.\n",
    "    \"\"\"\n",
    "    T_ = len(df_)\n",
    "    E_ = len(edges)\n",
    "    W_ = len(corr_windows)\n",
    "    Lg = len(lags)\n",
    "    out = np.zeros((T_, E_, W_ * Lg), dtype=np.float32)\n",
    "\n",
    "    lr_map = {a: df_[f\"lr_{a}\"].astype(float) for a in ASSETS}\n",
    "\n",
    "    for ei, (s, t) in enumerate(edges):\n",
    "        if s == t:\n",
    "            out[:, ei, :] = 1.0\n",
    "            continue\n",
    "\n",
    "        src0 = lr_map[s]\n",
    "        dst0 = lr_map[t]\n",
    "\n",
    "        feat_idx = 0\n",
    "        for lag in lags:\n",
    "            src = src0.shift(int(lag)) if int(lag) > 0 else src0\n",
    "\n",
    "            for w in corr_windows:\n",
    "                r = src.rolling(int(w), min_periods=1).corr(dst0)\n",
    "                r = np.nan_to_num(r.to_numpy(dtype=np.float32), nan=0.0, posinf=0.0, neginf=0.0)\n",
    "                if transform == \"fisher\":\n",
    "                    r = _fisher_z(r).astype(np.float32)\n",
    "                out[:, ei, feat_idx] = r\n",
    "                feat_idx += 1\n",
    "\n",
    "    return out.astype(np.float32)\n",
    "\n",
    "\n",
    "edge_feat = build_corr_array(\n",
    "    df,\n",
    "    CFG[\"corr_windows\"],\n",
    "    EDGE_LIST,\n",
    "    CFG[\"corr_lags\"],\n",
    "    transform=str(CFG.get(\"edge_transform\", \"fisher\")),\n",
    ")\n",
    "\n",
    "print(\"edge_feat shape:\", edge_feat.shape, \"(T,E,edge_dim)\")\n",
    "print(\"edge_dim =\", edge_feat.shape[-1], \" = windows * lags =\", len(CFG[\"corr_windows\"]) * len(CFG[\"corr_lags\"]))\n",
    "print(\"Edge names:\", EDGE_NAMES)\n",
    "print(\"edge_feat sample [t=100, first 3 edges]:\\n\", edge_feat[100, :3, :])\n",
    "print(\"edge_feat stats: mean=\", float(edge_feat.mean()), \"std=\", float(edge_feat.std()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Step 3 — Triple-barrier labels → two-stage labels + exit_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TB dist [down,flat,up]: [2875 7413 2543]\n",
      "Trade ratio (true): 0.42225859247135844\n"
     ]
    }
   ],
   "source": [
    "def triple_barrier_labels_from_lr(\n",
    "    lr: pd.Series,\n",
    "    horizon: int,\n",
    "    vol_window: int,\n",
    "    pt_mult: float,\n",
    "    sl_mult: float,\n",
    "    min_barrier: float,\n",
    "    max_barrier: float,\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      y_tb: {0=down, 1=flat/no-trade, 2=up}\n",
    "      exit_ret: realized log-return to exit (tp/sl/timeout)\n",
    "      exit_t: exit index\n",
    "      thr: barrier per t (float array, len T)\n",
    "    No leakage: vol is shift(1).\n",
    "    \"\"\"\n",
    "    lr = lr.astype(float).copy()\n",
    "    T = len(lr)\n",
    "\n",
    "    vol = lr.rolling(vol_window, min_periods=max(10, vol_window // 10)).std().shift(1)\n",
    "    thr = (vol * np.sqrt(horizon)).clip(lower=min_barrier, upper=max_barrier)\n",
    "\n",
    "    y = np.ones(T, dtype=np.int64)\n",
    "    exit_ret = np.zeros(T, dtype=np.float32)\n",
    "    exit_t = np.arange(T, dtype=np.int64)\n",
    "\n",
    "    lr_np = lr.fillna(0.0).to_numpy(dtype=np.float64)\n",
    "    thr_np = thr.fillna(min_barrier).to_numpy(dtype=np.float64)\n",
    "\n",
    "    for t in range(T - horizon - 1):\n",
    "        up = pt_mult * thr_np[t]\n",
    "        dn = -sl_mult * thr_np[t]\n",
    "\n",
    "        cum = 0.0\n",
    "        hit = 1\n",
    "        et = t + horizon\n",
    "        er = 0.0\n",
    "\n",
    "        for dt in range(1, horizon + 1):\n",
    "            cum += lr_np[t + dt]\n",
    "            if cum >= up:\n",
    "                hit, et, er = 2, t + dt, cum\n",
    "                break\n",
    "            if cum <= dn:\n",
    "                hit, et, er = 0, t + dt, cum\n",
    "                break\n",
    "\n",
    "        if hit == 1:\n",
    "            er = float(np.sum(lr_np[t + 1: t + horizon + 1]))\n",
    "            et = t + horizon\n",
    "\n",
    "        y[t] = hit\n",
    "        exit_ret[t] = er\n",
    "        exit_t[t] = et\n",
    "\n",
    "    return y, exit_ret, exit_t, thr_np\n",
    "\n",
    "\n",
    "y_tb, exit_ret, exit_t, tb_thr = triple_barrier_labels_from_lr(\n",
    "    df[\"lr_ETH\"],\n",
    "    horizon=CFG[\"tb_horizon\"],\n",
    "    vol_window=CFG[\"lookback\"],\n",
    "    pt_mult=CFG[\"tb_pt_mult\"],\n",
    "    sl_mult=CFG[\"tb_sl_mult\"],\n",
    "    min_barrier=CFG[\"tb_min_barrier\"],\n",
    "    max_barrier=CFG[\"tb_max_barrier\"],\n",
    ")\n",
    "\n",
    "# two-stage labels\n",
    "y_trade = (y_tb != 1).astype(np.int64)  # 1=trade, 0=no-trade\n",
    "y_dir = (y_tb == 2).astype(np.int64)    # 1=up, 0=down (meaningful only when y_trade==1)\n",
    "\n",
    "dist = np.bincount(y_tb, minlength=3)\n",
    "print(\"TB dist [down,flat,up]:\", dist)\n",
    "print(\"Trade ratio (true):\", float(y_trade.mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Step 4 — Build node tensor (T,N,F) + sample_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_node_raw: (12831, 3, 15) edge_feat: (12831, 9, 20)\n",
      "node_feat_names: ['lr', 'spread', 'log_buys', 'log_sells', 'ofi', 'DI_15', 'DI_L0', 'DI_L1', 'DI_L2', 'DI_L3', 'DI_L4', 'near_ratio_bid', 'near_ratio_ask', 'di_near', 'di_far']\n",
      "n_samples: 12561 | t range: 239 -> 12799\n",
      "Feature stats (TARGET asset, lr): mean= 1.5748046280350536e-05 std= 0.0010532913729548454\n"
     ]
    }
   ],
   "source": [
    "EPS = 1e-6\n",
    "\n",
    "\n",
    "def safe_log1p(x: np.ndarray) -> np.ndarray:\n",
    "    return np.log1p(np.maximum(x, 0.0))\n",
    "\n",
    "\n",
    "def build_node_tensor(df_: pd.DataFrame) -> Tuple[np.ndarray, List[str]]:\n",
    "    \"\"\"\n",
    "    Features per asset:\n",
    "      lr, spread,\n",
    "      log_buys, log_sells, ofi,\n",
    "      DI_15,\n",
    "      DI_L0..DI_L4,\n",
    "      near_ratio_bid, near_ratio_ask,\n",
    "      di_near, di_far\n",
    "    \"\"\"\n",
    "    book_levels = CFG[\"book_levels\"]\n",
    "    top_k = CFG[\"top_levels\"]\n",
    "    near_k = CFG[\"near_levels\"]\n",
    "\n",
    "    if near_k >= book_levels:\n",
    "        raise ValueError(\"CFG['near_levels'] must be < CFG['book_levels']\")\n",
    "\n",
    "    feat_names = [\n",
    "        \"lr\", \"spread\",\n",
    "        \"log_buys\", \"log_sells\", \"ofi\",\n",
    "        \"DI_15\",\n",
    "        \"DI_L0\", \"DI_L1\", \"DI_L2\", \"DI_L3\", \"DI_L4\",\n",
    "        \"near_ratio_bid\", \"near_ratio_ask\",\n",
    "        \"di_near\", \"di_far\",\n",
    "    ]\n",
    "\n",
    "    feats_all = []\n",
    "    for a in ASSETS:\n",
    "        lr = df_[f\"lr_{a}\"].values.astype(np.float32)\n",
    "        spread = df_[f\"spread_{a}\"].values.astype(np.float32)\n",
    "\n",
    "        buys = df_[f\"buys_{a}\"].values.astype(np.float32)\n",
    "        sells = df_[f\"sells_{a}\"].values.astype(np.float32)\n",
    "\n",
    "        log_buys = safe_log1p(buys).astype(np.float32)\n",
    "        log_sells = safe_log1p(sells).astype(np.float32)\n",
    "\n",
    "        ofi = ((buys - sells) / (buys + sells + EPS)).astype(np.float32)\n",
    "\n",
    "        bids_lvls = np.stack([df_[f\"bids_vol_{a}_{i}\"].values.astype(np.float32) for i in range(book_levels)], axis=1)\n",
    "        asks_lvls = np.stack([df_[f\"asks_vol_{a}_{i}\"].values.astype(np.float32) for i in range(book_levels)], axis=1)\n",
    "\n",
    "        bid_sum = bids_lvls.sum(axis=1)\n",
    "        ask_sum = asks_lvls.sum(axis=1)\n",
    "        di_15 = ((bid_sum - ask_sum) / (bid_sum + ask_sum + EPS)).astype(np.float32)\n",
    "\n",
    "        di_levels = []\n",
    "        for i in range(top_k):\n",
    "            b = bids_lvls[:, i]\n",
    "            s = asks_lvls[:, i]\n",
    "            di_levels.append(((b - s) / (b + s + EPS)).astype(np.float32))\n",
    "        di_l0_4 = np.stack(di_levels, axis=1)  # (T,5)\n",
    "\n",
    "        bid_near = bids_lvls[:, :near_k].sum(axis=1)\n",
    "        ask_near = asks_lvls[:, :near_k].sum(axis=1)\n",
    "        bid_far = bids_lvls[:, near_k:].sum(axis=1)\n",
    "        ask_far = asks_lvls[:, near_k:].sum(axis=1)\n",
    "\n",
    "        near_ratio_bid = (bid_near / (bid_far + EPS)).astype(np.float32)\n",
    "        near_ratio_ask = (ask_near / (ask_far + EPS)).astype(np.float32)\n",
    "\n",
    "        di_near = ((bid_near - ask_near) / (bid_near + ask_near + EPS)).astype(np.float32)\n",
    "        di_far = ((bid_far - ask_far) / (bid_far + ask_far + EPS)).astype(np.float32)\n",
    "\n",
    "        Xa = np.column_stack([\n",
    "            lr, spread,\n",
    "            log_buys, log_sells, ofi,\n",
    "            di_15,\n",
    "            di_l0_4[:, 0], di_l0_4[:, 1], di_l0_4[:, 2], di_l0_4[:, 3], di_l0_4[:, 4],\n",
    "            near_ratio_bid, near_ratio_ask,\n",
    "            di_near, di_far,\n",
    "        ]).astype(np.float32)\n",
    "\n",
    "        feats_all.append(Xa)\n",
    "\n",
    "    X = np.stack(feats_all, axis=1).astype(np.float32)  # (T,N,F)\n",
    "    return X, feat_names\n",
    "\n",
    "\n",
    "X_node_raw, node_feat_names = build_node_tensor(df)\n",
    "T = len(df)\n",
    "L = CFG[\"lookback\"]\n",
    "H = CFG[\"tb_horizon\"]\n",
    "\n",
    "t_min = L - 1\n",
    "t_max = T - H - 2\n",
    "sample_t = np.arange(t_min, t_max + 1)\n",
    "n_samples = len(sample_t)\n",
    "\n",
    "print(\"X_node_raw:\", X_node_raw.shape, \"edge_feat:\", edge_feat.shape)\n",
    "print(\"node_feat_names:\", node_feat_names)\n",
    "print(\"n_samples:\", n_samples, \"| t range:\", int(sample_t[0]), \"->\", int(sample_t[-1]))\n",
    "print(\n",
    "    \"Feature stats (TARGET asset, lr):\",\n",
    "    \"mean=\", float(X_node_raw[:, TARGET_NODE, node_feat_names.index(\"lr\")].mean()),\n",
    "    \"std=\", float(X_node_raw[:, TARGET_NODE, node_feat_names.index(\"lr\")].std()),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Step 5 — Final holdout split + walk-forward splits (CV-part only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holdout split:\n",
      "  n_samples total: 12561\n",
      "  n_samples CV   : 11305 (90.0%)\n",
      "  n_samples FINAL: 1256 (10.0%)\n",
      "  CV range   : 0 11304\n",
      "  FINAL range: 11305 12560\n",
      "\n",
      "Walk-forward folds: 4\n",
      "  fold 1: train=5652 | val=1130 | test=1130\n",
      "  fold 2: train=6782 | val=1130 | test=1130\n",
      "  fold 3: train=7912 | val=1130 | test=1130\n",
      "  fold 4: train=9042 | val=1130 | test=1130\n"
     ]
    }
   ],
   "source": [
    "def make_final_holdout_split(n_samples_: int, final_test_frac: float) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    if not (0.0 < final_test_frac < 0.5):\n",
    "        raise ValueError(\"final_test_frac should be in (0, 0.5)\")\n",
    "    n_final = max(1, int(round(final_test_frac * n_samples_)))\n",
    "    n_cv = n_samples_ - n_final\n",
    "    if n_cv <= 50:\n",
    "        raise ValueError(\"Too few samples left for CV after holdout split.\")\n",
    "    idx_cv = np.arange(0, n_cv, dtype=np.int64)\n",
    "    idx_final = np.arange(n_cv, n_samples_, dtype=np.int64)\n",
    "    return idx_cv, idx_final\n",
    "\n",
    "\n",
    "def make_walk_forward_splits(\n",
    "    n_samples_: int,\n",
    "    train_min_frac: float,\n",
    "    val_window_frac: float,\n",
    "    test_window_frac: float,\n",
    "    step_window_frac: float,\n",
    ") -> List[Tuple[np.ndarray, np.ndarray, np.ndarray]]:\n",
    "    train_min = int(train_min_frac * n_samples_)\n",
    "    val_w = max(1, int(val_window_frac * n_samples_))\n",
    "    test_w = max(1, int(test_window_frac * n_samples_))\n",
    "    step_w = max(1, int(step_window_frac * n_samples_))\n",
    "\n",
    "    splits = []\n",
    "    start = train_min\n",
    "    while True:\n",
    "        tr_end = start\n",
    "        va_end = tr_end + val_w\n",
    "        te_end = va_end + test_w\n",
    "        if te_end > n_samples_:\n",
    "            break\n",
    "\n",
    "        idx_train = np.arange(0, tr_end, dtype=np.int64)\n",
    "        idx_val = np.arange(tr_end, va_end, dtype=np.int64)\n",
    "        idx_test = np.arange(va_end, te_end, dtype=np.int64)\n",
    "        splits.append((idx_train, idx_val, idx_test))\n",
    "\n",
    "        start += step_w\n",
    "\n",
    "    return splits\n",
    "\n",
    "\n",
    "idx_cv_all, idx_final_test = make_final_holdout_split(n_samples, CFG[\"final_test_frac\"])\n",
    "n_samples_cv = len(idx_cv_all)\n",
    "n_samples_final = len(idx_final_test)\n",
    "\n",
    "print(\"Holdout split:\")\n",
    "print(f\"  n_samples total: {n_samples}\")\n",
    "print(f\"  n_samples CV   : {n_samples_cv} ({100 * n_samples_cv / n_samples:.1f}%)\")\n",
    "print(f\"  n_samples FINAL: {n_samples_final} ({100 * n_samples_final / n_samples:.1f}%)\")\n",
    "print(\"  CV range   :\", int(idx_cv_all[0]), int(idx_cv_all[-1]))\n",
    "print(\"  FINAL range:\", int(idx_final_test[0]), int(idx_final_test[-1]))\n",
    "\n",
    "walk_splits = make_walk_forward_splits(\n",
    "    n_samples_=n_samples_cv,\n",
    "    train_min_frac=CFG[\"train_min_frac\"],\n",
    "    val_window_frac=CFG[\"val_window_frac\"],\n",
    "    test_window_frac=CFG[\"test_window_frac\"],\n",
    "    step_window_frac=CFG[\"step_window_frac\"],\n",
    ")\n",
    "\n",
    "print(\"\\nWalk-forward folds:\", len(walk_splits))\n",
    "for i, (a, b, c) in enumerate(walk_splits, 1):\n",
    "    print(f\"  fold {i}: train={len(a)} | val={len(b)} | test={len(c)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Step 6 — Dataset + scaling (train-only) + helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LobGraphSequenceDataset2Stage(Dataset):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      x_seq: (L,N,F)\n",
    "      e_seq: (L,E,edge_dim)\n",
    "      y_trade: scalar\n",
    "      y_dir: scalar\n",
    "      exit_ret: scalar\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        X_node: np.ndarray,\n",
    "        E_feat: np.ndarray,\n",
    "        y_trade_arr: np.ndarray,\n",
    "        y_dir_arr: np.ndarray,\n",
    "        exit_ret_arr: np.ndarray,\n",
    "        sample_t_: np.ndarray,\n",
    "        indices: np.ndarray,\n",
    "        lookback: int,\n",
    "    ):\n",
    "        self.X_node = X_node\n",
    "        self.E_feat = E_feat\n",
    "        self.y_trade = y_trade_arr\n",
    "        self.y_dir = y_dir_arr\n",
    "        self.exit_ret = exit_ret_arr\n",
    "        self.sample_t = sample_t_\n",
    "        self.indices = indices.astype(np.int64)\n",
    "        self.L = int(lookback)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return int(len(self.indices))\n",
    "\n",
    "    def __getitem__(self, i: int):\n",
    "        sidx = int(self.indices[i])\n",
    "        t = int(self.sample_t[sidx])\n",
    "        t0 = t - self.L + 1\n",
    "\n",
    "        x_seq = self.X_node[t0:t + 1]  # (L,N,F)\n",
    "        e_seq = self.E_feat[t0:t + 1]  # (L,E,D)\n",
    "\n",
    "        yt = int(self.y_trade[t])\n",
    "        yd = int(self.y_dir[t])\n",
    "        er = float(self.exit_ret[t])\n",
    "\n",
    "        return (\n",
    "            torch.from_numpy(x_seq),\n",
    "            torch.from_numpy(e_seq),\n",
    "            torch.tensor(yt, dtype=torch.long),\n",
    "            torch.tensor(yd, dtype=torch.long),\n",
    "            torch.tensor(er, dtype=torch.float32),\n",
    "        )\n",
    "\n",
    "\n",
    "def collate_fn_2stage(batch):\n",
    "    xs, es, yts, yds, ers = zip(*batch)\n",
    "    return (\n",
    "        torch.stack(xs, 0),   # (B,L,N,F)\n",
    "        torch.stack(es, 0),   # (B,L,E,D)\n",
    "        torch.stack(yts, 0),  # (B,)\n",
    "        torch.stack(yds, 0),  # (B,)\n",
    "        torch.stack(ers, 0),  # (B,)\n",
    "    )\n",
    "\n",
    "\n",
    "def fit_scale_nodes_train_only(\n",
    "    X_node_raw_: np.ndarray,\n",
    "    sample_t_: np.ndarray,\n",
    "    idx_train: np.ndarray,\n",
    "    max_abs: float = 10.0\n",
    ") -> Tuple[np.ndarray, RobustScaler]:\n",
    "    last_train_t = int(sample_t_[int(idx_train[-1])])\n",
    "    train_time_mask = np.arange(0, last_train_t + 1)\n",
    "\n",
    "    X_train_time = X_node_raw_[train_time_mask]  # (Ttr,N,F)\n",
    "    _, _, Fdim = X_train_time.shape\n",
    "\n",
    "    scaler = RobustScaler(with_centering=True, with_scaling=True, quantile_range=(5.0, 95.0))\n",
    "    scaler.fit(X_train_time.reshape(-1, Fdim))\n",
    "\n",
    "    X_scaled = scaler.transform(X_node_raw_.reshape(-1, Fdim)).reshape(X_node_raw_.shape).astype(np.float32)\n",
    "    X_scaled = np.clip(X_scaled, -max_abs, max_abs).astype(np.float32)\n",
    "    X_scaled = np.nan_to_num(X_scaled, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n",
    "    return X_scaled, scaler\n",
    "\n",
    "\n",
    "def fit_scale_edges_train_only(\n",
    "    E_raw_: np.ndarray,\n",
    "    sample_t_: np.ndarray,\n",
    "    idx_train: np.ndarray,\n",
    "    max_abs: float = 6.0\n",
    ") -> Tuple[np.ndarray, RobustScaler]:\n",
    "    \"\"\"\n",
    "    Robust-scale edge features per fold (train timeline only).\n",
    "    Fisher-transformed correlations can be heavy-tailed.\n",
    "    \"\"\"\n",
    "    last_train_t = int(sample_t_[int(idx_train[-1])])\n",
    "    train_time_mask = np.arange(0, last_train_t + 1)\n",
    "\n",
    "    E_train_time = E_raw_[train_time_mask]  # (Ttr,E,D)\n",
    "    _, _, D = E_train_time.shape\n",
    "\n",
    "    scaler = RobustScaler(with_centering=True, with_scaling=True, quantile_range=(5.0, 95.0))\n",
    "    scaler.fit(E_train_time.reshape(-1, D))\n",
    "\n",
    "    E_scaled = scaler.transform(E_raw_.reshape(-1, D)).reshape(E_raw_.shape).astype(np.float32)\n",
    "    E_scaled = np.clip(E_scaled, -max_abs, max_abs).astype(np.float32)\n",
    "    E_scaled = np.nan_to_num(E_scaled, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n",
    "    return E_scaled, scaler\n",
    "\n",
    "\n",
    "def subset_trade_indices(indices: np.ndarray, sample_t_: np.ndarray, y_trade_arr: np.ndarray) -> np.ndarray:\n",
    "    tt = sample_t_[indices]\n",
    "    mask = (y_trade_arr[tt] == 1)\n",
    "    return indices[mask]\n",
    "\n",
    "\n",
    "def split_trade_ratio(indices: np.ndarray, sample_t_: np.ndarray, y_trade_arr: np.ndarray) -> float:\n",
    "    tt = sample_t_[indices]\n",
    "    return float(y_trade_arr[tt].mean()) if len(tt) else float(\"nan\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Step 7 — MTGNN-style model: (Conv -> AttnPool) + learnable adjacency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model sanity logits: torch.Size([2, 2]) | finite: True\n",
      "Aux sanity: {'alpha': 0.5, 'l1_off': 0.33330589532852173, 'mse_prior': 0.0040902262553572655, 'attn_mean': 0.004166667349636555, 'attn_max': 0.005347977392375469}\n"
     ]
    }
   ],
   "source": [
    "def build_adj_prior_from_edge_attr(\n",
    "    edge_attr_last: torch.Tensor,    # (B,E,D)\n",
    "    edge_index: torch.Tensor,        # (E,2) [src,dst]\n",
    "    n_nodes: int,\n",
    "    use_abs: bool = False,\n",
    "    diag_boost: float = 1.0,\n",
    "    row_normalize: bool = True,\n",
    "    eps: float = 1e-8\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Build A_prior (B,N,N) from edge_attr at the last timestep.\n",
    "    Default mapping:\n",
    "      w = sigmoid(mean(edge_attr)) in [0,1]\n",
    "    Then fill A_prior[src,dst] = w, enforce diag >= diag_boost, row-normalize.\n",
    "    \"\"\"\n",
    "    edge_attr_last = torch.nan_to_num(edge_attr_last, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    B, E, D = edge_attr_last.shape\n",
    "    r = edge_attr_last.mean(dim=-1)  # (B,E)\n",
    "    if use_abs:\n",
    "        r = r.abs()\n",
    "\n",
    "    w = torch.sigmoid(r)  # (B,E) in [0,1]\n",
    "\n",
    "    A = torch.zeros((B, n_nodes, n_nodes), device=edge_attr_last.device, dtype=edge_attr_last.dtype)\n",
    "    src = edge_index[:, 0].to(edge_attr_last.device)\n",
    "    dst = edge_index[:, 1].to(edge_attr_last.device)\n",
    "    A[:, src, dst] = w\n",
    "\n",
    "    diag = torch.arange(n_nodes, device=edge_attr_last.device)\n",
    "    A[:, diag, diag] = torch.maximum(A[:, diag, diag], torch.full_like(A[:, diag, diag], float(diag_boost)))\n",
    "\n",
    "    if row_normalize:\n",
    "        A = A / (A.sum(dim=-1, keepdim=True) + eps)\n",
    "\n",
    "    return torch.nan_to_num(A, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "\n",
    "class LearnableAdjacency(nn.Module):\n",
    "    \"\"\"\n",
    "    Produces A_learned (N,N) as row-softmax over logits.\n",
    "    Also returns a \"sparsity proxy\" matrix for L1 regularization (sigmoid(logits)).\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes: int, cfg: Dict[str, Any]):\n",
    "        super().__init__()\n",
    "        self.n = int(n_nodes)\n",
    "        self.mode = str(cfg.get(\"adj_mode\", \"emb\"))\n",
    "        self.temp = float(cfg.get(\"adj_temperature\", 1.0))\n",
    "        self.temp = max(self.temp, 1e-3)\n",
    "\n",
    "        if self.mode == \"matrix\":\n",
    "            self.adj_logits = nn.Parameter(0.01 * torch.randn(self.n, self.n))\n",
    "        elif self.mode == \"emb\":\n",
    "            k = int(cfg.get(\"adj_emb_dim\", 8))\n",
    "            self.E1 = nn.Parameter(0.01 * torch.randn(self.n, k))\n",
    "            self.E2 = nn.Parameter(0.01 * torch.randn(self.n, k))\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown adj_mode={self.mode}\")\n",
    "\n",
    "        alpha_mode = str(cfg.get(\"alpha_mode\", \"fixed\"))\n",
    "        self.alpha_mode = alpha_mode\n",
    "        if self.alpha_mode == \"learned\":\n",
    "            init_alpha = float(cfg.get(\"adj_alpha\", 0.5))\n",
    "            init_alpha = float(np.clip(init_alpha, 1e-3, 1 - 1e-3))\n",
    "            self.alpha_logit = nn.Parameter(torch.tensor(math.log(init_alpha / (1 - init_alpha)), dtype=torch.float32))\n",
    "        else:\n",
    "            self.register_buffer(\"alpha_fixed\", torch.tensor(float(cfg.get(\"adj_alpha\", 0.5)), dtype=torch.float32))\n",
    "\n",
    "        self.alpha_min = float(cfg.get(\"adj_alpha_min\", 0.05))\n",
    "        self.alpha_max = float(cfg.get(\"adj_alpha_max\", 0.95))\n",
    "\n",
    "    def _get_logits(self) -> torch.Tensor:\n",
    "        if self.mode == \"matrix\":\n",
    "            return self.adj_logits\n",
    "        logits = self.E1 @ self.E2.t()  # (N,N)\n",
    "        return logits\n",
    "\n",
    "    def alpha(self) -> torch.Tensor:\n",
    "        if self.alpha_mode == \"learned\":\n",
    "            a = torch.sigmoid(self.alpha_logit)\n",
    "            return torch.clamp(a, min=self.alpha_min, max=self.alpha_max)\n",
    "        return torch.clamp(self.alpha_fixed, min=0.0, max=1.0)\n",
    "\n",
    "    def forward(self) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        logits = self._get_logits() / self.temp  # (N,N)\n",
    "        A = torch.softmax(logits, dim=-1)        # row-stochastic\n",
    "        sparsity_proxy = torch.sigmoid(logits)   # used for L1 on off-diagonal\n",
    "        return A, sparsity_proxy\n",
    "\n",
    "\n",
    "class GraphMixLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple adjacency-based message passing:\n",
    "      m_j = sum_i A[i,j] * h_i\n",
    "      out = GELU(W_self h + W_nei m)\n",
    "      gated residual\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.lin_self = nn.Linear(hidden, hidden)\n",
    "        self.lin_nei = nn.Linear(hidden, hidden)\n",
    "        self.gate = nn.Linear(2 * hidden, hidden)\n",
    "        self.ln = nn.LayerNorm(hidden)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "        for m in [self.lin_self, self.lin_nei, self.gate]:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, h: torch.Tensor, A: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        h: (B,L,N,H)\n",
    "        A: (B,N,N) with A[src,dst]\n",
    "        \"\"\"\n",
    "        h = torch.nan_to_num(h, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        A = torch.nan_to_num(A, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "        m = torch.einsum(\"bij,blih->bljh\", A, h)  # aggregate to dst=j\n",
    "        out = F.gelu(self.lin_self(h) + self.lin_nei(m))\n",
    "        out = self.drop(out)\n",
    "\n",
    "        g = torch.sigmoid(self.gate(torch.cat([h, m], dim=-1)))\n",
    "        y = g * out + (1.0 - g) * h\n",
    "        return torch.nan_to_num(self.ln(y), nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "\n",
    "class CausalConv1d(nn.Module):\n",
    "    def __init__(self, in_ch: int, out_ch: int, kernel_size: int, dilation: int = 1):\n",
    "        super().__init__()\n",
    "        self.kernel_size = int(kernel_size)\n",
    "        self.dilation = int(dilation)\n",
    "        self.conv = nn.Conv1d(in_ch, out_ch, kernel_size=self.kernel_size, dilation=self.dilation)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        pad_left = (self.kernel_size - 1) * self.dilation\n",
    "        x = F.pad(x, (pad_left, 0))\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, in_ch: int, out_ch: int, kernel_size: int, dilation: int, dropout: float, causal: bool = True):\n",
    "        super().__init__()\n",
    "        self.causal = bool(causal)\n",
    "\n",
    "        if self.causal:\n",
    "            self.conv1 = CausalConv1d(in_ch, out_ch, kernel_size, dilation=dilation)\n",
    "            self.conv2 = CausalConv1d(out_ch, out_ch, kernel_size, dilation=dilation)\n",
    "        else:\n",
    "            pad = ((kernel_size - 1) * dilation) // 2\n",
    "            self.conv1 = nn.Conv1d(in_ch, out_ch, kernel_size, dilation=dilation, padding=pad)\n",
    "            self.conv2 = nn.Conv1d(out_ch, out_ch, kernel_size, dilation=dilation, padding=pad)\n",
    "\n",
    "        self.act = nn.GELU()\n",
    "        self.drop = nn.Dropout(float(dropout))\n",
    "        self.downsample = nn.Identity() if in_ch == out_ch else nn.Conv1d(in_ch, out_ch, kernel_size=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = torch.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        y = self.drop(self.act(self.conv1(x)))\n",
    "        y = self.drop(self.act(self.conv2(y)))\n",
    "        res = self.downsample(x)\n",
    "        return torch.nan_to_num(self.act(y + res), nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "\n",
    "class TemporalConvNet(nn.Module):\n",
    "    def __init__(self, in_ch: int, channels: List[int], kernel_size: int, dropout: float, causal: bool = True):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        cur = int(in_ch)\n",
    "        for i, out_ch in enumerate(channels):\n",
    "            dilation = 2 ** i\n",
    "            layers.append(TemporalBlock(cur, int(out_ch), int(kernel_size), int(dilation), float(dropout), causal=causal))\n",
    "            cur = int(out_ch)\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class AttnPool1D(nn.Module):\n",
    "    \"\"\"\n",
    "    Lightweight attention pooling over time.\n",
    "    Input:  y (B,C,L)\n",
    "    Output: pooled (B,C)\n",
    "    \"\"\"\n",
    "    def __init__(self, channels: int, hidden: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Conv1d(channels, hidden, kernel_size=1),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Conv1d(hidden, 1, kernel_size=1),\n",
    "        )\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_uniform_(m.weight, a=math.sqrt(5))\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, y: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        y = torch.nan_to_num(y, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        scores = self.proj(y)                 # (B,1,L)\n",
    "        attn = torch.softmax(scores, dim=-1)  # (B,1,L)\n",
    "        pooled = (attn * y).sum(dim=-1)       # (B,C)\n",
    "        return pooled, attn.squeeze(1)        # (B,C), (B,L)\n",
    "\n",
    "\n",
    "class MTGNN_ConvAttn_Classifier(nn.Module):\n",
    "    \"\"\"\n",
    "    forward(x_seq, e_seq, edge_index) -> logits (B,2)\n",
    "    Also supports returning aux losses:\n",
    "      forward(..., return_aux=True) -> (logits, aux_dict)\n",
    "    \"\"\"\n",
    "    def __init__(self, node_in: int, edge_dim: int, cfg: Dict[str, Any], n_nodes: int, target_node: int, n_classes: int = 2):\n",
    "        super().__init__()\n",
    "        self.n_nodes = int(n_nodes)\n",
    "        self.target_node = int(target_node)\n",
    "\n",
    "        hidden = int(cfg[\"hidden\"])\n",
    "        dropout = float(cfg[\"dropout\"])\n",
    "\n",
    "        # adjacency modules\n",
    "        self.learn_adj = LearnableAdjacency(self.n_nodes, cfg)\n",
    "\n",
    "        # node feature projection\n",
    "        self.in_proj = nn.Sequential(\n",
    "            nn.Linear(int(node_in), hidden),\n",
    "            nn.LayerNorm(hidden),\n",
    "        )\n",
    "\n",
    "        # graph layers (use A_final for message passing)\n",
    "        self.gnn_layers = nn.ModuleList([GraphMixLayer(hidden, dropout=dropout) for _ in range(int(cfg[\"gnn_layers\"]))])\n",
    "\n",
    "        # temporal conv + attention pooling on target node trajectory\n",
    "        tcn_channels = int(cfg[\"tcn_channels\"])\n",
    "        tcn_layers_n = int(cfg[\"tcn_layers\"])\n",
    "        tcn_kernel = int(cfg[\"tcn_kernel\"])\n",
    "        tcn_dropout = float(cfg[\"tcn_dropout\"])\n",
    "        tcn_causal = bool(cfg[\"tcn_causal\"])\n",
    "\n",
    "        self.tcn_in = nn.Linear(hidden, tcn_channels)\n",
    "        self.tcn = TemporalConvNet(\n",
    "            in_ch=tcn_channels,\n",
    "            channels=[tcn_channels] * tcn_layers_n,\n",
    "            kernel_size=tcn_kernel,\n",
    "            dropout=tcn_dropout,\n",
    "            causal=tcn_causal,\n",
    "        )\n",
    "\n",
    "        self.pool = AttnPool1D(\n",
    "            channels=tcn_channels,\n",
    "            hidden=int(cfg.get(\"attn_pool_hidden\", tcn_channels)),\n",
    "            dropout=float(cfg.get(\"attn_pool_dropout\", 0.1)),\n",
    "        )\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(tcn_channels),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(tcn_channels, tcn_channels),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(tcn_channels, n_classes),\n",
    "        )\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def _compute_A_final(self, e_seq: torch.Tensor, edge_index: torch.Tensor, cfg: Dict[str, Any]) -> Tuple[torch.Tensor, Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        e_seq: (B,L,E,D)\n",
    "        Build A_prior from last timestep, mix with A_learned.\n",
    "        Return A_final (B,N,N) and aux dict with reg terms.\n",
    "        \"\"\"\n",
    "        B, L_, E, D = e_seq.shape\n",
    "        e_last = e_seq[:, -1, :, :]  # (B,E,D)\n",
    "\n",
    "        A_prior = build_adj_prior_from_edge_attr(\n",
    "            edge_attr_last=e_last,\n",
    "            edge_index=edge_index,\n",
    "            n_nodes=self.n_nodes,\n",
    "            use_abs=bool(cfg.get(\"prior_use_abs\", False)),\n",
    "            diag_boost=float(cfg.get(\"prior_diag_boost\", 1.0)),\n",
    "            row_normalize=bool(cfg.get(\"prior_row_normalize\", True)),\n",
    "        )  # (B,N,N)\n",
    "\n",
    "        A_learned_base, sparsity_proxy = self.learn_adj()  # (N,N), (N,N)\n",
    "        A_learned = A_learned_base.unsqueeze(0).expand(B, -1, -1)  # (B,N,N)\n",
    "\n",
    "        alpha = self.learn_adj.alpha().to(e_seq.device).to(e_seq.dtype)  # scalar\n",
    "        A_final = alpha * A_prior + (1.0 - alpha) * A_learned\n",
    "\n",
    "        # regularization\n",
    "        N = self.n_nodes\n",
    "        offdiag = (1.0 - torch.eye(N, device=e_seq.device, dtype=e_seq.dtype))\n",
    "        l1_off = (sparsity_proxy * offdiag).abs().mean()\n",
    "        mse_prior = ((A_learned - A_prior) ** 2 * offdiag).mean()\n",
    "\n",
    "        aux = {\n",
    "            \"alpha\": float(alpha.detach().cpu().item()),\n",
    "            \"l1_off\": float(l1_off.detach().cpu().item()),\n",
    "            \"mse_prior\": float(mse_prior.detach().cpu().item()),\n",
    "            # keep tensors for loss composition\n",
    "            \"_l1_off_t\": l1_off,\n",
    "            \"_mse_prior_t\": mse_prior,\n",
    "        }\n",
    "        return A_final, aux\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        e: torch.Tensor,\n",
    "        edge_index: torch.Tensor,\n",
    "        cfg: Optional[Dict[str, Any]] = None,\n",
    "        return_aux: bool = False\n",
    "    ):\n",
    "        cfg = CFG if cfg is None else cfg\n",
    "\n",
    "        x = torch.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        e = torch.nan_to_num(e, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "        B, L_, N, Fin = x.shape\n",
    "        assert N == self.n_nodes, f\"Expected N={self.n_nodes}, got {N}\"\n",
    "\n",
    "        A_final, aux = self._compute_A_final(e, edge_index, cfg)\n",
    "\n",
    "        h = self.in_proj(x)  # (B,L,N,H)\n",
    "        for gnn in self.gnn_layers:\n",
    "            h = gnn(h, A_final)  # (B,L,N,H)\n",
    "\n",
    "        h_tgt = h[:, :, self.target_node, :]  # (B,L,H)\n",
    "        z = self.tcn_in(h_tgt)                # (B,L,C)\n",
    "        z = z.transpose(1, 2)                 # (B,C,L)\n",
    "        y = self.tcn(z)                       # (B,C,L)\n",
    "        emb, attn_w = self.pool(y)            # (B,C), (B,L)\n",
    "\n",
    "        logits = self.head(emb)               # (B,2)\n",
    "        logits = torch.nan_to_num(logits, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "        if return_aux:\n",
    "            aux[\"attn_mean\"] = float(attn_w.mean().detach().cpu().item())\n",
    "            aux[\"attn_max\"] = float(attn_w.max().detach().cpu().item())\n",
    "            return logits, aux\n",
    "        return logits\n",
    "\n",
    "\n",
    "# sanity\n",
    "B_ = 2\n",
    "Fdim = X_node_raw.shape[-1]\n",
    "E_ = EDGE_INDEX.shape[0]\n",
    "Dedge = edge_feat.shape[-1]\n",
    "x_dummy = torch.randn(B_, L, len(ASSETS), Fdim)\n",
    "e_dummy = torch.randn(B_, L, E_, Dedge)\n",
    "m_dummy = MTGNN_ConvAttn_Classifier(node_in=Fdim, edge_dim=Dedge, cfg=CFG, n_nodes=len(ASSETS), target_node=TARGET_NODE).to(DEVICE)\n",
    "with torch.no_grad():\n",
    "    out, aux = m_dummy(x_dummy.to(DEVICE), e_dummy.to(DEVICE), EDGE_INDEX.to(DEVICE), cfg=CFG, return_aux=True)\n",
    "print(\"Model sanity logits:\", out.shape, \"| finite:\", bool(torch.isfinite(out).all().item()))\n",
    "print(\"Aux sanity:\", {k: aux[k] for k in [\"alpha\", \"l1_off\", \"mse_prior\", \"attn_mean\", \"attn_max\"]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Step 8 — Train/Eval helpers (AUC-oriented) + adjacency regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ce_weights_binary(y_np: np.ndarray) -> torch.Tensor:\n",
    "    y_np = np.asarray(y_np, dtype=np.int64)\n",
    "    counts = np.bincount(y_np, minlength=2).astype(np.float64)\n",
    "    counts = np.maximum(counts, 1.0)\n",
    "    w = counts.sum() / (2.0 * counts)\n",
    "    return torch.tensor(w, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "\n",
    "def make_weighted_sampler_from_labels(y_np: np.ndarray) -> WeightedRandomSampler:\n",
    "    y_np = np.asarray(y_np, dtype=np.int64)\n",
    "    counts = np.bincount(y_np, minlength=2).astype(np.float64)\n",
    "    counts = np.maximum(counts, 1.0)\n",
    "    class_w = counts.sum() / (2.0 * counts)\n",
    "    sample_w = class_w[y_np].astype(np.float64)\n",
    "    sample_w = torch.tensor(sample_w, dtype=torch.double)\n",
    "    return WeightedRandomSampler(weights=sample_w, num_samples=len(sample_w), replacement=True)\n",
    "\n",
    "\n",
    "def total_loss_with_adj_reg(ce_loss: torch.Tensor, aux: Dict[str, Any], cfg: Dict[str, Any]) -> torch.Tensor:\n",
    "    lam_l1 = float(cfg.get(\"adj_l1_lambda\", 0.0))\n",
    "    lam_pr = float(cfg.get(\"adj_prior_lambda\", 0.0))\n",
    "    reg = 0.0\n",
    "    if lam_l1 > 0:\n",
    "        reg = reg + lam_l1 * aux[\"_l1_off_t\"]\n",
    "    if lam_pr > 0:\n",
    "        reg = reg + lam_pr * aux[\"_mse_prior_t\"]\n",
    "    return ce_loss + reg\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_binary(model: nn.Module, loader: DataLoader, loss_fn: nn.Module, y_key: str, cfg: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    n = 0\n",
    "\n",
    "    ys = []\n",
    "    probs = []\n",
    "    ers = []\n",
    "    aux_accum = {\"alpha\": [], \"l1_off\": [], \"mse_prior\": []}\n",
    "\n",
    "    for x, e, y_trade_b, y_dir_b, er in loader:\n",
    "        x = x.to(DEVICE).float()\n",
    "        e = e.to(DEVICE).float()\n",
    "        y = (y_trade_b if y_key == \"trade\" else y_dir_b).to(DEVICE).long()\n",
    "\n",
    "        logits, aux = model(x, e, EDGE_INDEX.to(DEVICE), cfg=cfg, return_aux=True)\n",
    "        ce = loss_fn(logits, y)\n",
    "        loss = total_loss_with_adj_reg(ce, aux, cfg)\n",
    "\n",
    "        total_loss += float(loss.item()) * int(y.size(0))\n",
    "        n += int(y.size(0))\n",
    "\n",
    "        p = torch.softmax(logits, dim=-1).detach().cpu().numpy()\n",
    "        ys.append(y.detach().cpu().numpy())\n",
    "        probs.append(p)\n",
    "        ers.append(er.detach().cpu().numpy())\n",
    "\n",
    "        aux_accum[\"alpha\"].append(aux[\"alpha\"])\n",
    "        aux_accum[\"l1_off\"].append(aux[\"l1_off\"])\n",
    "        aux_accum[\"mse_prior\"].append(aux[\"mse_prior\"])\n",
    "\n",
    "    ys = np.concatenate(ys) if ys else np.array([], dtype=np.int64)\n",
    "    probs = np.concatenate(probs) if probs else np.zeros((0, 2), dtype=np.float32)\n",
    "    ers = np.concatenate(ers) if ers else np.array([], dtype=np.float32)\n",
    "\n",
    "    if len(ys) == 0:\n",
    "        return {\"loss\": np.nan, \"acc\": np.nan, \"f1m\": np.nan, \"auc\": np.nan, \"cm\": None, \"y\": ys, \"prob\": probs, \"er\": ers}\n",
    "\n",
    "    y_pred = probs.argmax(axis=1)\n",
    "    acc = accuracy_score(ys, y_pred)\n",
    "    f1m = f1_score(ys, y_pred, average=\"macro\")\n",
    "    auc = roc_auc_score(ys, probs[:, 1]) if len(np.unique(ys)) == 2 else np.nan\n",
    "    cm = confusion_matrix(ys, y_pred)\n",
    "\n",
    "    out = {\n",
    "        \"loss\": total_loss / max(1, n),\n",
    "        \"acc\": float(acc),\n",
    "        \"f1m\": float(f1m),\n",
    "        \"auc\": float(auc) if np.isfinite(auc) else np.nan,\n",
    "        \"cm\": cm,\n",
    "        \"y\": ys,\n",
    "        \"prob\": probs,\n",
    "        \"er\": ers,\n",
    "    }\n",
    "    if aux_accum[\"alpha\"]:\n",
    "        out[\"adj_alpha_mean\"] = float(np.mean(aux_accum[\"alpha\"]))\n",
    "        out[\"adj_l1_off_mean\"] = float(np.mean(aux_accum[\"l1_off\"]))\n",
    "        out[\"adj_mse_prior_mean\"] = float(np.mean(aux_accum[\"mse_prior\"]))\n",
    "    return out\n",
    "\n",
    "\n",
    "def train_binary_classifier(\n",
    "    X_scaled: np.ndarray,\n",
    "    edge_scaled: np.ndarray,\n",
    "    y_trade_arr: np.ndarray,\n",
    "    y_dir_arr: np.ndarray,\n",
    "    exit_ret_arr: np.ndarray,\n",
    "    sample_t_: np.ndarray,\n",
    "    idx_train: np.ndarray,\n",
    "    idx_val: np.ndarray,\n",
    "    idx_test: np.ndarray,\n",
    "    cfg: Dict[str, Any],\n",
    "    stage_name: str,  # \"trade\" or \"dir\"\n",
    ") -> Tuple[nn.Module, Dict[str, Any]]:\n",
    "    assert stage_name in (\"trade\", \"dir\")\n",
    "\n",
    "    L_ = int(cfg[\"lookback\"])\n",
    "    bs = int(cfg[\"batch_size\"])\n",
    "\n",
    "    tr_ds = LobGraphSequenceDataset2Stage(X_scaled, edge_scaled, y_trade_arr, y_dir_arr, exit_ret_arr, sample_t_, idx_train, L_)\n",
    "    va_ds = LobGraphSequenceDataset2Stage(X_scaled, edge_scaled, y_trade_arr, y_dir_arr, exit_ret_arr, sample_t_, idx_val,   L_)\n",
    "    te_ds = LobGraphSequenceDataset2Stage(X_scaled, edge_scaled, y_trade_arr, y_dir_arr, exit_ret_arr, sample_t_, idx_test,  L_)\n",
    "\n",
    "    # labels for sampler/weights (TRAIN only)\n",
    "    t_train = sample_t_[idx_train]\n",
    "    y_train_np = (y_trade_arr[t_train] if stage_name == \"trade\" else y_dir_arr[t_train]).astype(np.int64)\n",
    "\n",
    "    sampler = None\n",
    "    shuffle = True\n",
    "    if bool(cfg.get(\"use_weighted_sampler\", True)):\n",
    "        sampler = make_weighted_sampler_from_labels(y_train_np)\n",
    "        shuffle = False\n",
    "\n",
    "    tr_loader = DataLoader(tr_ds, batch_size=bs, shuffle=shuffle, sampler=sampler, drop_last=False, collate_fn=collate_fn_2stage, num_workers=0)\n",
    "    va_loader = DataLoader(va_ds, batch_size=bs, shuffle=False, drop_last=False, collate_fn=collate_fn_2stage, num_workers=0)\n",
    "    te_loader = DataLoader(te_ds, batch_size=bs, shuffle=False, drop_last=False, collate_fn=collate_fn_2stage, num_workers=0)\n",
    "\n",
    "    node_in = int(X_scaled.shape[-1])\n",
    "    edge_dim = int(edge_scaled.shape[-1])\n",
    "\n",
    "    model = MTGNN_ConvAttn_Classifier(\n",
    "        node_in=node_in,\n",
    "        edge_dim=edge_dim,\n",
    "        cfg=cfg,\n",
    "        n_nodes=len(ASSETS),\n",
    "        target_node=TARGET_NODE,\n",
    "        n_classes=2,\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    ce_w = make_ce_weights_binary(y_train_np)\n",
    "    loss_fn = nn.CrossEntropyLoss(weight=ce_w, label_smoothing=float(cfg.get(\"label_smoothing\", 0.0)))\n",
    "\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=float(cfg[\"lr\"]), weight_decay=float(cfg[\"weight_decay\"]))\n",
    "\n",
    "    use_onecycle = bool(cfg.get(\"use_onecycle\", True))\n",
    "    if use_onecycle:\n",
    "        sch = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            opt,\n",
    "            max_lr=float(cfg[\"lr\"]),\n",
    "            epochs=int(cfg[\"epochs\"]),\n",
    "            steps_per_epoch=max(1, len(tr_loader)),\n",
    "            pct_start=0.15,\n",
    "            div_factor=10.0,\n",
    "            final_div_factor=50.0,\n",
    "        )\n",
    "    else:\n",
    "        sch = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"max\", factor=0.5, patience=3)\n",
    "\n",
    "    best_auc = -1e18\n",
    "    best_state = None\n",
    "    best_epoch = -1\n",
    "    patience = 7\n",
    "    bad = 0\n",
    "\n",
    "    for ep in range(1, int(cfg[\"epochs\"]) + 1):\n",
    "        model.train()\n",
    "        tot_loss = 0.0\n",
    "        n = 0\n",
    "\n",
    "        for x, e, y_trade_b, y_dir_b, _er in tr_loader:\n",
    "            x = x.to(DEVICE).float()\n",
    "            e = e.to(DEVICE).float()\n",
    "            y = (y_trade_b if stage_name == \"trade\" else y_dir_b).to(DEVICE).long()\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "\n",
    "            logits, aux = model(x, e, EDGE_INDEX.to(DEVICE), cfg=cfg, return_aux=True)\n",
    "            ce = loss_fn(logits, y)\n",
    "            loss = total_loss_with_adj_reg(ce, aux, cfg)\n",
    "\n",
    "            if not torch.isfinite(loss):\n",
    "                continue\n",
    "\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), float(cfg[\"grad_clip\"]))\n",
    "            opt.step()\n",
    "\n",
    "            if use_onecycle:\n",
    "                sch.step()\n",
    "\n",
    "            tot_loss += float(loss.item()) * int(y.size(0))\n",
    "            n += int(y.size(0))\n",
    "\n",
    "        tr_loss = tot_loss / max(1, n)\n",
    "\n",
    "        va = eval_binary(model, va_loader, loss_fn, y_key=stage_name, cfg=cfg)\n",
    "        va_auc = va[\"auc\"]\n",
    "        sel_auc = float(va_auc) if np.isfinite(va_auc) else -1e18\n",
    "\n",
    "        if sel_auc > best_auc:\n",
    "            best_auc = sel_auc\n",
    "            best_epoch = ep\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            bad = 0\n",
    "        else:\n",
    "            bad += 1\n",
    "\n",
    "        if not use_onecycle:\n",
    "            sch.step(sel_auc)\n",
    "\n",
    "        lr_now = opt.param_groups[0][\"lr\"]\n",
    "        print(\n",
    "            f\"[{stage_name}] ep {ep:02d} lr={lr_now:.2e} \"\n",
    "            f\"tr_loss={tr_loss:.4f} va_loss={va['loss']:.4f} va_auc={va_auc:.3f} \"\n",
    "            f\"alpha={va.get('adj_alpha_mean', float('nan')):.3f} \"\n",
    "            f\"reg(l1={va.get('adj_l1_off_mean', float('nan')):.4f}, prior={va.get('adj_mse_prior_mean', float('nan')):.4f}) \"\n",
    "            f\"best={best_auc:.3f}@ep{best_epoch:02d}\"\n",
    "        )\n",
    "\n",
    "        if bad >= patience:\n",
    "            break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    va = eval_binary(model, va_loader, loss_fn, y_key=stage_name, cfg=cfg)\n",
    "    te = eval_binary(model, te_loader, loss_fn, y_key=stage_name, cfg=cfg)\n",
    "\n",
    "    res = {\n",
    "        \"best_epoch\": int(best_epoch),\n",
    "        \"best_val_auc\": float(best_auc) if np.isfinite(best_auc) else np.nan,\n",
    "        \"val\": va,\n",
    "        \"test\": te,\n",
    "    }\n",
    "    return model, res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Step 9 — Two-stage PnL + threshold sweep (val only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_trade_threshold_grid(\n",
    "    p_trade: np.ndarray,\n",
    "    base_grid: Optional[List[float]] = None,\n",
    "    target_trades_list: Optional[List[int]] = None,\n",
    "    min_thr: float = 0.01,\n",
    "    max_thr: float = 0.99,\n",
    ") -> List[float]:\n",
    "    p_trade = np.asarray(p_trade, dtype=np.float64)\n",
    "    p_trade = p_trade[np.isfinite(p_trade)]\n",
    "    if p_trade.size == 0:\n",
    "        return base_grid or [0.5]\n",
    "\n",
    "    thrs = set(float(t) for t in (base_grid or []))\n",
    "\n",
    "    if target_trades_list:\n",
    "        N = int(p_trade.size)\n",
    "        for k in target_trades_list:\n",
    "            k = int(k)\n",
    "            if k <= 0:\n",
    "                continue\n",
    "            if k >= N:\n",
    "                thr = float(np.min(p_trade))\n",
    "            else:\n",
    "                q = 1.0 - (k / N)\n",
    "                thr = float(np.quantile(p_trade, q))\n",
    "            thrs.add(float(np.clip(thr, min_thr, max_thr)))\n",
    "\n",
    "    out = sorted(thrs)\n",
    "    cleaned = []\n",
    "    for t in out:\n",
    "        if not cleaned or abs(t - cleaned[-1]) > 1e-6:\n",
    "            cleaned.append(float(t))\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def two_stage_trade_mask(prob_trade: np.ndarray, prob_dir: np.ndarray, thr_trade: float, thr_dir: float) -> np.ndarray:\n",
    "    p_trade = prob_trade[:, 1]\n",
    "    p_up = prob_dir[:, 1]\n",
    "    conf_dir = np.maximum(p_up, 1.0 - p_up)\n",
    "    return (p_trade >= float(thr_trade)) & (conf_dir >= float(thr_dir))\n",
    "\n",
    "\n",
    "def two_stage_pnl_by_threshold(\n",
    "    prob_trade: np.ndarray,\n",
    "    prob_dir: np.ndarray,\n",
    "    exit_ret_arr: np.ndarray,\n",
    "    thr_trade: float,\n",
    "    thr_dir: float,\n",
    "    cost_bps: float,\n",
    ") -> Dict[str, Any]:\n",
    "    p_up = prob_dir[:, 1]\n",
    "    mask = two_stage_trade_mask(prob_trade, prob_dir, thr_trade, thr_dir)\n",
    "\n",
    "    action = np.zeros_like(exit_ret_arr, dtype=np.float32)\n",
    "    action[mask] = np.where(p_up[mask] >= 0.5, 1.0, -1.0).astype(np.float32)\n",
    "\n",
    "    cost = (float(cost_bps) * 1e-4) * mask.astype(np.float32)\n",
    "    pnl = action * exit_ret_arr - cost\n",
    "\n",
    "    n = int(len(exit_ret_arr))\n",
    "    n_tr = int(mask.sum())\n",
    "\n",
    "    return {\n",
    "        \"n\": n,\n",
    "        \"n_trades\": n_tr,\n",
    "        \"trade_rate\": float(n_tr / max(1, n)),\n",
    "        \"pnl_sum\": float(pnl.sum()),\n",
    "        \"pnl_mean\": float(pnl.mean()) if n else np.nan,\n",
    "        \"pnl_per_trade\": float(pnl.sum() / max(1, n_tr)),\n",
    "        \"pnl_sharpe\": float((pnl.mean() / (pnl.std() + 1e-12)) * np.sqrt(288)) if n else np.nan,\n",
    "    }\n",
    "\n",
    "\n",
    "def sweep_thresholds(\n",
    "    prob_trade: np.ndarray,\n",
    "    prob_dir: np.ndarray,\n",
    "    exit_ret_arr: np.ndarray,\n",
    "    cfg: Dict[str, Any],\n",
    "    min_trades: int = 0,\n",
    "    target_trade_rate: Optional[float] = None,\n",
    ") -> pd.DataFrame:\n",
    "    p_trade = prob_trade[:, 1]\n",
    "    thr_trade_grid = build_trade_threshold_grid(\n",
    "        p_trade=p_trade,\n",
    "        base_grid=cfg.get(\"thr_trade_grid\", [0.5]),\n",
    "        target_trades_list=cfg.get(\"proxy_target_trades\", None),\n",
    "        min_thr=0.01,\n",
    "        max_thr=0.99,\n",
    "    )\n",
    "    thr_dir_grid = cfg.get(\"thr_dir_grid\", [0.5])\n",
    "\n",
    "    obj = str(cfg.get(\"thr_objective\", \"pnl_sum\"))\n",
    "    max_rate = cfg.get(\"max_trade_rate_val\", None)\n",
    "    penalty = float(cfg.get(\"trade_rate_penalty\", 0.0))\n",
    "\n",
    "    rows = []\n",
    "    for thr_t in thr_trade_grid:\n",
    "        for thr_d in thr_dir_grid:\n",
    "            m = two_stage_pnl_by_threshold(prob_trade, prob_dir, exit_ret_arr, thr_t, thr_d, cfg[\"cost_bps\"])\n",
    "            if int(m[\"n_trades\"]) < int(min_trades):\n",
    "                continue\n",
    "            if max_rate is not None and float(m[\"trade_rate\"]) > float(max_rate):\n",
    "                continue\n",
    "\n",
    "            base = float(m.get(obj, np.nan))\n",
    "            if not np.isfinite(base):\n",
    "                continue\n",
    "\n",
    "            if target_trade_rate is not None:\n",
    "                score = base - penalty * abs(float(m[\"trade_rate\"]) - float(target_trade_rate))\n",
    "            else:\n",
    "                score = base - penalty * float(m[\"trade_rate\"])\n",
    "\n",
    "            rows.append({\"thr_trade\": float(thr_t), \"thr_dir\": float(thr_d), \"score\": float(score), **m})\n",
    "\n",
    "    if not rows:\n",
    "        return sweep_thresholds(prob_trade, prob_dir, exit_ret_arr, cfg, min_trades=1, target_trade_rate=target_trade_rate)\n",
    "\n",
    "    df_ = pd.DataFrame(rows).sort_values([\"score\", \"pnl_sum\"], ascending=False)\n",
    "    return df_\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_probs_on_indices(\n",
    "    model: nn.Module,\n",
    "    X_scaled: np.ndarray,\n",
    "    edge_scaled: np.ndarray,\n",
    "    indices: np.ndarray,\n",
    "    cfg: Dict[str, Any]\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    ds = LobGraphSequenceDataset2Stage(X_scaled, edge_scaled, y_trade, y_dir, exit_ret, sample_t, indices, cfg[\"lookback\"])\n",
    "    loader = DataLoader(ds, batch_size=int(cfg[\"batch_size\"]), shuffle=False, collate_fn=collate_fn_2stage, num_workers=0)\n",
    "\n",
    "    model.eval()\n",
    "    probs = []\n",
    "    ers = []\n",
    "    for x, e, _yt, _yd, er in loader:\n",
    "        x = x.to(DEVICE).float()\n",
    "        e = e.to(DEVICE).float()\n",
    "        logits = model(x, e, EDGE_INDEX.to(DEVICE), cfg=cfg, return_aux=False)\n",
    "        p = torch.softmax(logits, dim=-1).cpu().numpy()\n",
    "        probs.append(p)\n",
    "        ers.append(er.cpu().numpy())\n",
    "\n",
    "    return np.concatenate(probs, axis=0), np.concatenate(ers, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Step 10 — Run walk-forward folds (CV-part) + store fold artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FOLD 1/4 sizes: train=5652 val=1130 test=1130\n",
      "True trade ratio (val):  0.365\n",
      "True trade ratio (test): 0.304\n",
      "[trade] ep 01 lr=9.84e-05 tr_loss=0.8423 va_loss=0.8237 va_auc=0.497 alpha=0.500 reg(l1=0.3334, prior=0.0062) best=0.497@ep01\n",
      "[trade] ep 02 lr=2.34e-04 tr_loss=0.6442 va_loss=0.8329 va_auc=0.500 alpha=0.499 reg(l1=0.3334, prior=0.0062) best=0.500@ep02\n",
      "[trade] ep 03 lr=3.00e-04 tr_loss=0.6199 va_loss=0.7585 va_auc=0.541 alpha=0.498 reg(l1=0.3334, prior=0.0062) best=0.541@ep03\n",
      "[trade] ep 04 lr=2.97e-04 tr_loss=0.5835 va_loss=0.9874 va_auc=0.525 alpha=0.497 reg(l1=0.3333, prior=0.0062) best=0.541@ep03\n",
      "[trade] ep 05 lr=2.90e-04 tr_loss=0.5714 va_loss=0.9474 va_auc=0.536 alpha=0.496 reg(l1=0.3334, prior=0.0062) best=0.541@ep03\n",
      "[trade] ep 06 lr=2.77e-04 tr_loss=0.5472 va_loss=0.9511 va_auc=0.585 alpha=0.495 reg(l1=0.3334, prior=0.0062) best=0.585@ep06\n",
      "[trade] ep 07 lr=2.61e-04 tr_loss=0.5393 va_loss=0.8753 va_auc=0.511 alpha=0.494 reg(l1=0.3334, prior=0.0063) best=0.585@ep06\n",
      "[trade] ep 08 lr=2.40e-04 tr_loss=0.5316 va_loss=0.7674 va_auc=0.548 alpha=0.493 reg(l1=0.3334, prior=0.0063) best=0.585@ep06\n",
      "[trade] ep 09 lr=2.16e-04 tr_loss=0.5095 va_loss=0.7686 va_auc=0.585 alpha=0.493 reg(l1=0.3335, prior=0.0063) best=0.585@ep06\n",
      "[trade] ep 10 lr=1.91e-04 tr_loss=0.4992 va_loss=0.7982 va_auc=0.586 alpha=0.492 reg(l1=0.3335, prior=0.0063) best=0.586@ep10\n",
      "[trade] ep 11 lr=1.64e-04 tr_loss=0.5076 va_loss=0.9646 va_auc=0.576 alpha=0.492 reg(l1=0.3335, prior=0.0063) best=0.586@ep10\n",
      "[trade] ep 12 lr=1.36e-04 tr_loss=0.4849 va_loss=0.7800 va_auc=0.525 alpha=0.492 reg(l1=0.3335, prior=0.0063) best=0.586@ep10\n",
      "[trade] ep 13 lr=1.09e-04 tr_loss=0.4797 va_loss=0.7514 va_auc=0.553 alpha=0.492 reg(l1=0.3335, prior=0.0063) best=0.586@ep10\n",
      "[trade] ep 14 lr=8.30e-05 tr_loss=0.4655 va_loss=0.8086 va_auc=0.543 alpha=0.492 reg(l1=0.3335, prior=0.0063) best=0.586@ep10\n",
      "[trade] ep 15 lr=5.96e-05 tr_loss=0.4672 va_loss=0.8448 va_auc=0.552 alpha=0.491 reg(l1=0.3335, prior=0.0063) best=0.586@ep10\n",
      "[trade] ep 16 lr=3.93e-05 tr_loss=0.4611 va_loss=0.8631 va_auc=0.540 alpha=0.491 reg(l1=0.3335, prior=0.0063) best=0.586@ep10\n",
      "[trade] ep 17 lr=2.27e-05 tr_loss=0.4670 va_loss=0.8443 va_auc=0.544 alpha=0.491 reg(l1=0.3335, prior=0.0063) best=0.586@ep10\n",
      "[dir] ep 01 lr=1.00e-04 tr_loss=0.7809 va_loss=0.6808 va_auc=0.608 alpha=0.500 reg(l1=0.3333, prior=0.0063) best=0.608@ep01\n",
      "[dir] ep 02 lr=2.37e-04 tr_loss=0.6793 va_loss=0.7928 va_auc=0.554 alpha=0.500 reg(l1=0.3333, prior=0.0063) best=0.608@ep01\n",
      "[dir] ep 03 lr=3.00e-04 tr_loss=0.5954 va_loss=0.9177 va_auc=0.648 alpha=0.499 reg(l1=0.3333, prior=0.0063) best=0.648@ep03\n",
      "[dir] ep 04 lr=2.97e-04 tr_loss=0.5246 va_loss=0.5929 va_auc=0.676 alpha=0.499 reg(l1=0.3333, prior=0.0063) best=0.676@ep04\n",
      "[dir] ep 05 lr=2.89e-04 tr_loss=0.5122 va_loss=1.0050 va_auc=0.618 alpha=0.499 reg(l1=0.3334, prior=0.0063) best=0.676@ep04\n",
      "[dir] ep 06 lr=2.77e-04 tr_loss=0.4626 va_loss=0.7610 va_auc=0.659 alpha=0.498 reg(l1=0.3334, prior=0.0063) best=0.676@ep04\n",
      "[dir] ep 07 lr=2.60e-04 tr_loss=0.4645 va_loss=1.0586 va_auc=0.630 alpha=0.498 reg(l1=0.3334, prior=0.0063) best=0.676@ep04\n",
      "[dir] ep 08 lr=2.39e-04 tr_loss=0.4347 va_loss=1.0378 va_auc=0.632 alpha=0.498 reg(l1=0.3334, prior=0.0063) best=0.676@ep04\n",
      "[dir] ep 09 lr=2.16e-04 tr_loss=0.3965 va_loss=1.4070 va_auc=0.678 alpha=0.498 reg(l1=0.3334, prior=0.0063) best=0.678@ep09\n",
      "[dir] ep 10 lr=1.90e-04 tr_loss=0.4102 va_loss=0.9805 va_auc=0.594 alpha=0.498 reg(l1=0.3334, prior=0.0063) best=0.678@ep09\n",
      "[dir] ep 11 lr=1.62e-04 tr_loss=0.3842 va_loss=1.7291 va_auc=0.542 alpha=0.498 reg(l1=0.3334, prior=0.0063) best=0.678@ep09\n",
      "[dir] ep 12 lr=1.35e-04 tr_loss=0.4021 va_loss=1.4299 va_auc=0.624 alpha=0.498 reg(l1=0.3334, prior=0.0063) best=0.678@ep09\n",
      "[dir] ep 13 lr=1.08e-04 tr_loss=0.3547 va_loss=1.5185 va_auc=0.632 alpha=0.498 reg(l1=0.3334, prior=0.0063) best=0.678@ep09\n",
      "[dir] ep 14 lr=8.21e-05 tr_loss=0.3730 va_loss=1.3051 va_auc=0.630 alpha=0.498 reg(l1=0.3334, prior=0.0063) best=0.678@ep09\n",
      "[dir] ep 15 lr=5.88e-05 tr_loss=0.3480 va_loss=1.1771 va_auc=0.640 alpha=0.498 reg(l1=0.3334, prior=0.0063) best=0.678@ep09\n",
      "[dir] ep 16 lr=3.86e-05 tr_loss=0.3326 va_loss=1.5683 va_auc=0.647 alpha=0.498 reg(l1=0.3334, prior=0.0063) best=0.678@ep09\n",
      "\n",
      "Chosen thresholds (from VAL):\n",
      "  thr_trade*=0.842 thr_dir*=0.500 | score=0.0230\n",
      "  val trade_rate(pred)=0.044 | val pnl_sum=0.0551 | val sharpe=1.364\n",
      "\n",
      "Top-5 VAL threshold candidates:\n",
      "    thr_trade  thr_dir     score  trade_rate   pnl_sum  pnl_sharpe  n_trades\n",
      "20    0.84198     0.50  0.022993    0.044248  0.055117    1.364078        50\n",
      "21    0.84198     0.55  0.022993    0.044248  0.055117    1.364078        50\n",
      "22    0.84198     0.60  0.022993    0.044248  0.055117    1.364078        50\n",
      "23    0.84198     0.65  0.022993    0.044248  0.055117    1.364078        50\n",
      "24    0.84198     0.70  0.022993    0.044248  0.055117    1.364078        50\n",
      "\n",
      "TEST (fixed thr from VAL):\n",
      "  trade_rate(pred)=0.269 | pnl_sum=-0.1318 | pnl_mean=-0.000117 | trades=304\n",
      "\n",
      "================================================================================\n",
      "FOLD 2/4 sizes: train=6782 val=1130 test=1130\n",
      "True trade ratio (val):  0.304\n",
      "True trade ratio (test): 0.463\n",
      "[trade] ep 01 lr=9.83e-05 tr_loss=0.7333 va_loss=0.7508 va_auc=0.605 alpha=0.500 reg(l1=0.3333, prior=0.0058) best=0.605@ep01\n",
      "[trade] ep 02 lr=2.34e-04 tr_loss=0.6500 va_loss=0.6823 va_auc=0.689 alpha=0.500 reg(l1=0.3333, prior=0.0058) best=0.689@ep02\n",
      "[trade] ep 03 lr=3.00e-04 tr_loss=0.6137 va_loss=0.7117 va_auc=0.680 alpha=0.498 reg(l1=0.3333, prior=0.0058) best=0.689@ep02\n",
      "[trade] ep 04 lr=2.97e-04 tr_loss=0.5933 va_loss=0.7221 va_auc=0.590 alpha=0.498 reg(l1=0.3333, prior=0.0058) best=0.689@ep02\n",
      "[trade] ep 05 lr=2.90e-04 tr_loss=0.5875 va_loss=0.7225 va_auc=0.609 alpha=0.497 reg(l1=0.3334, prior=0.0058) best=0.689@ep02\n",
      "[trade] ep 06 lr=2.77e-04 tr_loss=0.5593 va_loss=0.7442 va_auc=0.582 alpha=0.496 reg(l1=0.3334, prior=0.0058) best=0.689@ep02\n",
      "[trade] ep 07 lr=2.61e-04 tr_loss=0.5490 va_loss=0.7663 va_auc=0.591 alpha=0.495 reg(l1=0.3334, prior=0.0058) best=0.689@ep02\n",
      "[trade] ep 08 lr=2.40e-04 tr_loss=0.5433 va_loss=0.7628 va_auc=0.584 alpha=0.493 reg(l1=0.3335, prior=0.0058) best=0.689@ep02\n",
      "[trade] ep 09 lr=2.17e-04 tr_loss=0.5232 va_loss=0.8313 va_auc=0.581 alpha=0.492 reg(l1=0.3336, prior=0.0059) best=0.689@ep02\n",
      "[dir] ep 01 lr=9.96e-05 tr_loss=0.8668 va_loss=0.7326 va_auc=0.739 alpha=0.500 reg(l1=0.3333, prior=0.0059) best=0.739@ep01\n",
      "[dir] ep 02 lr=2.37e-04 tr_loss=0.6765 va_loss=0.6887 va_auc=0.578 alpha=0.500 reg(l1=0.3333, prior=0.0059) best=0.739@ep01\n",
      "[dir] ep 03 lr=3.00e-04 tr_loss=0.5913 va_loss=0.5915 va_auc=0.754 alpha=0.501 reg(l1=0.3333, prior=0.0059) best=0.754@ep03\n",
      "[dir] ep 04 lr=2.97e-04 tr_loss=0.5378 va_loss=0.7008 va_auc=0.545 alpha=0.500 reg(l1=0.3333, prior=0.0059) best=0.754@ep03\n",
      "[dir] ep 05 lr=2.89e-04 tr_loss=0.4938 va_loss=0.7129 va_auc=0.592 alpha=0.500 reg(l1=0.3333, prior=0.0059) best=0.754@ep03\n",
      "[dir] ep 06 lr=2.77e-04 tr_loss=0.4727 va_loss=0.6884 va_auc=0.538 alpha=0.500 reg(l1=0.3333, prior=0.0059) best=0.754@ep03\n",
      "[dir] ep 07 lr=2.60e-04 tr_loss=0.4558 va_loss=0.6439 va_auc=0.669 alpha=0.499 reg(l1=0.3333, prior=0.0059) best=0.754@ep03\n",
      "[dir] ep 08 lr=2.39e-04 tr_loss=0.4216 va_loss=0.9508 va_auc=0.679 alpha=0.499 reg(l1=0.3333, prior=0.0059) best=0.754@ep03\n",
      "[dir] ep 09 lr=2.16e-04 tr_loss=0.4047 va_loss=0.7664 va_auc=0.636 alpha=0.498 reg(l1=0.3333, prior=0.0059) best=0.754@ep03\n",
      "[dir] ep 10 lr=1.90e-04 tr_loss=0.4175 va_loss=0.6642 va_auc=0.654 alpha=0.498 reg(l1=0.3333, prior=0.0059) best=0.754@ep03\n",
      "\n",
      "Chosen thresholds (from VAL):\n",
      "  thr_trade*=0.550 thr_dir*=0.500 | score=0.8030\n",
      "  val trade_rate(pred)=0.544 | val pnl_sum=0.8270 | val sharpe=4.347\n",
      "\n",
      "Top-5 VAL threshold candidates:\n",
      "   thr_trade  thr_dir     score  trade_rate   pnl_sum  pnl_sharpe  n_trades\n",
      "4       0.55     0.50  0.802963    0.544248  0.827033    4.346645       615\n",
      "5       0.55     0.55  0.768614    0.424779  0.780738    4.408844       480\n",
      "6       0.55     0.60  0.682235    0.261062  0.686483    4.361653       295\n",
      "0       0.50     0.55  0.641222    0.615044  0.672373    3.321238       695\n",
      "1       0.50     0.60  0.608484    0.387611  0.616891    3.573466       438\n",
      "\n",
      "TEST (fixed thr from VAL):\n",
      "  trade_rate(pred)=0.071 | pnl_sum=0.3220 | pnl_mean=0.000285 | trades=80\n",
      "\n",
      "================================================================================\n",
      "FOLD 3/4 sizes: train=7912 val=1130 test=1130\n",
      "True trade ratio (val):  0.463\n",
      "True trade ratio (test): 0.558\n",
      "[trade] ep 01 lr=9.82e-05 tr_loss=0.7027 va_loss=0.7310 va_auc=0.433 alpha=0.500 reg(l1=0.3333, prior=0.0060) best=0.433@ep01\n",
      "[trade] ep 02 lr=2.34e-04 tr_loss=0.6257 va_loss=0.7415 va_auc=0.471 alpha=0.500 reg(l1=0.3333, prior=0.0060) best=0.471@ep02\n",
      "[trade] ep 03 lr=3.00e-04 tr_loss=0.6054 va_loss=0.7406 va_auc=0.482 alpha=0.499 reg(l1=0.3333, prior=0.0060) best=0.482@ep03\n",
      "[trade] ep 04 lr=2.97e-04 tr_loss=0.5846 va_loss=1.0055 va_auc=0.484 alpha=0.497 reg(l1=0.3333, prior=0.0061) best=0.484@ep04\n",
      "[trade] ep 05 lr=2.90e-04 tr_loss=0.5833 va_loss=0.8745 va_auc=0.550 alpha=0.496 reg(l1=0.3334, prior=0.0061) best=0.550@ep05\n",
      "[trade] ep 06 lr=2.77e-04 tr_loss=0.5536 va_loss=0.8269 va_auc=0.497 alpha=0.494 reg(l1=0.3335, prior=0.0061) best=0.550@ep05\n",
      "[trade] ep 07 lr=2.61e-04 tr_loss=0.5356 va_loss=0.9674 va_auc=0.617 alpha=0.493 reg(l1=0.3335, prior=0.0062) best=0.617@ep07\n",
      "[trade] ep 08 lr=2.40e-04 tr_loss=0.5245 va_loss=0.9673 va_auc=0.635 alpha=0.492 reg(l1=0.3336, prior=0.0062) best=0.635@ep08\n",
      "[trade] ep 09 lr=2.17e-04 tr_loss=0.5043 va_loss=0.7058 va_auc=0.623 alpha=0.491 reg(l1=0.3337, prior=0.0063) best=0.635@ep08\n",
      "[trade] ep 10 lr=1.91e-04 tr_loss=0.5025 va_loss=1.0146 va_auc=0.675 alpha=0.490 reg(l1=0.3338, prior=0.0063) best=0.675@ep10\n",
      "[trade] ep 11 lr=1.64e-04 tr_loss=0.4823 va_loss=1.0077 va_auc=0.653 alpha=0.490 reg(l1=0.3338, prior=0.0064) best=0.675@ep10\n",
      "[trade] ep 12 lr=1.36e-04 tr_loss=0.4750 va_loss=1.1323 va_auc=0.622 alpha=0.489 reg(l1=0.3339, prior=0.0064) best=0.675@ep10\n",
      "[trade] ep 13 lr=1.09e-04 tr_loss=0.4631 va_loss=0.9970 va_auc=0.612 alpha=0.489 reg(l1=0.3338, prior=0.0064) best=0.675@ep10\n",
      "[trade] ep 14 lr=8.32e-05 tr_loss=0.4606 va_loss=1.0670 va_auc=0.629 alpha=0.488 reg(l1=0.3339, prior=0.0065) best=0.675@ep10\n",
      "[trade] ep 15 lr=5.97e-05 tr_loss=0.4448 va_loss=0.9662 va_auc=0.619 alpha=0.488 reg(l1=0.3339, prior=0.0065) best=0.675@ep10\n",
      "[trade] ep 16 lr=3.94e-05 tr_loss=0.4492 va_loss=1.0916 va_auc=0.643 alpha=0.488 reg(l1=0.3339, prior=0.0065) best=0.675@ep10\n",
      "[trade] ep 17 lr=2.28e-05 tr_loss=0.4438 va_loss=1.2322 va_auc=0.646 alpha=0.488 reg(l1=0.3339, prior=0.0065) best=0.675@ep10\n",
      "[dir] ep 01 lr=9.93e-05 tr_loss=0.7444 va_loss=0.6583 va_auc=0.696 alpha=0.500 reg(l1=0.3334, prior=0.0059) best=0.696@ep01\n",
      "[dir] ep 02 lr=2.36e-04 tr_loss=0.6499 va_loss=0.9008 va_auc=0.687 alpha=0.499 reg(l1=0.3334, prior=0.0059) best=0.696@ep01\n",
      "[dir] ep 03 lr=3.00e-04 tr_loss=0.5960 va_loss=0.7827 va_auc=0.683 alpha=0.499 reg(l1=0.3334, prior=0.0059) best=0.696@ep01\n",
      "[dir] ep 04 lr=2.97e-04 tr_loss=0.5360 va_loss=0.9155 va_auc=0.656 alpha=0.499 reg(l1=0.3334, prior=0.0059) best=0.696@ep01\n",
      "[dir] ep 05 lr=2.89e-04 tr_loss=0.5128 va_loss=0.8746 va_auc=0.647 alpha=0.498 reg(l1=0.3334, prior=0.0059) best=0.696@ep01\n",
      "[dir] ep 06 lr=2.77e-04 tr_loss=0.5099 va_loss=0.8028 va_auc=0.677 alpha=0.498 reg(l1=0.3334, prior=0.0059) best=0.696@ep01\n",
      "[dir] ep 07 lr=2.60e-04 tr_loss=0.4929 va_loss=0.7131 va_auc=0.619 alpha=0.498 reg(l1=0.3334, prior=0.0059) best=0.696@ep01\n",
      "[dir] ep 08 lr=2.40e-04 tr_loss=0.4303 va_loss=0.7327 va_auc=0.625 alpha=0.498 reg(l1=0.3334, prior=0.0059) best=0.696@ep01\n",
      "\n",
      "Chosen thresholds (from VAL):\n",
      "  thr_trade*=0.416 thr_dir*=0.500 | score=0.5476\n",
      "  val trade_rate(pred)=0.177 | val pnl_sum=0.5762 | val sharpe=3.887\n",
      "\n",
      "Top-5 VAL threshold candidates:\n",
      "   thr_trade  thr_dir     score  trade_rate   pnl_sum  pnl_sharpe  n_trades\n",
      "0   0.415588     0.50  0.547627    0.176991  0.576211    3.887135       200\n",
      "3   0.500000     0.50  0.491463    0.139823  0.523764    3.870094       158\n",
      "6   0.550000     0.50  0.459600    0.115044  0.494379    4.080718       130\n",
      "1   0.415588     0.55  0.383313    0.114159  0.418180    3.277831       129\n",
      "9   0.583994     0.50  0.322165    0.088496  0.359598    3.489985       100\n",
      "\n",
      "TEST (fixed thr from VAL):\n",
      "  trade_rate(pred)=0.096 | pnl_sum=-0.2102 | pnl_mean=-0.000186 | trades=108\n",
      "\n",
      "================================================================================\n",
      "FOLD 4/4 sizes: train=9042 val=1130 test=1130\n",
      "True trade ratio (val):  0.558\n",
      "True trade ratio (test): 0.506\n",
      "[trade] ep 01 lr=9.81e-05 tr_loss=0.6894 va_loss=0.6158 va_auc=0.652 alpha=0.500 reg(l1=0.3333, prior=0.0053) best=0.652@ep01\n",
      "[trade] ep 02 lr=2.34e-04 tr_loss=0.6191 va_loss=0.6115 va_auc=0.658 alpha=0.499 reg(l1=0.3333, prior=0.0053) best=0.658@ep02\n",
      "[trade] ep 03 lr=3.00e-04 tr_loss=0.6079 va_loss=0.6311 va_auc=0.594 alpha=0.497 reg(l1=0.3334, prior=0.0053) best=0.658@ep02\n",
      "[trade] ep 04 lr=2.97e-04 tr_loss=0.5754 va_loss=0.6800 va_auc=0.657 alpha=0.496 reg(l1=0.3334, prior=0.0054) best=0.658@ep02\n",
      "[trade] ep 05 lr=2.90e-04 tr_loss=0.5635 va_loss=0.7186 va_auc=0.659 alpha=0.495 reg(l1=0.3334, prior=0.0054) best=0.659@ep05\n",
      "[trade] ep 06 lr=2.77e-04 tr_loss=0.5345 va_loss=0.6221 va_auc=0.684 alpha=0.493 reg(l1=0.3335, prior=0.0054) best=0.684@ep06\n",
      "[trade] ep 07 lr=2.61e-04 tr_loss=0.5147 va_loss=0.6015 va_auc=0.688 alpha=0.492 reg(l1=0.3336, prior=0.0054) best=0.688@ep07\n",
      "[trade] ep 08 lr=2.40e-04 tr_loss=0.5115 va_loss=0.7652 va_auc=0.650 alpha=0.491 reg(l1=0.3337, prior=0.0055) best=0.688@ep07\n",
      "[trade] ep 09 lr=2.17e-04 tr_loss=0.5013 va_loss=0.6385 va_auc=0.700 alpha=0.490 reg(l1=0.3337, prior=0.0055) best=0.700@ep09\n",
      "[trade] ep 10 lr=1.91e-04 tr_loss=0.4906 va_loss=0.7106 va_auc=0.646 alpha=0.489 reg(l1=0.3338, prior=0.0056) best=0.700@ep09\n",
      "[trade] ep 11 lr=1.64e-04 tr_loss=0.4744 va_loss=0.6595 va_auc=0.677 alpha=0.489 reg(l1=0.3338, prior=0.0056) best=0.700@ep09\n",
      "[trade] ep 12 lr=1.36e-04 tr_loss=0.4729 va_loss=0.7605 va_auc=0.653 alpha=0.488 reg(l1=0.3339, prior=0.0056) best=0.700@ep09\n",
      "[trade] ep 13 lr=1.09e-04 tr_loss=0.4652 va_loss=0.6377 va_auc=0.677 alpha=0.488 reg(l1=0.3339, prior=0.0057) best=0.700@ep09\n",
      "[trade] ep 14 lr=8.32e-05 tr_loss=0.4485 va_loss=0.6685 va_auc=0.653 alpha=0.488 reg(l1=0.3339, prior=0.0057) best=0.700@ep09\n",
      "[trade] ep 15 lr=5.98e-05 tr_loss=0.4588 va_loss=0.7723 va_auc=0.650 alpha=0.487 reg(l1=0.3339, prior=0.0057) best=0.700@ep09\n",
      "[trade] ep 16 lr=3.94e-05 tr_loss=0.4380 va_loss=0.7131 va_auc=0.660 alpha=0.487 reg(l1=0.3339, prior=0.0057) best=0.700@ep09\n",
      "[dir] ep 01 lr=9.90e-05 tr_loss=0.7561 va_loss=0.7283 va_auc=0.412 alpha=0.500 reg(l1=0.3333, prior=0.0052) best=0.412@ep01\n",
      "[dir] ep 02 lr=2.36e-04 tr_loss=0.6626 va_loss=0.7097 va_auc=0.449 alpha=0.500 reg(l1=0.3333, prior=0.0052) best=0.449@ep02\n",
      "[dir] ep 03 lr=3.00e-04 tr_loss=0.5861 va_loss=0.7855 va_auc=0.399 alpha=0.499 reg(l1=0.3333, prior=0.0052) best=0.449@ep02\n",
      "[dir] ep 04 lr=2.97e-04 tr_loss=0.5696 va_loss=0.7550 va_auc=0.463 alpha=0.499 reg(l1=0.3333, prior=0.0052) best=0.463@ep04\n",
      "[dir] ep 05 lr=2.90e-04 tr_loss=0.5141 va_loss=0.8174 va_auc=0.506 alpha=0.499 reg(l1=0.3333, prior=0.0052) best=0.506@ep05\n",
      "[dir] ep 06 lr=2.77e-04 tr_loss=0.4973 va_loss=0.7452 va_auc=0.484 alpha=0.498 reg(l1=0.3333, prior=0.0052) best=0.506@ep05\n",
      "[dir] ep 07 lr=2.60e-04 tr_loss=0.4818 va_loss=0.8202 va_auc=0.541 alpha=0.498 reg(l1=0.3333, prior=0.0052) best=0.541@ep07\n",
      "[dir] ep 08 lr=2.40e-04 tr_loss=0.4723 va_loss=0.7372 va_auc=0.525 alpha=0.498 reg(l1=0.3333, prior=0.0052) best=0.541@ep07\n",
      "[dir] ep 09 lr=2.16e-04 tr_loss=0.4730 va_loss=0.7444 va_auc=0.515 alpha=0.497 reg(l1=0.3333, prior=0.0052) best=0.541@ep07\n",
      "[dir] ep 10 lr=1.90e-04 tr_loss=0.3977 va_loss=0.7774 va_auc=0.516 alpha=0.497 reg(l1=0.3333, prior=0.0052) best=0.541@ep07\n",
      "[dir] ep 11 lr=1.63e-04 tr_loss=0.4326 va_loss=0.7805 va_auc=0.501 alpha=0.497 reg(l1=0.3333, prior=0.0052) best=0.541@ep07\n",
      "[dir] ep 12 lr=1.35e-04 tr_loss=0.3763 va_loss=0.8143 va_auc=0.517 alpha=0.497 reg(l1=0.3333, prior=0.0052) best=0.541@ep07\n",
      "[dir] ep 13 lr=1.08e-04 tr_loss=0.3736 va_loss=0.8832 va_auc=0.508 alpha=0.497 reg(l1=0.3333, prior=0.0052) best=0.541@ep07\n",
      "[dir] ep 14 lr=8.27e-05 tr_loss=0.3667 va_loss=0.8020 va_auc=0.529 alpha=0.497 reg(l1=0.3333, prior=0.0052) best=0.541@ep07\n",
      "\n",
      "Chosen thresholds (from VAL):\n",
      "  thr_trade*=0.550 thr_dir*=0.650 | score=0.4049\n",
      "  val trade_rate(pred)=0.404 | val pnl_sum=0.4202 | val sharpe=1.455\n",
      "\n",
      "Top-5 VAL threshold candidates:\n",
      "    thr_trade  thr_dir     score  trade_rate   pnl_sum  pnl_sharpe  n_trades\n",
      "8        0.55     0.65  0.404873    0.404425  0.420183    1.454804       457\n",
      "13       0.60     0.65  0.390944    0.307080  0.415988    1.581386       347\n",
      "7        0.55     0.60  0.390462    0.409735  0.405241    1.388932       463\n",
      "10       0.60     0.50  0.383399    0.307965  0.408355    1.549564       348\n",
      "11       0.60     0.55  0.383399    0.307965  0.408355    1.549564       348\n",
      "\n",
      "TEST (fixed thr from VAL):\n",
      "  trade_rate(pred)=0.310 | pnl_sum=-0.6322 | pnl_mean=-0.000559 | trades=350\n",
      "\n",
      "================================================================================\n",
      "CV summary (fold TEST, fixed thresholds from fold-VAL):\n",
      "   fold  trade_test_auc  dir_test_auc  test_trade_rate_pred  test_pnl_sum  \\\n",
      "0     1        0.606621      0.466759              0.269027     -0.131845   \n",
      "1     2        0.528931      0.540764              0.070796      0.322038   \n",
      "2     3        0.554654      0.388572              0.095575     -0.210188   \n",
      "3     4        0.641339      0.469763              0.309735     -0.632232   \n",
      "\n",
      "   test_pnl_mean  thr_trade  thr_dir  n_trades  best_val_score  \n",
      "0      -0.000117   0.841980     0.50       304        0.022993  \n",
      "1       0.000285   0.550000     0.50        80        0.802963  \n",
      "2      -0.000186   0.415588     0.50       108        0.547627  \n",
      "3      -0.000559   0.550000     0.65       350        0.404873  \n",
      "\n",
      "Means (just for debugging, NOT a final decision rule):\n",
      "fold                      2.500000\n",
      "trade_test_auc            0.582886\n",
      "dir_test_auc              0.466464\n",
      "test_trade_rate_pred      0.186283\n",
      "test_pnl_sum             -0.163057\n",
      "test_pnl_mean            -0.000144\n",
      "thr_trade                 0.589392\n",
      "thr_dir                   0.537500\n",
      "n_trades                210.500000\n",
      "best_val_score            0.444614\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def _state_dict_to_cpu(sd: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "    return {k: v.detach().cpu().clone() for k, v in sd.items()}\n",
    "\n",
    "\n",
    "def run_walk_forward_cv() -> Tuple[pd.DataFrame, List[Dict[str, Any]], nn.Module, nn.Module]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      - cv_summary: per-fold TEST metrics (using thresholds selected on that fold VAL)\n",
    "      - fold_artifacts: list of per-fold dicts (models + thresholds + VAL preds)\n",
    "      - m_trade_last, m_dir_last: last fold trained models\n",
    "    \"\"\"\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "    fold_artifacts: List[Dict[str, Any]] = []\n",
    "\n",
    "    m_trade_last: Optional[nn.Module] = None\n",
    "    m_dir_last: Optional[nn.Module] = None\n",
    "\n",
    "    for fi, (idx_tr, idx_va, idx_te) in enumerate(walk_splits, 1):\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(f\"FOLD {fi}/{len(walk_splits)} sizes: train={len(idx_tr)} val={len(idx_va)} test={len(idx_te)}\")\n",
    "        true_val_trade = split_trade_ratio(idx_va, sample_t, y_trade)\n",
    "        true_te_trade = split_trade_ratio(idx_te, sample_t, y_trade)\n",
    "        print(f\"True trade ratio (val):  {true_val_trade:.3f}\")\n",
    "        print(f\"True trade ratio (test): {true_te_trade:.3f}\")\n",
    "\n",
    "        # fold scaling (fit only on fold train timeline)\n",
    "        X_scaled, _ = fit_scale_nodes_train_only(X_node_raw, sample_t, idx_tr, max_abs=CFG[\"max_abs_feat\"])\n",
    "        if bool(CFG.get(\"edge_scale\", True)):\n",
    "            edge_scaled, _ = fit_scale_edges_train_only(edge_feat, sample_t, idx_tr, max_abs=CFG[\"max_abs_edge\"])\n",
    "        else:\n",
    "            edge_scaled = edge_feat\n",
    "\n",
    "        # Stage A\n",
    "        m_trade, r_trade = train_binary_classifier(\n",
    "            X_scaled, edge_scaled, y_trade, y_dir, exit_ret, sample_t,\n",
    "            idx_tr, idx_va, idx_te, CFG, stage_name=\"trade\"\n",
    "        )\n",
    "\n",
    "        # Stage B (trade-only indices)\n",
    "        idx_tr_T = subset_trade_indices(idx_tr, sample_t, y_trade)\n",
    "        idx_va_T = subset_trade_indices(idx_va, sample_t, y_trade)\n",
    "        idx_te_T = subset_trade_indices(idx_te, sample_t, y_trade)\n",
    "\n",
    "        if len(idx_tr_T) < max(200, 2 * CFG[\"batch_size\"]) or len(idx_va_T) < 50 or len(idx_te_T) < 50:\n",
    "            print(\"[dir] skip: not enough trade samples in this fold.\")\n",
    "\n",
    "            rows.append({\n",
    "                \"fold\": fi,\n",
    "                \"trade_test_auc\": r_trade[\"test\"][\"auc\"],\n",
    "                \"dir_test_auc\": np.nan,\n",
    "                \"test_trade_rate_pred\": np.nan,\n",
    "                \"test_pnl_sum\": np.nan,\n",
    "                \"test_pnl_mean\": np.nan,\n",
    "                \"thr_trade\": np.nan,\n",
    "                \"thr_dir\": np.nan,\n",
    "                \"n_trades\": np.nan,\n",
    "                \"best_val_score\": np.nan,\n",
    "            })\n",
    "\n",
    "            fold_artifacts.append({\n",
    "                \"fold\": fi,\n",
    "                \"idx_tr\": idx_tr, \"idx_va\": idx_va, \"idx_te\": idx_te,\n",
    "                \"thr_trade\": np.nan, \"thr_dir\": np.nan,\n",
    "                \"best_val_score\": np.nan,\n",
    "                \"trade_state\": _state_dict_to_cpu(m_trade.state_dict()),\n",
    "                \"dir_state\": None,\n",
    "                \"prob_trade_val\": None, \"prob_dir_val\": None, \"er_val\": None,\n",
    "                \"val_true_trade_rate\": float(true_val_trade),\n",
    "            })\n",
    "\n",
    "            m_trade_last = m_trade\n",
    "            m_dir_last = None\n",
    "            continue\n",
    "\n",
    "        m_dir, r_dir = train_binary_classifier(\n",
    "            X_scaled, edge_scaled, y_trade, y_dir, exit_ret, sample_t,\n",
    "            idx_tr_T, idx_va_T, idx_te_T, CFG, stage_name=\"dir\"\n",
    "        )\n",
    "\n",
    "        # Choose thresholds on VAL (VAL only)\n",
    "        prob_trade_val, er_val = predict_probs_on_indices(m_trade, X_scaled, edge_scaled, idx_va, CFG)\n",
    "        prob_dir_val, _ = predict_probs_on_indices(m_dir, X_scaled, edge_scaled, idx_va, CFG)\n",
    "\n",
    "        sweep_val = sweep_thresholds(\n",
    "            prob_trade_val, prob_dir_val, er_val, CFG,\n",
    "            min_trades=int(CFG[\"eval_min_trades\"]),\n",
    "            target_trade_rate=float(true_val_trade),\n",
    "        )\n",
    "        best_val = sweep_val.iloc[0].to_dict()\n",
    "        thr_trade_star = float(best_val[\"thr_trade\"])\n",
    "        thr_dir_star = float(best_val[\"thr_dir\"])\n",
    "\n",
    "        val_metrics = two_stage_pnl_by_threshold(prob_trade_val, prob_dir_val, er_val, thr_trade_star, thr_dir_star, CFG[\"cost_bps\"])\n",
    "        print(\"\\nChosen thresholds (from VAL):\")\n",
    "        print(f\"  thr_trade*={thr_trade_star:.3f} thr_dir*={thr_dir_star:.3f} | score={best_val['score']:.4f}\")\n",
    "        print(f\"  val trade_rate(pred)={val_metrics['trade_rate']:.3f} | val pnl_sum={val_metrics['pnl_sum']:.4f} | val sharpe={val_metrics['pnl_sharpe']:.3f}\")\n",
    "\n",
    "        print(\"\\nTop-5 VAL threshold candidates:\")\n",
    "        print(sweep_val.head(5)[[\"thr_trade\", \"thr_dir\", \"score\", \"trade_rate\", \"pnl_sum\", \"pnl_sharpe\", \"n_trades\"]])\n",
    "\n",
    "        # Evaluate on TEST with fixed thresholds\n",
    "        prob_trade_te, er_te = predict_probs_on_indices(m_trade, X_scaled, edge_scaled, idx_te, CFG)\n",
    "        prob_dir_te, _ = predict_probs_on_indices(m_dir, X_scaled, edge_scaled, idx_te, CFG)\n",
    "        te_metrics = two_stage_pnl_by_threshold(prob_trade_te, prob_dir_te, er_te, thr_trade_star, thr_dir_star, CFG[\"cost_bps\"])\n",
    "\n",
    "        print(\"\\nTEST (fixed thr from VAL):\")\n",
    "        print(f\"  trade_rate(pred)={te_metrics['trade_rate']:.3f} | pnl_sum={te_metrics['pnl_sum']:.4f} | pnl_mean={te_metrics['pnl_mean']:.6f} | trades={te_metrics['n_trades']}\")\n",
    "\n",
    "        rows.append({\n",
    "            \"fold\": fi,\n",
    "            \"trade_test_auc\": r_trade[\"test\"][\"auc\"],\n",
    "            \"dir_test_auc\": r_dir[\"test\"][\"auc\"],\n",
    "            \"test_trade_rate_pred\": te_metrics[\"trade_rate\"],\n",
    "            \"test_pnl_sum\": te_metrics[\"pnl_sum\"],\n",
    "            \"test_pnl_mean\": te_metrics[\"pnl_mean\"],\n",
    "            \"thr_trade\": thr_trade_star,\n",
    "            \"thr_dir\": thr_dir_star,\n",
    "            \"n_trades\": te_metrics[\"n_trades\"],\n",
    "            \"best_val_score\": float(best_val[\"score\"]),\n",
    "        })\n",
    "\n",
    "        fold_artifacts.append({\n",
    "            \"fold\": fi,\n",
    "            \"idx_tr\": idx_tr, \"idx_va\": idx_va, \"idx_te\": idx_te,\n",
    "            \"thr_trade\": thr_trade_star, \"thr_dir\": thr_dir_star,\n",
    "            \"best_val_score\": float(best_val[\"score\"]),\n",
    "            \"trade_state\": _state_dict_to_cpu(m_trade.state_dict()),\n",
    "            \"dir_state\": _state_dict_to_cpu(m_dir.state_dict()),\n",
    "            \"prob_trade_val\": prob_trade_val,\n",
    "            \"prob_dir_val\": prob_dir_val,\n",
    "            \"er_val\": er_val,\n",
    "            \"val_true_trade_rate\": float(true_val_trade),\n",
    "        })\n",
    "\n",
    "        m_trade_last = m_trade\n",
    "        m_dir_last = m_dir\n",
    "\n",
    "    if m_trade_last is None:\n",
    "        raise RuntimeError(\"No folds were trained; check your split configuration.\")\n",
    "\n",
    "    cv_summary = pd.DataFrame(rows)\n",
    "    return cv_summary, fold_artifacts, m_trade_last, m_dir_last\n",
    "\n",
    "\n",
    "cv_summary, fold_artifacts, m_trade_last, m_dir_last = run_walk_forward_cv()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CV summary (fold TEST, fixed thresholds from fold-VAL):\")\n",
    "print(cv_summary)\n",
    "print(\"\\nMeans (just for debugging, NOT a final decision rule):\")\n",
    "print(cv_summary.mean(numeric_only=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Step 11 — Post-CV checks on FINAL holdout (10%) WITHOUT refit (3 methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 11 — HOLDOUT CHECKS WITHOUT ANY REFIT (3 METHODS)\n",
      "Holdout size: 1256 samples | sidx 11305..12560\n",
      "\n",
      "Results (compare these, not mean/median over folds):\n",
      "                                                                                  method  thr_trade  thr_dir  holdout_trade_auc  holdout_dir_auc  trade_rate_pred   pnl_sum  pnl_sharpe  n_trades\n",
      "                                      1) LAST fold model + LAST fold thresholds (fold=4)       0.55     0.65           0.366232         0.363468         0.481688 -0.319047   -1.175128       605\n",
      "                 2) BEST-VAL fold model + BEST-VAL thresholds (fold=2, val_score=0.8030)       0.55     0.50           0.457953         0.350512         0.543790 -0.342841   -1.002334       683\n",
      "3) LAST fold model + GLOBAL thresholds (VAL-concat; true_val_trade=0.422) (model_fold=4)       0.55     0.50           0.366232         0.363468         0.615446  0.217940    0.626969       773\n",
      "\n",
      "Global thresholds (method 3) top-5 candidates (VAL-concat):\n",
      "    thr_trade  thr_dir     score  trade_rate   pnl_sum  pnl_sharpe  n_trades\n",
      "5        0.55     0.50  0.949005    0.506858  0.957457    1.093841      2291\n",
      "6        0.55     0.55  0.701944    0.467478  0.706457    0.826247      2113\n",
      "0        0.50     0.50  0.497125    0.597566  0.514647    0.535044      2701\n",
      "7        0.55     0.60  0.457580    0.419248  0.457890    0.552376      1895\n",
      "10       0.60     0.50  0.345625    0.340708  0.353789    0.470271      1540\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "def _safe_auc_binary(y_true: np.ndarray, p1: np.ndarray) -> float:\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    p1 = np.asarray(p1, dtype=np.float64)\n",
    "    if y_true.size == 0 or len(np.unique(y_true)) < 2:\n",
    "        return float(\"nan\")\n",
    "    return float(roc_auc_score(y_true, p1))\n",
    "\n",
    "\n",
    "def _build_model_from_state(node_in: int, edge_dim: int, cfg: Dict[str, Any], state: Dict[str, torch.Tensor]) -> nn.Module:\n",
    "    m = MTGNN_ConvAttn_Classifier(\n",
    "        node_in=node_in,\n",
    "        edge_dim=edge_dim,\n",
    "        cfg=cfg,\n",
    "        n_nodes=len(ASSETS),\n",
    "        target_node=TARGET_NODE,\n",
    "        n_classes=2,\n",
    "    ).to(DEVICE)\n",
    "    m.load_state_dict(state)\n",
    "    m.eval()\n",
    "    return m\n",
    "\n",
    "\n",
    "def _get_scaled_arrays_for_fold(idx_tr_fold: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    X_scaled, _ = fit_scale_nodes_train_only(X_node_raw, sample_t, idx_tr_fold, max_abs=CFG[\"max_abs_feat\"])\n",
    "    if bool(CFG.get(\"edge_scale\", True)):\n",
    "        edge_scaled, _ = fit_scale_edges_train_only(edge_feat, sample_t, idx_tr_fold, max_abs=CFG[\"max_abs_edge\"])\n",
    "    else:\n",
    "        edge_scaled = edge_feat\n",
    "    return X_scaled, edge_scaled\n",
    "\n",
    "\n",
    "def _eval_holdout_with_models_and_thresholds(\n",
    "    method: str,\n",
    "    m_trade: nn.Module,\n",
    "    m_dir: nn.Module,\n",
    "    thr_trade: float,\n",
    "    thr_dir: float,\n",
    "    X_scaled: np.ndarray,\n",
    "    edge_scaled: np.ndarray,\n",
    "    idx_holdout: np.ndarray,\n",
    ") -> Dict[str, Any]:\n",
    "    prob_trade_hold, er_hold = predict_probs_on_indices(m_trade, X_scaled, edge_scaled, idx_holdout, CFG)\n",
    "    prob_dir_hold, _ = predict_probs_on_indices(m_dir, X_scaled, edge_scaled, idx_holdout, CFG)\n",
    "\n",
    "    t_hold = sample_t[idx_holdout]\n",
    "    y_trade_hold = y_trade[t_hold].astype(np.int64)\n",
    "    y_dir_hold = y_dir[t_hold].astype(np.int64)\n",
    "\n",
    "    trade_auc = _safe_auc_binary(y_trade_hold, prob_trade_hold[:, 1])\n",
    "    mask_true_trade = (y_trade_hold == 1)\n",
    "    dir_auc = _safe_auc_binary(y_dir_hold[mask_true_trade], prob_dir_hold[mask_true_trade, 1])\n",
    "\n",
    "    pnl = two_stage_pnl_by_threshold(\n",
    "        prob_trade_hold, prob_dir_hold, er_hold,\n",
    "        thr_trade=thr_trade, thr_dir=thr_dir,\n",
    "        cost_bps=CFG[\"cost_bps\"],\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"method\": method,\n",
    "        \"thr_trade\": float(thr_trade),\n",
    "        \"thr_dir\": float(thr_dir),\n",
    "        \"holdout_trade_auc\": float(trade_auc) if np.isfinite(trade_auc) else np.nan,\n",
    "        \"holdout_dir_auc\": float(dir_auc) if np.isfinite(dir_auc) else np.nan,\n",
    "        \"trade_rate_pred\": float(pnl[\"trade_rate\"]),\n",
    "        \"pnl_sum\": float(pnl[\"pnl_sum\"]),\n",
    "        \"pnl_sharpe\": float(pnl[\"pnl_sharpe\"]),\n",
    "        \"n_trades\": int(pnl[\"n_trades\"]),\n",
    "    }\n",
    "\n",
    "\n",
    "def run_post_cv_holdout_checks() -> pd.DataFrame:\n",
    "    idx_holdout = idx_final_test.astype(np.int64)\n",
    "    node_in = int(X_node_raw.shape[-1])\n",
    "    edge_dim = int(edge_feat.shape[-1])\n",
    "\n",
    "    ok_folds = [fa for fa in fold_artifacts if fa.get(\"dir_state\") is not None and np.isfinite(fa.get(\"thr_trade\", np.nan))]\n",
    "    if len(ok_folds) == 0:\n",
    "        raise RuntimeError(\"No folds with a trained DIR model were stored; cannot run Step 11 checks.\")\n",
    "\n",
    "    # 1) LAST fold model + LAST fold thresholds\n",
    "    fa_last = ok_folds[-1]\n",
    "    X_last, E_last = _get_scaled_arrays_for_fold(fa_last[\"idx_tr\"])\n",
    "    m_trade_last_ = _build_model_from_state(node_in, edge_dim, CFG, fa_last[\"trade_state\"])\n",
    "    m_dir_last_ = _build_model_from_state(node_in, edge_dim, CFG, fa_last[\"dir_state\"])\n",
    "    r1 = _eval_holdout_with_models_and_thresholds(\n",
    "        method=f\"1) LAST fold model + LAST fold thresholds (fold={fa_last['fold']})\",\n",
    "        m_trade=m_trade_last_,\n",
    "        m_dir=m_dir_last_,\n",
    "        thr_trade=float(fa_last[\"thr_trade\"]),\n",
    "        thr_dir=float(fa_last[\"thr_dir\"]),\n",
    "        X_scaled=X_last,\n",
    "        edge_scaled=E_last,\n",
    "        idx_holdout=idx_holdout,\n",
    "    )\n",
    "\n",
    "    # 2) BEST-VAL fold model + BEST-VAL thresholds\n",
    "    fa_best = max(ok_folds, key=lambda d: float(d.get(\"best_val_score\", -1e18)))\n",
    "    X_best, E_best = _get_scaled_arrays_for_fold(fa_best[\"idx_tr\"])\n",
    "    m_trade_best = _build_model_from_state(node_in, edge_dim, CFG, fa_best[\"trade_state\"])\n",
    "    m_dir_best = _build_model_from_state(node_in, edge_dim, CFG, fa_best[\"dir_state\"])\n",
    "    r2 = _eval_holdout_with_models_and_thresholds(\n",
    "        method=f\"2) BEST-VAL fold model + BEST-VAL thresholds (fold={fa_best['fold']}, val_score={fa_best['best_val_score']:.4f})\",\n",
    "        m_trade=m_trade_best,\n",
    "        m_dir=m_dir_best,\n",
    "        thr_trade=float(fa_best[\"thr_trade\"]),\n",
    "        thr_dir=float(fa_best[\"thr_dir\"]),\n",
    "        X_scaled=X_best,\n",
    "        edge_scaled=E_best,\n",
    "        idx_holdout=idx_holdout,\n",
    "    )\n",
    "\n",
    "    # 3) LAST fold model + GLOBAL thresholds on concatenated fold-VAL predictions\n",
    "    prob_trade_all = np.concatenate([fa[\"prob_trade_val\"] for fa in ok_folds], axis=0)\n",
    "    prob_dir_all = np.concatenate([fa[\"prob_dir_val\"] for fa in ok_folds], axis=0)\n",
    "    er_all = np.concatenate([fa[\"er_val\"] for fa in ok_folds], axis=0)\n",
    "\n",
    "    idx_va_all = np.concatenate([fa[\"idx_va\"] for fa in ok_folds], axis=0)\n",
    "    true_trade_rate_all = split_trade_ratio(idx_va_all, sample_t, y_trade)\n",
    "\n",
    "    sweep_global = sweep_thresholds(\n",
    "        prob_trade_all, prob_dir_all, er_all, CFG,\n",
    "        min_trades=int(CFG[\"eval_min_trades\"]),\n",
    "        target_trade_rate=float(true_trade_rate_all),\n",
    "    )\n",
    "    best_global = sweep_global.iloc[0].to_dict()\n",
    "    thr_trade_global = float(best_global[\"thr_trade\"])\n",
    "    thr_dir_global = float(best_global[\"thr_dir\"])\n",
    "\n",
    "    r3 = _eval_holdout_with_models_and_thresholds(\n",
    "        method=f\"3) LAST fold model + GLOBAL thresholds (VAL-concat; true_val_trade={true_trade_rate_all:.3f}) (model_fold={fa_last['fold']})\",\n",
    "        m_trade=m_trade_last_,\n",
    "        m_dir=m_dir_last_,\n",
    "        thr_trade=thr_trade_global,\n",
    "        thr_dir=thr_dir_global,\n",
    "        X_scaled=X_last,\n",
    "        edge_scaled=E_last,\n",
    "        idx_holdout=idx_holdout,\n",
    "    )\n",
    "\n",
    "    out = pd.DataFrame([r1, r2, r3])\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STEP 11 — HOLDOUT CHECKS WITHOUT ANY REFIT (3 METHODS)\")\n",
    "    print(f\"Holdout size: {len(idx_holdout)} samples | sidx {int(idx_holdout[0])}..{int(idx_holdout[-1])}\")\n",
    "\n",
    "    print(\"\\nResults (compare these, not mean/median over folds):\")\n",
    "    print(out[[\n",
    "        \"method\", \"thr_trade\", \"thr_dir\",\n",
    "        \"holdout_trade_auc\", \"holdout_dir_auc\",\n",
    "        \"trade_rate_pred\", \"pnl_sum\", \"pnl_sharpe\", \"n_trades\"\n",
    "    ]].to_string(index=False))\n",
    "\n",
    "    print(\"\\nGlobal thresholds (method 3) top-5 candidates (VAL-concat):\")\n",
    "    print(sweep_global.head(5)[[\"thr_trade\", \"thr_dir\", \"score\", \"trade_rate\", \"pnl_sum\", \"pnl_sharpe\", \"n_trades\"]])\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    return out\n",
    "\n",
    "\n",
    "post_cv_holdout = run_post_cv_holdout_checks()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Step 12 — Production-fit: train on CV(90%) → select thresholds on val_final → eval on FINAL holdout(10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 12 — PRODUCTION-FIT (TRAIN ON CV(90%) → SELECT THR ON val_final → EVAL ON FINAL HOLDOUT(10%))\n",
      "Final split sizes:\n",
      "  train_final: 10175\n",
      "  val_final  : 1130\n",
      "  holdout    : 1256\n",
      "True trade ratio (val_final): 0.504\n",
      "True trade ratio (holdout):   0.576\n",
      "[trade] ep 01 lr=9.80e-05 tr_loss=0.7392 va_loss=0.6292 va_auc=0.683 alpha=0.500 reg(l1=0.3333, prior=0.0050) best=0.683@ep01\n",
      "[trade] ep 02 lr=2.34e-04 tr_loss=0.6344 va_loss=0.6343 va_auc=0.677 alpha=0.498 reg(l1=0.3333, prior=0.0050) best=0.683@ep01\n",
      "[trade] ep 03 lr=3.00e-04 tr_loss=0.6063 va_loss=0.6461 va_auc=0.679 alpha=0.496 reg(l1=0.3334, prior=0.0050) best=0.683@ep01\n",
      "[trade] ep 04 lr=2.97e-04 tr_loss=0.5897 va_loss=0.6450 va_auc=0.639 alpha=0.494 reg(l1=0.3334, prior=0.0050) best=0.683@ep01\n",
      "[trade] ep 05 lr=2.90e-04 tr_loss=0.5587 va_loss=0.6729 va_auc=0.672 alpha=0.492 reg(l1=0.3335, prior=0.0051) best=0.683@ep01\n",
      "[trade] ep 06 lr=2.77e-04 tr_loss=0.5426 va_loss=0.7655 va_auc=0.675 alpha=0.489 reg(l1=0.3337, prior=0.0051) best=0.683@ep01\n",
      "[trade] ep 07 lr=2.61e-04 tr_loss=0.5445 va_loss=0.6692 va_auc=0.661 alpha=0.487 reg(l1=0.3338, prior=0.0052) best=0.683@ep01\n",
      "[trade] ep 08 lr=2.40e-04 tr_loss=0.5208 va_loss=0.6726 va_auc=0.684 alpha=0.485 reg(l1=0.3340, prior=0.0053) best=0.684@ep08\n",
      "[trade] ep 09 lr=2.17e-04 tr_loss=0.5110 va_loss=0.7101 va_auc=0.663 alpha=0.484 reg(l1=0.3341, prior=0.0054) best=0.684@ep08\n",
      "[trade] ep 10 lr=1.91e-04 tr_loss=0.4937 va_loss=0.7448 va_auc=0.655 alpha=0.483 reg(l1=0.3342, prior=0.0054) best=0.684@ep08\n",
      "[trade] ep 11 lr=1.64e-04 tr_loss=0.4863 va_loss=0.6751 va_auc=0.609 alpha=0.483 reg(l1=0.3342, prior=0.0054) best=0.684@ep08\n",
      "[trade] ep 12 lr=1.36e-04 tr_loss=0.4775 va_loss=0.7142 va_auc=0.630 alpha=0.482 reg(l1=0.3342, prior=0.0055) best=0.684@ep08\n",
      "[trade] ep 13 lr=1.09e-04 tr_loss=0.4614 va_loss=0.7035 va_auc=0.583 alpha=0.481 reg(l1=0.3343, prior=0.0055) best=0.684@ep08\n",
      "[trade] ep 14 lr=8.33e-05 tr_loss=0.4586 va_loss=0.7077 va_auc=0.606 alpha=0.481 reg(l1=0.3343, prior=0.0055) best=0.684@ep08\n",
      "[trade] ep 15 lr=5.98e-05 tr_loss=0.4538 va_loss=0.7701 va_auc=0.615 alpha=0.481 reg(l1=0.3343, prior=0.0056) best=0.684@ep08\n",
      "\n",
      "Trade-only sizes for DIR:\n",
      "  train_final_T: 3980\n",
      "  val_final_T  : 569\n",
      "  holdout_T    : 724\n",
      "[dir] ep 01 lr=9.88e-05 tr_loss=0.7558 va_loss=0.6615 va_auc=0.704 alpha=0.500 reg(l1=0.3333, prior=0.0050) best=0.704@ep01\n",
      "[dir] ep 02 lr=2.35e-04 tr_loss=0.6752 va_loss=0.6560 va_auc=0.709 alpha=0.500 reg(l1=0.3333, prior=0.0050) best=0.709@ep02\n",
      "[dir] ep 03 lr=3.00e-04 tr_loss=0.5980 va_loss=0.6966 va_auc=0.646 alpha=0.499 reg(l1=0.3333, prior=0.0050) best=0.709@ep02\n",
      "[dir] ep 04 lr=2.97e-04 tr_loss=0.5529 va_loss=0.6672 va_auc=0.628 alpha=0.498 reg(l1=0.3334, prior=0.0050) best=0.709@ep02\n",
      "[dir] ep 05 lr=2.90e-04 tr_loss=0.5273 va_loss=0.7736 va_auc=0.509 alpha=0.497 reg(l1=0.3333, prior=0.0050) best=0.709@ep02\n",
      "[dir] ep 06 lr=2.77e-04 tr_loss=0.5162 va_loss=0.6426 va_auc=0.681 alpha=0.497 reg(l1=0.3334, prior=0.0050) best=0.709@ep02\n",
      "[dir] ep 07 lr=2.60e-04 tr_loss=0.5320 va_loss=0.6633 va_auc=0.643 alpha=0.496 reg(l1=0.3334, prior=0.0050) best=0.709@ep02\n",
      "[dir] ep 08 lr=2.40e-04 tr_loss=0.4814 va_loss=0.7129 va_auc=0.596 alpha=0.496 reg(l1=0.3334, prior=0.0050) best=0.709@ep02\n",
      "[dir] ep 09 lr=2.16e-04 tr_loss=0.4550 va_loss=0.8428 va_auc=0.524 alpha=0.495 reg(l1=0.3334, prior=0.0050) best=0.709@ep02\n",
      "\n",
      "Chosen thresholds on val_final:\n",
      "  thr_trade*=0.700 thr_dir*=0.500 | score=1.4417\n",
      "  val trade_rate(pred)=0.525 | val pnl_sum=1.4439 | val sharpe=4.826 | trades=593\n",
      "\n",
      "FINAL HOLDOUT RESULT (production-fit, fixed thresholds from val_final):\n",
      "  AUC trade=0.448 | AUC dir(trade-only)=0.265\n",
      "  trade_rate(pred)=0.763\n",
      "  pnl_sum=-1.8953 | pnl_mean=-0.001509 | trades=958\n",
      "  sharpe(per-bar proxy)=-5.040\n",
      "\n",
      "AUC summary (val_final vs holdout):\n",
      "  TRADE: val_auc=0.684 | holdout_auc=0.448\n",
      "  DIR  : val_auc=0.709 | holdout_auc=0.265\n",
      "\n",
      "Top-5 val_final threshold candidates:\n",
      "   thr_trade  thr_dir     score  trade_rate   pnl_sum  pnl_sharpe  n_trades\n",
      "4       0.70     0.50  1.441738    0.524779  1.443862    4.826334       593\n",
      "6       0.75     0.50  1.341050    0.423009  1.349103    4.772245       478\n",
      "1       0.55     0.55  1.097993    0.401770  1.108170    4.477809       454\n",
      "2       0.60     0.55  1.097255    0.400000  1.107609    4.475476       452\n",
      "3       0.65     0.55  1.039930    0.379646  1.052319    4.280357       429\n",
      "================================================================================\n",
      "\n",
      "Production-fit summary dict:\n",
      "{'thr_trade': 0.7, 'thr_dir': 0.5, 'val_true_trade_rate': 0.5035398230088496, 'hold_true_trade_rate': 0.5764331210191083, 'holdout_trade_auc': 0.4477812279317077, 'holdout_dir_auc': 0.2646206786171575, 'n': 1256, 'n_trades': 958, 'trade_rate': 0.7627388535031847, 'pnl_sum': -1.8952815532684326, 'pnl_mean': -0.001508982153609395, 'pnl_per_trade': -0.0019783731549978256, 'pnl_sharpe': -5.040416297633378}\n"
     ]
    }
   ],
   "source": [
    "def run_production_fit() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Train on the full CV-part (90%) with a final validation window (val_final),\n",
    "    select thresholds on val_final, then evaluate on FINAL holdout (10%).\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STEP 12 — PRODUCTION-FIT (TRAIN ON CV(90%) → SELECT THR ON val_final → EVAL ON FINAL HOLDOUT(10%))\")\n",
    "\n",
    "    val_w = max(1, int(CFG[\"val_window_frac\"] * n_samples_cv))\n",
    "    train_end = n_samples_cv - val_w\n",
    "\n",
    "    idx_train_final = np.arange(0, train_end, dtype=np.int64)\n",
    "    idx_val_final = np.arange(train_end, n_samples_cv, dtype=np.int64)\n",
    "    idx_holdout = idx_final_test.astype(np.int64)\n",
    "\n",
    "    true_val_trade = split_trade_ratio(idx_val_final, sample_t, y_trade)\n",
    "    true_hold_trade = split_trade_ratio(idx_holdout, sample_t, y_trade)\n",
    "\n",
    "    print(\"Final split sizes:\")\n",
    "    print(\"  train_final:\", len(idx_train_final))\n",
    "    print(\"  val_final  :\", len(idx_val_final))\n",
    "    print(\"  holdout    :\", len(idx_holdout))\n",
    "    print(f\"True trade ratio (val_final): {true_val_trade:.3f}\")\n",
    "    print(f\"True trade ratio (holdout):   {true_hold_trade:.3f}\")\n",
    "\n",
    "    X_scaled_final, _ = fit_scale_nodes_train_only(X_node_raw, sample_t, idx_train_final, max_abs=CFG[\"max_abs_feat\"])\n",
    "    if bool(CFG.get(\"edge_scale\", True)):\n",
    "        edge_scaled_final, _ = fit_scale_edges_train_only(edge_feat, sample_t, idx_train_final, max_abs=CFG[\"max_abs_edge\"])\n",
    "    else:\n",
    "        edge_scaled_final = edge_feat\n",
    "\n",
    "    # Stage A\n",
    "    m_trade_f, r_trade = train_binary_classifier(\n",
    "        X_scaled_final, edge_scaled_final, y_trade, y_dir, exit_ret, sample_t,\n",
    "        idx_train_final, idx_val_final, idx_holdout, CFG, stage_name=\"trade\"\n",
    "    )\n",
    "\n",
    "    # Stage B (trade-only)\n",
    "    idx_train_T = subset_trade_indices(idx_train_final, sample_t, y_trade)\n",
    "    idx_val_T = subset_trade_indices(idx_val_final, sample_t, y_trade)\n",
    "    idx_hold_T = subset_trade_indices(idx_holdout, sample_t, y_trade)\n",
    "\n",
    "    print(\"\\nTrade-only sizes for DIR:\")\n",
    "    print(\"  train_final_T:\", len(idx_train_T))\n",
    "    print(\"  val_final_T  :\", len(idx_val_T))\n",
    "    print(\"  holdout_T    :\", len(idx_hold_T))\n",
    "\n",
    "    m_dir_f, r_dir = train_binary_classifier(\n",
    "        X_scaled_final, edge_scaled_final, y_trade, y_dir, exit_ret, sample_t,\n",
    "        idx_train_T, idx_val_T, idx_hold_T, CFG, stage_name=\"dir\"\n",
    "    )\n",
    "\n",
    "    # Choose thresholds on val_final (VAL only)\n",
    "    prob_trade_val, er_val = predict_probs_on_indices(m_trade_f, X_scaled_final, edge_scaled_final, idx_val_final, CFG)\n",
    "    prob_dir_val, _ = predict_probs_on_indices(m_dir_f, X_scaled_final, edge_scaled_final, idx_val_final, CFG)\n",
    "\n",
    "    sweep_val = sweep_thresholds(\n",
    "        prob_trade_val, prob_dir_val, er_val, CFG,\n",
    "        min_trades=int(CFG[\"eval_min_trades\"]),\n",
    "        target_trade_rate=float(true_val_trade),\n",
    "    )\n",
    "    best_val = sweep_val.iloc[0].to_dict()\n",
    "    thr_trade_star = float(best_val[\"thr_trade\"])\n",
    "    thr_dir_star = float(best_val[\"thr_dir\"])\n",
    "\n",
    "    val_metrics = two_stage_pnl_by_threshold(prob_trade_val, prob_dir_val, er_val, thr_trade_star, thr_dir_star, CFG[\"cost_bps\"])\n",
    "    print(\"\\nChosen thresholds on val_final:\")\n",
    "    print(f\"  thr_trade*={thr_trade_star:.3f} thr_dir*={thr_dir_star:.3f} | score={best_val['score']:.4f}\")\n",
    "    print(f\"  val trade_rate(pred)={val_metrics['trade_rate']:.3f} | val pnl_sum={val_metrics['pnl_sum']:.4f} | val sharpe={val_metrics['pnl_sharpe']:.3f} | trades={val_metrics['n_trades']}\")\n",
    "\n",
    "    # Evaluate holdout with fixed thresholds\n",
    "    prob_trade_hold, er_hold = predict_probs_on_indices(m_trade_f, X_scaled_final, edge_scaled_final, idx_holdout, CFG)\n",
    "    prob_dir_hold, _ = predict_probs_on_indices(m_dir_f, X_scaled_final, edge_scaled_final, idx_holdout, CFG)\n",
    "    hold_metrics = two_stage_pnl_by_threshold(prob_trade_hold, prob_dir_hold, er_hold, thr_trade_star, thr_dir_star, CFG[\"cost_bps\"])\n",
    "\n",
    "    # AUCs on holdout\n",
    "    t_hold = sample_t[idx_holdout]\n",
    "    y_trade_hold = y_trade[t_hold].astype(np.int64)\n",
    "    y_dir_hold = y_dir[t_hold].astype(np.int64)\n",
    "    trade_auc_hold = _safe_auc_binary(y_trade_hold, prob_trade_hold[:, 1])\n",
    "    mask_true_trade = (y_trade_hold == 1)\n",
    "    dir_auc_hold = _safe_auc_binary(y_dir_hold[mask_true_trade], prob_dir_hold[mask_true_trade, 1])\n",
    "\n",
    "    print(\"\\nFINAL HOLDOUT RESULT (production-fit, fixed thresholds from val_final):\")\n",
    "    print(f\"  AUC trade={trade_auc_hold:.3f} | AUC dir(trade-only)={dir_auc_hold:.3f}\")\n",
    "    print(f\"  trade_rate(pred)={hold_metrics['trade_rate']:.3f}\")\n",
    "    print(f\"  pnl_sum={hold_metrics['pnl_sum']:.4f} | pnl_mean={hold_metrics['pnl_mean']:.6f} | trades={hold_metrics['n_trades']}\")\n",
    "    print(f\"  sharpe(per-bar proxy)={hold_metrics['pnl_sharpe']:.3f}\")\n",
    "\n",
    "    print(\"\\nAUC summary (val_final vs holdout):\")\n",
    "    print(f\"  TRADE: val_auc={r_trade['val']['auc']:.3f} | holdout_auc={trade_auc_hold:.3f}\")\n",
    "    print(f\"  DIR  : val_auc={r_dir['val']['auc']:.3f} | holdout_auc={dir_auc_hold:.3f}\")\n",
    "\n",
    "    print(\"\\nTop-5 val_final threshold candidates:\")\n",
    "    print(sweep_val.head(5)[[\"thr_trade\", \"thr_dir\", \"score\", \"trade_rate\", \"pnl_sum\", \"pnl_sharpe\", \"n_trades\"]])\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    return {\n",
    "        \"thr_trade\": thr_trade_star,\n",
    "        \"thr_dir\": thr_dir_star,\n",
    "        \"val_true_trade_rate\": float(true_val_trade),\n",
    "        \"hold_true_trade_rate\": float(true_hold_trade),\n",
    "        \"holdout_trade_auc\": float(trade_auc_hold) if np.isfinite(trade_auc_hold) else np.nan,\n",
    "        \"holdout_dir_auc\": float(dir_auc_hold) if np.isfinite(dir_auc_hold) else np.nan,\n",
    "        **hold_metrics,\n",
    "    }\n",
    "\n",
    "\n",
    "prod_fit_result = run_production_fit()\n",
    "print(\"\\nProduction-fit summary dict:\")\n",
    "print(prod_fit_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nOptions for selecting a \"final\" configuration after CV (conceptually):\\n\\n- \"LAST fold\" (method 1):\\n  Most realistic if your production regime is closest to the latest market state.\\n  Uses that fold\\'s preprocessing (scalers), models, and thresholds.\\n\\n- \"BEST-VAL fold\" (method 2):\\n  Picks the fold whose own VAL threshold-sweep score is best (still VAL-only).\\n  Often helps if some folds are noisy / bad regime.\\n\\n- \"GLOBAL thresholds\" (method 3):\\n  Keeps the LAST model, but stabilizes threshold selection by aggregating all fold-VAL predictions.\\n  This can be less brittle than using only the last fold VAL.\\n\\nAdjacency knobs:\\n- CFG[\"adj_mode\"] = \"emb\" or \"matrix\"\\n- CFG[\"alpha_mode\"] = \"learned\" typically works better than fixed alpha\\n- CFG[\"adj_l1_lambda\"] increases sparsity pressure (on sigmoid(logits) off-diagonal)\\n- CFG[\"adj_prior_lambda\"] enforces consistency with A_prior from edge_attr\\n- CFG[\"prior_use_abs\"] controls whether negative corr strengthens adjacency (abs) or weakens it (no abs)\\n- CFG[\"adj_temperature\"] controls softness of A_learned row-softmax\\n\\nTemporal knobs:\\n- CFG[\"tcn_layers\"], CFG[\"tcn_kernel\"], CFG[\"tcn_dropout\"]\\n- CFG[\"attn_pool_hidden\"] for pooling MLP capacity\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Options for selecting a \"final\" configuration after CV (conceptually):\n",
    "\n",
    "- \"LAST fold\" (method 1):\n",
    "  Most realistic if your production regime is closest to the latest market state.\n",
    "  Uses that fold's preprocessing (scalers), models, and thresholds.\n",
    "\n",
    "- \"BEST-VAL fold\" (method 2):\n",
    "  Picks the fold whose own VAL threshold-sweep score is best (still VAL-only).\n",
    "  Often helps if some folds are noisy / bad regime.\n",
    "\n",
    "- \"GLOBAL thresholds\" (method 3):\n",
    "  Keeps the LAST model, but stabilizes threshold selection by aggregating all fold-VAL predictions.\n",
    "  This can be less brittle than using only the last fold VAL.\n",
    "\n",
    "Adjacency knobs:\n",
    "- CFG[\"adj_mode\"] = \"emb\" or \"matrix\"\n",
    "- CFG[\"alpha_mode\"] = \"learned\" typically works better than fixed alpha\n",
    "- CFG[\"adj_l1_lambda\"] increases sparsity pressure (on sigmoid(logits) off-diagonal)\n",
    "- CFG[\"adj_prior_lambda\"] enforces consistency with A_prior from edge_attr\n",
    "- CFG[\"prior_use_abs\"] controls whether negative corr strengthens adjacency (abs) or weakens it (no abs)\n",
    "- CFG[\"adj_temperature\"] controls softness of A_learned row-softmax\n",
    "\n",
    "Temporal knobs:\n",
    "- CFG[\"tcn_layers\"], CFG[\"tcn_kernel\"], CFG[\"tcn_dropout\"]\n",
    "- CFG[\"attn_pool_hidden\"] for pooling MLP capacity\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recbole_new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
